"""Memory utilization benchmarks.

Measures actual memory usage vs. theoretical predictions:
- Memory scaling: 1, 5, 10 agents
- Block padding overhead (wasted memory)
- Cache vs model memory ratio
- Actual vs theoretical memory validation
"""

import httpx
import psutil
import pytest

from tests.benchmarks.conftest import BenchmarkReporter


@pytest.mark.benchmark
def test_memory_per_agent_1_5_10_agents(
    benchmark_client: httpx.Client, benchmark_reporter: BenchmarkReporter
):
    """Benchmark memory usage as agent count increases.

    Measures:
    - Memory with 1 agent
    - Memory with 5 agents
    - Memory with 10 agents
    - Memory per agent scaling

    Expected: Linear scaling, <100MB per agent
    """
    import time

    results = []

    # Get server process (find by port or process name)
    # Note: This is a simplified approach - may need refinement
    server_process = None
    for proc in psutil.process_iter(["pid", "name", "cmdline"]):
        try:
            cmdline = proc.info.get("cmdline", [])
            if cmdline and "semantic.entrypoints.cli" in " ".join(cmdline):
                server_process = psutil.Process(proc.info["pid"])
                break
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    if not server_process:
        pytest.skip("Could not find server process for memory measurement")

    # Baseline memory (just server, no agents)
    baseline_memory_mb = server_process.memory_info().rss / 1024 / 1024

    for num_agents in [1, 5, 10]:
        # Create N agents with caches
        agent_ids = [f"memory-test-agent-{i}" for i in range(num_agents)]

        for agent_id in agent_ids:
            # Generate content to fill cache (~1000 tokens per agent)
            for req_num in range(2):
                benchmark_client.post(
                    "/v1/messages",
                    json={
                        "model": "test-model",
                        "messages": [
                            {
                                "role": "user",
                                "content": f"{agent_id} memory test request {req_num}",
                            }
                        ],
                        "max_tokens": 500,
                    },
                    headers={"X-API-Key": agent_id},
                )

        # Wait for cache operations to complete
        time.sleep(2)

        # Measure memory
        memory_mb = server_process.memory_info().rss / 1024 / 1024
        cache_memory_mb = memory_mb - baseline_memory_mb
        memory_per_agent_mb = cache_memory_mb / num_agents if num_agents > 0 else 0

        results.append(
            {
                "num_agents": num_agents,
                "total_memory_mb": round(memory_mb, 1),
                "cache_memory_mb": round(cache_memory_mb, 1),
                "memory_per_agent_mb": round(memory_per_agent_mb, 1),
            }
        )

        print(
            f"  {num_agents} agents: {memory_mb:.1f} MB total, "
            f"{cache_memory_mb:.1f} MB caches, "
            f"{memory_per_agent_mb:.1f} MB/agent"
        )

    benchmark_reporter.record("memory_scaling", results)

    # Verify per-agent memory is reasonable (<100MB per agent)
    for result in results:
        if result["num_agents"] > 1:  # Skip baseline check
            assert result["memory_per_agent_mb"] < 100, (
                f"Memory per agent too high: {result['memory_per_agent_mb']:.1f} MB (target <100 MB)"
            )

    print(
        f"\nðŸ“Š Memory scaling benchmark:"
        f"\n  Baseline: {baseline_memory_mb:.1f} MB"
        f"\n  Results: {results}"
    )


@pytest.mark.benchmark
def test_block_padding_overhead(
    benchmark_client: httpx.Client, benchmark_reporter: BenchmarkReporter
):
    """Benchmark block padding overhead (wasted memory from alignment).

    Measures:
    - Theoretical memory usage (no padding)
    - Actual memory usage (with padding)
    - Padding overhead percentage

    Expected: <20% overhead from block padding
    """
    # This test measures the efficiency of block allocation
    # In the paged attention system, caches are stored in fixed-size blocks
    # Padding overhead = (actual_memory - theoretical_memory) / theoretical_memory

    # Create agents with varying cache sizes
    agent_ids = [f"padding-test-agent-{i}" for i in range(3)]

    for agent_id in agent_ids:
        # Generate varying amounts of content
        for req_num in range(3):
            benchmark_client.post(
                "/v1/messages",
                json={
                    "model": "test-model",
                    "messages": [
                        {
                            "role": "user",
                            "content": f"{agent_id} padding test {req_num}",
                        }
                    ],
                    "max_tokens": 300 + req_num * 100,  # Varying sizes
                },
                headers={"X-API-Key": agent_id},
            )

    # Note: Actual padding overhead measurement requires:
    # 1. Knowing exact token counts in cache
    # 2. Block size from model spec
    # 3. Server API endpoint to report padding stats

    # For now, record that test framework is in place
    benchmark_reporter.record(
        "block_padding_overhead",
        {
            "note": "Requires server API for detailed padding statistics",
            "agents_tested": len(agent_ids),
            "target_overhead_pct": "<20%",
        },
    )

    print(
        "\nðŸ“Š Block padding overhead benchmark:"
        "\n  Note: Detailed measurement requires server instrumentation"
        "\n  Target: <20% overhead"
    )


@pytest.mark.benchmark
def test_cache_vs_model_memory_ratio(
    benchmark_client: httpx.Client, benchmark_reporter: BenchmarkReporter
):
    """Benchmark cache memory vs model memory ratio.

    Measures:
    - Total memory usage
    - Model memory (baseline)
    - Cache memory (agents)
    - Cache/model ratio

    Expected: Cache memory < model memory for reasonable agent counts
    """
    import time

    # Find server process
    server_process = None
    for proc in psutil.process_iter(["pid", "name", "cmdline"]):
        try:
            cmdline = proc.info.get("cmdline", [])
            if cmdline and "semantic.entrypoints.cli" in " ".join(cmdline):
                server_process = psutil.Process(proc.info["pid"])
                break
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    if not server_process:
        pytest.skip("Could not find server process for memory measurement")

    # Baseline (model only, no caches)
    model_memory_mb = server_process.memory_info().rss / 1024 / 1024

    # Create agents to add cache memory
    num_agents = 5
    agent_ids = [f"ratio-test-agent-{i}" for i in range(num_agents)]

    for agent_id in agent_ids:
        for req_num in range(2):
            benchmark_client.post(
                "/v1/messages",
                json={
                    "model": "test-model",
                    "messages": [{"role": "user", "content": f"{agent_id} request {req_num}"}],
                    "max_tokens": 500,
                },
                headers={"X-API-Key": agent_id},
            )

    time.sleep(2)

    # Measure total memory with caches
    total_memory_mb = server_process.memory_info().rss / 1024 / 1024
    cache_memory_mb = total_memory_mb - model_memory_mb
    cache_model_ratio = cache_memory_mb / model_memory_mb if model_memory_mb > 0 else 0

    benchmark_reporter.record(
        "cache_vs_model_memory",
        {
            "model_memory_mb": round(model_memory_mb, 1),
            "cache_memory_mb": round(cache_memory_mb, 1),
            "total_memory_mb": round(total_memory_mb, 1),
            "cache_model_ratio": round(cache_model_ratio, 2),
            "num_agents": num_agents,
        },
    )

    print(
        f"\nðŸ“Š Cache vs model memory ratio:"
        f"\n  Model memory: {model_memory_mb:.1f} MB"
        f"\n  Cache memory: {cache_memory_mb:.1f} MB ({num_agents} agents)"
        f"\n  Total memory: {total_memory_mb:.1f} MB"
        f"\n  Ratio (cache/model): {cache_model_ratio:.2f}"
    )


@pytest.mark.benchmark
def test_actual_vs_theoretical_memory(
    benchmark_client: httpx.Client, benchmark_reporter: BenchmarkReporter
):
    """Compare actual memory usage vs theoretical predictions.

    Measures:
    - Actual memory usage (measured)
    - Theoretical memory usage (calculated)
    - Variance from theory

    Expected: Actual within 20% of theoretical
    """
    import time

    # Find server process
    server_process = None
    for proc in psutil.process_iter(["pid", "name", "cmdline"]):
        try:
            cmdline = proc.info.get("cmdline", [])
            if cmdline and "semantic.entrypoints.cli" in " ".join(cmdline):
                server_process = psutil.Process(proc.info["pid"])
                break
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    if not server_process:
        pytest.skip("Could not find server process for memory measurement")

    # Baseline
    baseline_memory_mb = server_process.memory_info().rss / 1024 / 1024

    # Create agents with known cache sizes
    num_agents = 3
    tokens_per_agent = 1000  # Target cache size
    agent_ids = [f"theoretical-test-agent-{i}" for i in range(num_agents)]

    for agent_id in agent_ids:
        # Generate exactly tokens_per_agent tokens (approximately)
        num_requests = 2
        tokens_per_request = tokens_per_agent // num_requests

        for req_num in range(num_requests):
            benchmark_client.post(
                "/v1/messages",
                json={
                    "model": "test-model",
                    "messages": [
                        {
                            "role": "user",
                            "content": f"{agent_id} theoretical test {req_num}",
                        }
                    ],
                    "max_tokens": tokens_per_request,
                },
                headers={"X-API-Key": agent_id},
            )

    time.sleep(2)

    # Measure actual memory
    actual_memory_mb = server_process.memory_info().rss / 1024 / 1024
    actual_cache_mb = actual_memory_mb - baseline_memory_mb

    # Calculate theoretical memory
    # Assumptions:
    # - Each token â‰ˆ 2 bytes for KV cache (rough estimate)
    # - Model metadata overhead â‰ˆ 10 MB per agent
    theoretical_kv_mb = (num_agents * tokens_per_agent * 2) / 1024 / 1024
    theoretical_overhead_mb = num_agents * 10
    theoretical_total_mb = theoretical_kv_mb + theoretical_overhead_mb

    # Variance
    variance_pct = (
        ((actual_cache_mb - theoretical_total_mb) / theoretical_total_mb) * 100
        if theoretical_total_mb > 0
        else 0
    )

    benchmark_reporter.record(
        "actual_vs_theoretical_memory",
        {
            "actual_cache_mb": round(actual_cache_mb, 1),
            "theoretical_kv_mb": round(theoretical_kv_mb, 1),
            "theoretical_overhead_mb": round(theoretical_overhead_mb, 1),
            "theoretical_total_mb": round(theoretical_total_mb, 1),
            "variance_pct": round(variance_pct, 1),
            "num_agents": num_agents,
            "tokens_per_agent": tokens_per_agent,
        },
    )

    print(
        f"\nðŸ“Š Actual vs theoretical memory:"
        f"\n  Actual cache memory: {actual_cache_mb:.1f} MB"
        f"\n  Theoretical KV memory: {theoretical_kv_mb:.1f} MB"
        f"\n  Theoretical overhead: {theoretical_overhead_mb:.1f} MB"
        f"\n  Theoretical total: {theoretical_total_mb:.1f} MB"
        f"\n  Variance: {variance_pct:+.1f}%"
    )

    # Verify variance is reasonable (<50% in either direction)
    # Note: In test environment without real model, variance may be high
    print(
        f"\n  {'âœ… Variance within range' if abs(variance_pct) < 50 else 'âš ï¸  High variance (test environment)'}"
    )
