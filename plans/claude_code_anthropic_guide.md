# Claude Code CLI and Anthropic Messages API Comprehensive Guide

**Version**: 1.0.0
**Last Updated**: 2026-01-25
**Anthropic SDK Version**: 0.76.0+

This document provides comprehensive documentation about Claude Code CLI functionality and Anthropic API usage, including the Messages API, streaming protocols, tool use, extended thinking, and best practices.

---

## Table of Contents

1. [Claude Code CLI Overview](#1-claude-code-cli-overview)
2. [Anthropic Messages API](#2-anthropic-messages-api)
3. [Message Format](#3-message-format)
4. [Streaming Protocol (SSE)](#4-streaming-protocol-sse)
5. [Tool Use](#5-tool-use)
6. [Extended Thinking](#6-extended-thinking)
7. [Prompt Caching](#7-prompt-caching)
8. [Authentication and Headers](#8-authentication-and-headers)
9. [Error Handling](#9-error-handling)
10. [Claude Models (2025-2026)](#10-claude-models-2025-2026)
11. [Python SDK (anthropic)](#11-python-sdk-anthropic)
12. [Best Practices](#12-best-practices)

---

## 1. Claude Code CLI Overview

### 1.1 What is Claude Code CLI

Claude Code is Anthropic's official agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster. It can:

- Execute routine tasks through natural language commands
- Explain complex code
- Handle git workflows
- Read issues, write code, run tests, and submit PRs
- Integrate with VS Code, Cursor, Windsurf, and JetBrains IDEs

**Official Resources**:
- [Official Documentation](https://code.claude.com/docs/en/overview)
- [GitHub Repository](https://github.com/anthropics/claude-code)
- [Product Page](https://www.anthropic.com/claude-code)
- [Best Practices Guide](https://www.anthropic.com/engineering/claude-code-best-practices)

### 1.2 Architecture and Design

Claude Code ships as a single `cli.js` file (~10.5MB) that runs in Node.js. It follows a three-layer architecture:

```
┌─────────────────────────────────────────────────────────┐
│  PRESENTATION LAYER                                      │
│  - React/Ink UI                                          │
│  - Status Bar                                            │
│  - Input Handler                                         │
├─────────────────────────────────────────────────────────┤
│  CORE SERVICES                                           │
│  - Agent Loop                                            │
│  - Session State                                         │
│  - Tool Registry                                         │
│  - Permissions                                           │
├─────────────────────────────────────────────────────────┤
│  INTEGRATION LAYER                                       │
│  - Anthropic API Client                                  │
│  - Tool Executors                                        │
│  - MCP Servers                                           │
└─────────────────────────────────────────────────────────┘
```

**Bundle Contents**:
- Node.js application (agent logic, UI components, tool implementations)
- Vendored ripgrep binaries (platform-specific `rg` executables)
- Tree-sitter WASM modules (WebAssembly parsers for code understanding)

### 1.3 Session Management (Client-Side)

**Critical Insight**: Claude Code CLI uses **client-side session management**. Sessions are stored locally in `~/.claude/projects/`, NOT on Anthropic's servers.

Session data includes:
- `sessionId`: Unique identifier
- `startTime` and `lastActive`: Timestamps
- `messages`: Full conversation history
- `backgroundTasks`: Running commands with status, pid, output
- `fileContext`: Read, modified, and created files
- `permissions`: Approved tools and directories

**Key Commands**:
```bash
claude "query"          # Start new session
claude -c               # Resume last conversation
claude --resume         # Resume specific session
/init                   # Generate CLAUDE.md
/clear                  # Reset context window
```

### 1.4 How It Connects to Anthropic API

Claude Code uses the **standard `/v1/messages` endpoint** - NOT a special Agent SDK endpoint. Key characteristics:

1. **Full conversation history sent each time**: Every request includes all prior messages
2. **Session IDs are UUIDs**: Generated by client, NOT sent to server
3. **System prompt is ~18KB**: Includes tool definitions, CLAUDE.md, environment info
4. **Subagents spawn separate `/v1/messages` calls**: Different system prompts, different histories

```
Claude Code CLI
    │
    ▼ POST /v1/messages
┌─────────────────────────────────────────────────────┐
│  Standard Anthropic Messages API                     │
│  - No special session handling                       │
│  - Full history in each request                      │
│  - Streaming via SSE                                 │
└─────────────────────────────────────────────────────┘
```

### 1.5 CLAUDE.md - Project Memory

`CLAUDE.md` is a special Markdown file automatically read at session start. It provides:
- Persistent, project-specific memory
- Style conventions and design guidelines
- Common mistakes to avoid
- PR templates and best practices

Teams at Anthropic maintain ~2.5k token CLAUDE.md files per project.

---

## 2. Anthropic Messages API

### 2.1 Endpoint

```
POST https://api.anthropic.com/v1/messages
```

### 2.2 Request Format

```json
{
  "model": "claude-sonnet-4-5-20250929",
  "max_tokens": 8192,
  "temperature": 1.0,
  "system": [
    {
      "type": "text",
      "text": "You are Claude Code, Anthropic's official CLI...",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [
    {"role": "user", "content": "Help me fix the bug"},
    {"role": "assistant", "content": [
      {"type": "thinking", "thinking": "Let me analyze..."},
      {"type": "text", "text": "I'll look at the code."},
      {"type": "tool_use", "id": "toolu_123", "name": "Read", "input": {"file_path": "/src/main.py"}}
    ]},
    {"role": "user", "content": [
      {"type": "tool_result", "tool_use_id": "toolu_123", "content": "...file contents..."}
    ]}
  ],
  "tools": [
    {"name": "Read", "description": "...", "input_schema": {...}},
    {"name": "Write", "description": "...", "input_schema": {...}}
  ],
  "stream": true
}
```

### 2.3 Required Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | Model ID (e.g., `claude-sonnet-4-5-20250929`) |
| `messages` | array | Input messages with alternating user/assistant turns |
| `max_tokens` | integer | Maximum tokens to generate |

### 2.4 Optional Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `system` | string or array | - | System prompt (string or array with cache_control) |
| `tools` | array | - | Tool definitions |
| `temperature` | float | 1.0 | Randomness (0.0-1.0) |
| `stream` | boolean | false | Enable SSE streaming |
| `thinking` | object | - | Extended thinking configuration |
| `metadata` | object | - | Request metadata |

### 2.5 Response Format (Non-Streaming)

```json
{
  "id": "msg_01XFDUDYJgAACzvnptvVoYEL",
  "type": "message",
  "role": "assistant",
  "content": [
    {"type": "text", "text": "Here's the fix..."}
  ],
  "model": "claude-sonnet-4-5-20250929",
  "stop_reason": "end_turn",
  "usage": {
    "input_tokens": 1523,
    "output_tokens": 256
  }
}
```

### 2.6 Count Tokens Endpoint

```
POST https://api.anthropic.com/v1/messages/count_tokens
```

Count tokens without generating a response:

```python
import anthropic
client = anthropic.Anthropic()

response = client.messages.count_tokens(
    model="claude-sonnet-4-5-20250929",
    system="You are a scientist",
    messages=[{
        "role": "user",
        "content": "Hello, Claude"
    }],
)
print(response.input_tokens)
```

**Key Points**:
- Free to use but subject to rate limits
- Separate rate limits from message creation
- Supports PDFs, images, and tools
- Token counts may include system-added tokens (not billed)

---

## 3. Message Format

### 3.1 System Prompt

**String Format** (simple):
```json
{
  "system": "You are a helpful assistant."
}
```

**Array Format** (with cache_control):
```json
{
  "system": [
    {
      "type": "text",
      "text": "You are Claude Code...",
      "cache_control": {"type": "ephemeral", "ttl": "5m"}
    },
    {
      "type": "text",
      "text": "Tool definitions...",
      "cache_control": {"type": "ephemeral", "ttl": "1h"}
    }
  ]
}
```

### 3.2 Messages Array Structure

Messages must alternate between `user` and `assistant` roles:

```json
{
  "messages": [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "How are you?"},
    {"role": "assistant", "content": "I'm doing well, thank you!"}
  ]
}
```

**Important**: Consecutive turns of the same role are automatically combined.

### 3.3 Content Block Types

#### Text Block
```json
{"type": "text", "text": "Hello, world!"}
```

#### Thinking Block (Extended Thinking)
```json
{"type": "thinking", "thinking": "Let me analyze this problem..."}
```

#### Tool Use Block
```json
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lgs",
  "name": "Read",
  "input": {"file_path": "/src/main.py"}
}
```

#### Tool Result Block
```json
{
  "type": "tool_result",
  "tool_use_id": "toolu_01A09q90qw90lq917835lgs",
  "content": "File contents here...",
  "is_error": false
}
```

### 3.4 Role Field Values

| Role | Description |
|------|-------------|
| `user` | Human or tool results |
| `assistant` | Claude's responses |

**Note**: There is no `system` role in messages - use the top-level `system` parameter.

---

## 4. Streaming Protocol (SSE)

### 4.1 Overview

Anthropic uses Server-Sent Events (SSE) for streaming. Response includes:
- `content-type: text/event-stream` header
- Events separated by `\r\n\r\n`
- Each event has `event:` and `data:` lines

### 4.2 Event Types

| Event Type | Description |
|------------|-------------|
| `message_start` | Initial message metadata |
| `content_block_start` | Start of a content block |
| `content_block_delta` | Incremental content update |
| `content_block_stop` | End of a content block |
| `message_delta` | Final message updates (stop_reason, usage) |
| `message_stop` | End of message stream |
| `ping` | Keep-alive (may appear throughout) |

### 4.3 Complete Event Sequence Example

```
event: message_start
data: {"type":"message_start","message":{"id":"msg_...","type":"message","role":"assistant","content":[],"model":"claude-sonnet-4-5-20250929","usage":{"input_tokens":25,"output_tokens":0}}}

event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" world"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"end_turn"},"usage":{"output_tokens":5}}

event: message_stop
data: {"type":"message_stop"}
```

### 4.4 Delta Types

| Delta Type | Used For |
|------------|----------|
| `text_delta` | Text content streaming |
| `input_json_delta` | Tool input streaming (partial JSON) |
| `thinking_delta` | Thinking content streaming |
| `signature_delta` | Thinking block signature (verification) |

### 4.5 Tool Use Streaming

```
event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_...","name":"Read","input":{}}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"{\"file_path\":"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"\"/src/main.py\"}"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"tool_use"},"usage":{"output_tokens":25}}

event: message_stop
data: {"type":"message_stop"}
```

**Note**: Tool use deltas are partial JSON strings. Accumulate them and parse JSON after `content_block_stop`.

### 4.6 Thinking Block Streaming

```
event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"thinking","thinking":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"thinking_delta","thinking":"Let me analyze..."}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"signature_delta","signature":"..."}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: content_block_start
data: {"type":"content_block_start","index":1,"content_block":{"type":"text","text":""}}
... (text content follows)
```

---

## 5. Tool Use

### 5.1 Tool Definitions

```json
{
  "tools": [
    {
      "name": "Read",
      "description": "Reads a file from the filesystem",
      "input_schema": {
        "type": "object",
        "properties": {
          "file_path": {
            "type": "string",
            "description": "Absolute path to the file"
          }
        },
        "required": ["file_path"]
      }
    }
  ]
}
```

### 5.2 Tool Use Content Blocks

When Claude wants to use a tool, it returns a `tool_use` content block:

```json
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lgs",
  "name": "Read",
  "input": {
    "file_path": "/src/main.py"
  }
}
```

### 5.3 Tool Result Content Blocks

After executing the tool, send the result:

```json
{
  "role": "user",
  "content": [
    {
      "type": "tool_result",
      "tool_use_id": "toolu_01A09q90qw90lq917835lgs",
      "content": "def main():\n    print('Hello')\n",
      "is_error": false
    }
  ]
}
```

For errors:
```json
{
  "type": "tool_result",
  "tool_use_id": "toolu_01A09q90qw90lq917835lgs",
  "content": "Error: File not found",
  "is_error": true
}
```

### 5.4 Stop Reason: tool_use

When Claude wants to use tools, the response has:
```json
{
  "stop_reason": "tool_use"
}
```

Continue the conversation by sending tool results in the next user message.

### 5.5 Server Tools

Server tools execute on Anthropic's servers:
- `web_search_20250305`
- `web_fetch_20250305`
- `text_editor_20250124`

### 5.6 Advanced Tool Use Features (Beta)

Add header `anthropic-beta: advanced-tool-use-2025-11-20` to enable:

1. **Tool Search Tool**: Access thousands of tools without consuming context
2. **Programmatic Tool Calling**: Invoke tools in code execution environment
3. **Tool Use Examples**: Demonstrate effective tool usage

### 5.7 Fine-Grained Tool Streaming

Add header `anthropic-beta: fine-grained-tool-streaming-2025-05-14` to stream tool parameters without buffering, reducing latency for large parameters.

---

## 6. Extended Thinking

### 6.1 What is Extended Thinking

Extended thinking allows Claude to reason more deeply about complex problems. When enabled, Claude produces `thinking` blocks that show its reasoning process.

### 6.2 Configuration

```json
{
  "model": "claude-sonnet-4-5-20250929",
  "max_tokens": 16000,
  "thinking": {
    "type": "enabled",
    "budget_tokens": 10000
  },
  "messages": [...]
}
```

### 6.3 Budget Tokens

- `budget_tokens`: Maximum tokens for internal reasoning
- Minimum: 1,024 tokens
- Can exceed `max_tokens` when using interleaved thinking
- Start at minimum and increase incrementally

### 6.4 Thinking Blocks in Content

```json
{
  "role": "assistant",
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me break down this problem:\n1. First, I need to...\n2. Then I should...",
      "signature": "..."
    },
    {
      "type": "text",
      "text": "Based on my analysis, here's the solution..."
    }
  ]
}
```

### 6.5 Interleaved Thinking (Beta)

Add header `anthropic-beta: interleaved-thinking-2025-05-14` to enable thinking blocks between tool uses.

**Benefits**:
- "Think-Act-Think-Act" loop
- Reduces hallucination and error propagation
- Better for long, multi-step coding tasks

**Supported Models**: Claude Opus 4.1, Opus 4, Sonnet 4 (on Anthropic API)

### 6.6 Pricing

Extended thinking tokens are billed as **output tokens**, not as a separate tier.

---

## 7. Prompt Caching

### 7.1 cache_control Field

```json
{
  "system": [
    {
      "type": "text",
      "text": "Long system prompt...",
      "cache_control": {
        "type": "ephemeral",
        "ttl": "5m"
      }
    }
  ]
}
```

### 7.2 TTL Options

| TTL | Description | Pricing Multiplier |
|-----|-------------|-------------------|
| `5m` | 5 minutes (default) | 1.25x base input |
| `1h` | 1 hour (extended) | 2x base input |

**Cache read tokens**: 0.1x base input price

### 7.3 Extended TTL Support

Supported models for 1-hour TTL:
- Claude Opus 4.5
- Claude Sonnet 4.5
- Claude Haiku 4.5

**Required header** (auto-added by SDKs):
```
anthropic-beta: extended-cache-ttl-2025-04-11
```

### 7.4 How Caching Works

1. Cache works on **prefixes** - content at the start of the request
2. TTL resets with each cache hit
3. Longer TTL entries must appear before shorter ones
4. Cache is isolated per workspace (as of February 5, 2026)

### 7.5 Cache Order

Anthropic caches prefixes in this order:
```
tools → system → messages
```

Two requests with the same tools + system but different messages share the prefix cache.

---

## 8. Authentication and Headers

### 8.1 Required Headers

```
x-api-key: sk-ant-api03-...
anthropic-version: 2023-06-01
content-type: application/json
```

### 8.2 Alternative Authentication

```
Authorization: Bearer sk-ant-api03-...
```

### 8.3 Beta Headers

| Header | Feature |
|--------|---------|
| `anthropic-beta: interleaved-thinking-2025-05-14` | Interleaved thinking |
| `anthropic-beta: extended-cache-ttl-2025-04-11` | Extended cache TTL |
| `anthropic-beta: fine-grained-tool-streaming-2025-05-14` | Fine-grained tool streaming |
| `anthropic-beta: advanced-tool-use-2025-11-20` | Advanced tool features |
| `anthropic-beta: context-management-2025-06-27` | Automatic tool call clearing |
| `anthropic-beta: context-1m-2025-08-07` | 1M token context (Sonnet 4.5) |

Multiple beta features can be combined:
```
anthropic-beta: interleaved-thinking-2025-05-14,extended-cache-ttl-2025-04-11
```

---

## 9. Error Handling

### 9.1 Error Response Format

```json
{
  "type": "error",
  "error": {
    "type": "invalid_request_error",
    "message": "Invalid request: messages must alternate between user and assistant"
  }
}
```

### 9.2 Error Types and HTTP Status Codes

| HTTP Status | Error Type | Description |
|-------------|------------|-------------|
| 400 | `invalid_request_error` | Malformed request |
| 401 | `authentication_error` | Invalid/missing API key |
| 403 | `permission_denied_error` | Access forbidden |
| 404 | `not_found_error` | Resource not found |
| 413 | `request_too_large` | Request exceeds size limit |
| 422 | `unprocessable_entity` | Semantic validation failed |
| 429 | `rate_limit_error` | Too many requests |
| 500 | `api_error` | Internal server error |
| 529 | `overloaded_error` | Server temporarily overloaded |

### 9.3 Python SDK Exception Classes

```python
import anthropic

try:
    response = client.messages.create(...)
except anthropic.BadRequestError as e:
    print(f"Bad request (400): {e}")
except anthropic.AuthenticationError as e:
    print(f"Auth failed (401): {e}")
except anthropic.PermissionDeniedError as e:
    print(f"Permission denied (403): {e}")
except anthropic.NotFoundError as e:
    print(f"Not found (404): {e}")
except anthropic.UnprocessableEntityError as e:
    print(f"Unprocessable (422): {e}")
except anthropic.RateLimitError as e:
    print(f"Rate limited (429): {e}")
except anthropic.InternalServerError as e:
    if e.status_code == 529:
        print(f"Overloaded (529): {e}")
    else:
        print(f"Internal error (500): {e}")
except anthropic.APIConnectionError as e:
    print(f"Connection error: {e}")
```

### 9.4 Retry Behavior

For 429 and 529 errors:
1. Implement exponential backoff
2. Check `retry-after` header if present
3. Start with 1-2 second delay, double on each retry
4. Max 5-10 retries before failing

### 9.5 Streaming Errors

Errors can occur after a 200 response during streaming. Handle by checking for `error` events in the stream.

---

## 10. Claude Models (2025-2026)

### 10.1 Claude Opus 4.5

**Model ID**: `claude-opus-4-5-20251101`

| Specification | Value |
|--------------|-------|
| Context Window | 200K tokens |
| Max Output | 64K tokens |
| Knowledge Cutoff | March 2025 |
| Release Date | November 24, 2025 |

**Unique Features**:
- Supports `effort` parameter for response depth control
- Most intelligent model in the Claude family
- Best for complex reasoning and analysis

**Pricing**: $5 / $25 per million tokens (input/output)

### 10.2 Claude Sonnet 4.5

**Model ID**: `claude-sonnet-4-5-20250929`

| Specification | Value |
|--------------|-------|
| Context Window | 200K tokens (1M with beta header) |
| Max Output | 64K tokens |
| Knowledge Cutoff | January 2025 (trained through July 2025) |
| Release Date | September 29, 2025 |

**Unique Features**:
- 1M token context with `context-1m-2025-08-07` beta header
- Interleaved thinking support
- Best balance of intelligence and speed

**Pricing**: $3 / $15 per million tokens (input/output)

### 10.3 Claude Haiku 4.5

**Model ID**: `claude-haiku-4-5-20251001`

| Specification | Value |
|--------------|-------|
| Context Window | 200K tokens |
| Max Output | 64K tokens |
| Knowledge Cutoff | February 2025 |
| Release Date | October 15, 2025 |

**Unique Features**:
- Fastest model in Claude family
- First Haiku with extended thinking
- Computer use capabilities
- Most cost-effective

**Pricing**: $1 / $5 per million tokens (input/output)

### 10.4 Model Comparison

| Feature | Opus 4.5 | Sonnet 4.5 | Haiku 4.5 |
|---------|----------|------------|-----------|
| Context Window | 200K | 200K (1M beta) | 200K |
| Max Output | 64K | 64K | 64K |
| Extended Thinking | Yes | Yes | Yes |
| Interleaved Thinking | Yes* | Yes | No |
| 1-Hour Cache TTL | Yes | Yes | Yes |
| Computer Use | Yes | Yes | Yes |
| Effort Parameter | Yes | No | No |

*Via Opus 4.1

### 10.5 Model Selection Guide

| Use Case | Recommended Model |
|----------|------------------|
| Complex reasoning | Opus 4.5 |
| Coding/agents | Sonnet 4.5 |
| High-volume tasks | Haiku 4.5 |
| Cost-sensitive | Haiku 4.5 |
| Production quality | Sonnet 4.5 |
| Research/analysis | Opus 4.5 |

---

## 11. Python SDK (anthropic)

### 11.1 Installation

```bash
pip install anthropic
```

Current version: 0.76.0+ (as of January 2026)

### 11.2 Client Initialization

```python
from anthropic import Anthropic, AsyncAnthropic

# Sync client
client = Anthropic(api_key="sk-ant-api03-...")

# Async client
async_client = AsyncAnthropic(api_key="sk-ant-api03-...")

# From environment variable (ANTHROPIC_API_KEY)
client = Anthropic()
```

### 11.3 Basic messages.create()

```python
from anthropic import Anthropic

client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude!"}
    ]
)

print(response.content[0].text)
```

### 11.4 With System Prompt and Tools

```python
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    system="You are a helpful coding assistant.",
    messages=[
        {"role": "user", "content": "Read the main.py file"}
    ],
    tools=[
        {
            "name": "Read",
            "description": "Read a file from the filesystem",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {"type": "string"}
                },
                "required": ["file_path"]
            }
        }
    ]
)
```

### 11.5 Streaming with stream=True

```python
stream = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello, Claude"}],
    stream=True,
)

for event in stream:
    if event.type == "content_block_delta":
        if event.delta.type == "text_delta":
            print(event.delta.text, end="", flush=True)
```

### 11.6 Using messages.stream() Helper

```python
with client.messages.stream(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}],
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### 11.7 Async Streaming

```python
from anthropic import AsyncAnthropic

client = AsyncAnthropic()

async def main():
    async with client.messages.stream(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}],
    ) as stream:
        async for event in stream:
            if event.type == "text":
                print(event.text, end="", flush=True)
```

### 11.8 Extended Thinking

```python
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[
        {"role": "user", "content": "Solve this complex problem..."}
    ]
)

for block in response.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Response: {block.text}")
```

### 11.9 With Beta Headers

```python
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[...],
    extra_headers={
        "anthropic-beta": "interleaved-thinking-2025-05-14,extended-cache-ttl-2025-04-11"
    }
)
```

### 11.10 Error Handling

```python
import anthropic

client = anthropic.Anthropic()

try:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
except anthropic.APIConnectionError as e:
    print(f"Connection error: {e}")
except anthropic.RateLimitError as e:
    print(f"Rate limited: {e}")
    # Implement backoff
except anthropic.APIStatusError as e:
    print(f"API error {e.status_code}: {e.message}")
```

---

## 12. Best Practices

### 12.1 Token Counting Before Requests

Always count tokens for long inputs:

```python
count = client.messages.count_tokens(
    model="claude-sonnet-4-5-20250929",
    messages=messages,
    system=system_prompt,
    tools=tools
)

if count.input_tokens > 150000:
    # Truncate or split the request
    pass
```

### 12.2 Prompt Caching Optimization

1. **Put stable content first**: Tools → System → Messages
2. **Use appropriate TTL**:
   - 5m for frequently changing content
   - 1h for stable system prompts
3. **Order by TTL**: 1-hour entries before 5-minute entries
4. **Cache large content**: System prompts, tool definitions, examples

```python
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": large_stable_prompt,
            "cache_control": {"type": "ephemeral", "ttl": "1h"}
        },
        {
            "type": "text",
            "text": dynamic_context,
            "cache_control": {"type": "ephemeral", "ttl": "5m"}
        }
    ],
    messages=[...]
)
```

### 12.3 Handling Long Conversations

1. **Summarize periodically**: Compress old messages
2. **Sliding window**: Keep only recent N messages
3. **Use `/clear` in Claude Code**: Reset context between tasks
4. **Monitor token usage**: Track input_tokens in responses

### 12.4 Tool Use Patterns

1. **Clear error messages**: Help Claude understand failures
2. **Structured schemas**: Use detailed input_schema with descriptions
3. **Stream tool inputs**: Use fine-grained streaming for large inputs
4. **Batch related tools**: Define tools that work together

### 12.5 Error Handling Patterns

```python
import time
import anthropic

def call_with_retry(client, max_retries=5, **kwargs):
    for attempt in range(max_retries):
        try:
            return client.messages.create(**kwargs)
        except anthropic.RateLimitError as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt
            time.sleep(wait_time)
        except anthropic.InternalServerError as e:
            if e.status_code == 529 and attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise
```

### 12.6 Temperature Settings

| Use Case | Temperature |
|----------|-------------|
| Structured output (JSON) | 0.0 - 0.3 |
| Code generation | 0.0 - 0.5 |
| General tasks | 0.7 - 1.0 |
| Creative writing | 0.8 - 1.0 |
| Dataset generation | 0.8 |

### 12.7 Security Best Practices

1. **Never hardcode API keys**: Use environment variables
2. **Store keys securely**: Use secrets management
3. **Rotate keys periodically**: Update keys regularly
4. **Monitor usage**: Watch for anomalies
5. **Use workspace isolation**: Separate development/production

```python
# Good
import os
client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

# Bad - Never do this
client = Anthropic(api_key="sk-ant-api03-...")
```

---

## References

### Official Documentation
- [Claude Code Documentation](https://code.claude.com/docs/en/overview)
- [Anthropic Messages API](https://docs.anthropic.com/en/api/messages)
- [Streaming Messages](https://docs.anthropic.com/en/api/messages-streaming)
- [Prompt Caching](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
- [Extended Thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
- [Tool Use](https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview)
- [Token Counting](https://platform.claude.com/docs/en/build-with-claude/token-counting)
- [Errors](https://docs.anthropic.com/en/api/errors)

### GitHub Resources
- [Claude Code Repository](https://github.com/anthropics/claude-code)
- [Anthropic Python SDK](https://github.com/anthropics/anthropic-sdk-python)
- [Anthropic TypeScript SDK](https://github.com/anthropics/anthropic-sdk-typescript)

### Additional Resources
- [Claude Code Best Practices](https://www.anthropic.com/engineering/claude-code-best-practices)
- [Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview)
- [API Console](https://console.anthropic.com/)
- [Anthropic Status](https://status.anthropic.com/)

---

## Changelog

### Version 1.0.0 (2026-01-25)
- Initial comprehensive guide
- Covers Claude Code CLI v2.0.76+
- Anthropic SDK v0.76.0+
- Claude 4.5 model family (Opus, Sonnet, Haiku)
