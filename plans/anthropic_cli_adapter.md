# Multi-Protocol API Adapter: Agent Integration Plan

## Executive Summary

A **multi-protocol adapter layer** that maps both stateless (Anthropic Messages API) and stateful (OpenAI-compatible, direct) API requests to our block-pool agent backend. The adapter supports two agent identification strategies:

1. **Content-based** (Anthropic/Claude Code): Token prefix hashing for automatic cache matching without client changes
2. **Explicit agent_id** (non-Anthropic): Client passes `session_id` or agent identifier directly in the request (simpler, preferred for custom systems)

The system is **model-agnostic**: the `model` field in requests determines which model is loaded (hot-swap if needed), while agent state persists independently of model choice.

**Key Insight**: Claude Code CLI sends the FULL conversation history on every request (content-based identification needed). Non-Anthropic systems can simply pass an agent identifier (much better for later systems running locally).

---

## 1. Claude Code CLI Protocol Analysis

### 1.1 How Claude Code Actually Works

From extensive research into the Claude Code CLI protocol:

1. **Standard `/v1/messages` endpoint** - NOT a special Agent SDK endpoint
2. **Session management is CLIENT-SIDE** - stored in `~/.claude/` as SQLite + JSONL
3. **Session IDs are UUIDs** - generated by client, NOT sent to server
4. **Full conversation history sent each time** - every request includes all prior messages
5. **System prompt is ~18KB** - includes tool definitions, CLAUDE.md, environment info
6. **Subagents spawn separate `/v1/messages` calls** - different system prompts, different histories

### 1.2 Request Format

```json
{
  "model": "claude-sonnet-4-5-20250929",
  "max_tokens": 8192,
  "temperature": 1.0,
  "system": [
    {
      "type": "text",
      "text": "You are Claude Code, Anthropic's official CLI...",
      "cache_control": {"type": "ephemeral"}
    },
    {
      "type": "text",
      "text": "# Tool definitions\n...",
      "cache_control": {"type": "ephemeral"}
    }
  ],
  "messages": [
    {"role": "user", "content": "Help me fix the bug"},
    {"role": "assistant", "content": [
      {"type": "thinking", "thinking": "Let me analyze..."},
      {"type": "text", "text": "I'll look at the code."},
      {"type": "tool_use", "id": "toolu_123", "name": "Read", "input": {"file_path": "/src/main.py"}}
    ]},
    {"role": "user", "content": [
      {"type": "tool_result", "tool_use_id": "toolu_123", "content": "...file contents..."}
    ]},
    {"role": "assistant", "content": [
      {"type": "text", "text": "I see the issue..."}
    ]},
    {"role": "user", "content": "Now fix it"}
  ],
  "tools": [
    {"name": "Read", "description": "...", "input_schema": {...}},
    {"name": "Write", "description": "...", "input_schema": {...}},
    {"name": "Bash", "description": "...", "input_schema": {...}}
  ],
  "stream": true
}
```

### 1.3 Key Protocol Features

| Feature | Format | Adapter Impact |
|---------|--------|----------------|
| System prompt | Array of text blocks with `cache_control` | Stable prefix for agent identification |
| Messages | Alternating user/assistant, growing list | Prefix grows monotonically per session |
| Tool definitions | Array in `tools` field | Part of cacheable prefix |
| Thinking blocks | `{"type": "thinking", ...}` in content | Must be preserved across turns |
| Tool use | `{"type": "tool_use", ...}` in assistant content | Triggers tool_result in next user message |
| Streaming | SSE: message_start → content_block_delta → message_stop | Per-token delivery |
| Extended thinking | `anthropic-beta: interleaved-thinking-2025-05-14` | Budget-aware token counting |
| Cache TTL | `cache_control: {"type": "ephemeral", "ttl": "1h"}` | Maps to our persistence lifetime |
| Count tokens | `POST /v1/messages/count_tokens` | Must support before send |

### 1.4 Headers Required

```
x-api-key: sk-ant-...          (or Authorization: Bearer sk-ant-...)
anthropic-version: 2023-06-01
anthropic-beta: extended-cache-ttl-2025-04-11  (optional)
content-type: application/json
```

---

## 2. Content-Based Agent Identification

### 2.1 The Problem

Claude Code sends no session ID or agent ID to the server. We must identify which agent (KV cache) a request belongs to using only the request content.

### 2.2 The Solution: Token Prefix Hashing

Successive requests from the same Claude Code session share a growing prefix:

```
Request 1: [system + tools + msg1]
Request 2: [system + tools + msg1 + assistant1 + msg2]
Request 3: [system + tools + msg1 + assistant1 + msg2 + assistant2 + msg3]
```

The system prompt + tools + early messages form a STABLE PREFIX that identifies the session.

### 2.3 Cache Key Generation

```python
def compute_cache_key(request: MessagesRequest) -> CacheKey:
    """
    Compute content-based cache key for agent identification.

    Strategy: Hash the token sequence of the full request prefix.
    Two requests with the same prefix will map to the same cached agent.

    For efficiency, we hash in blocks of 256 tokens (matching our block size).
    This enables partial prefix matching: if Request 2 extends Request 1's
    prefix, we can reuse Request 1's cache and only process the new tokens.
    """
    # Tokenize the full input (system + tools + messages)
    tokens = tokenize_request(request)

    # Compute block-aligned prefix hash
    # Each 256-token block gets its own hash
    block_hashes = []
    for i in range(0, len(tokens), BLOCK_SIZE):
        block = tokens[i:i+BLOCK_SIZE]
        block_hash = hashlib.sha256(
            mx.array(block).tobytes()
        ).hexdigest()[:16]
        block_hashes.append(block_hash)

    return CacheKey(
        full_hash=hashlib.sha256(
            "|".join(block_hashes).encode()
        ).hexdigest()[:24],
        block_hashes=block_hashes,
        total_tokens=len(tokens)
    )
```

### 2.4 Prefix Matching Algorithm

```python
def find_cached_agent(cache_key: CacheKey) -> Tuple[Optional[str], List[int]]:
    """
    Find the best matching cached agent for this request.

    Returns:
        (agent_id, remaining_tokens): agent to resume from, tokens still to process

    Strategy (from mlx-lm CacheManager trie):
    1. Exact match: all block hashes match → resume from full cache
    2. Prefix match: first N block hashes match → resume from block N, process rest
    3. No match: new agent, process all tokens
    """
    # Check trie for longest prefix match
    result = cache_store.trie_search(cache_key.block_hashes)

    if result.exact_match:
        # Same request repeated (e.g., retry) → use full cache
        return result.agent_id, []

    elif result.prefix_length > 0:
        # Extending a previous conversation → reuse prefix blocks
        prefix_tokens = result.prefix_length * BLOCK_SIZE
        remaining = tokens[prefix_tokens:]
        return result.agent_id, remaining

    else:
        # New session → create new agent
        agent_id = generate_agent_id()
        return agent_id, tokens
```

### 2.5 Subagent Identification

Claude Code spawns subagents (Task, Explore, Plan) with DIFFERENT system prompts. These automatically become separate agents because their token prefix differs at the system prompt level:

```
Main agent:   hash([main_system_prompt + tools + ...])  → agent_abc123
Task subagent: hash([task_system_prompt + tools + ...]) → agent_def456
```

No special handling needed - content-based hashing naturally separates them.

---

## 3. Request Processing Pipeline

### 3.1 Full Pipeline

```
Claude Code CLI
    │
    ▼ POST /v1/messages
┌─────────────────────────────────────────────────────┐
│  1. Parse Request                                    │
│     - Validate headers (anthropic-version, x-api-key)│
│     - Parse MessagesRequest body                     │
│     - Extract system, tools, messages, stream flag   │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  2. Tokenize & Compute Cache Key                     │
│     - Serialize: tools → system → messages (order!)  │
│     - Tokenize full sequence                         │
│     - Compute block-aligned hash                     │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  3. Cache Lookup (Trie + Disk)                       │
│     - Search trie for prefix match                   │
│     - If miss: check disk for saved agent            │
│     - If cold miss: create new agent                 │
│     - Determine: reusable blocks + remaining tokens  │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  4. Block Pool Allocation                            │
│     - Load agent blocks from pool (or disk)          │
│     - Allocate new blocks for remaining tokens       │
│     - Adjust batch size if memory tight              │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  5. Generation (BlockPoolBatchEngine)                │
│     - Prefill remaining tokens (extend cache)        │
│     - Decode up to max_tokens                        │
│     - Extract updated blocks on completion           │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  6. Cache Update                                     │
│     - Update trie with new cache key                 │
│     - Store updated blocks in agent store            │
│     - Persist to disk if configured                  │
└────────────────────┬────────────────────────────────┘
                     ▼
┌─────────────────────────────────────────────────────┐
│  7. Response (Streaming or Non-Streaming)            │
│     - Format as Anthropic MessagesResponse           │
│     - SSE events if stream=true                      │
│     - Include usage (input_tokens, output_tokens)    │
└─────────────────────────────────────────────────────┘
```

### 3.2 Tokenization Order

The Anthropic API caches prefixes in this order:
```
tools → system → messages
```

Our tokenization MUST match this order so that cache keys align with how Anthropic's own caching would work. This means two requests with the same tools + system but different messages will share the tools+system prefix cache.

### 3.3 Handling Thinking Blocks

Extended thinking adds complexity:
- Thinking blocks must be preserved in the message history
- They count toward input tokens but NOT context window
- Our tokenizer must handle thinking block format
- Cache key includes thinking block content (it's part of the prefix)

```python
def tokenize_content_block(block: dict) -> List[int]:
    """Tokenize a single content block."""
    if block["type"] == "text":
        return tokenizer.encode(block["text"])
    elif block["type"] == "thinking":
        # Thinking blocks are part of the prefix
        return tokenizer.encode(f"<thinking>{block['thinking']}</thinking>")
    elif block["type"] == "tool_use":
        return tokenizer.encode(
            f"<tool_use name='{block['name']}'>{json.dumps(block['input'])}</tool_use>"
        )
    elif block["type"] == "tool_result":
        return tokenizer.encode(
            f"<tool_result id='{block['tool_use_id']}'>{block['content']}</tool_result>"
        )
```

---

## 4. Streaming Response Format

### 4.1 SSE Event Sequence

Claude Code expects this exact sequence:

```
event: message_start
data: {"type":"message_start","message":{"id":"msg_...","type":"message","role":"assistant","content":[],"model":"...","usage":{"input_tokens":N,"output_tokens":0}}}

event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":" world"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"end_turn"},"usage":{"output_tokens":M}}

event: message_stop
data: {"type":"message_stop"}
```

### 4.2 Tool Use Streaming

When the model wants to use a tool:

```
event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_...","name":"Read","input":{}}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"{\"file_path\":"}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"\"/src/main.py\"}"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"tool_use"},"usage":{"output_tokens":M}}
```

### 4.3 Thinking Block Streaming (Extended Thinking)

```
event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"thinking","thinking":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"thinking_delta","thinking":"Let me analyze..."}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}

event: content_block_start
data: {"type":"content_block_start","index":1,"content_block":{"type":"text","text":""}}
... (text content follows)
```

### 4.4 Current Limitation and Solution

**Current**: The API server generates the full response then simulates streaming by chunking.

**Target**: True per-token streaming from the batch engine:
```python
async def stream_from_engine(self, agent_id, prompt, max_tokens):
    """Yield tokens as they are generated by the batch engine."""
    uid = self.engine.submit(agent_id, prompt, cache, max_tokens)

    async for partial in self.engine.stream(uid):
        yield self._sse_event("content_block_delta", {
            "type": "content_block_delta",
            "index": 0,
            "delta": {"type": "text_delta", "text": partial.token}
        })
```

---

## 5. Additional Endpoints

### 5.1 Token Counting

```
POST /v1/messages/count_tokens
```

Request body: same as `/v1/messages` but returns token count instead of generating.

```python
@app.post("/v1/messages/count_tokens")
async def count_tokens(request: MessagesRequest):
    """Count tokens without generating."""
    tokens = tokenize_request(request)
    return {"input_tokens": len(tokens)}
```

### 5.2 Health Check

```
GET /health
```

Returns server status, model info, active agents, pool utilization.

### 5.3 Models List (Optional)

```
GET /v1/models
```

Returns available models (for compatibility with clients that check).

---

## 6. Cache Lifecycle Management

### 6.1 Cache TTL Mapping

Anthropic's `cache_control` hints map to our persistence policy:

| Anthropic TTL | Our Behavior |
|---------------|--------------|
| `"ephemeral"` (default 5min) | Keep in pool for 5 minutes after last access |
| `"1h"` (extended) | Keep in pool for 1 hour after last access |
| No cache_control | Persist to disk, evict from pool on LRU basis |

### 6.2 Eviction Policy

Three-tier cache lifecycle:

```
HOT (in-pool blocks)     WARM (on-disk safetensors)     COLD (deleted)
     ──LRU eviction──►        ──TTL expiry──►
     ◄──disk reload───
```

- **HOT → WARM**: When pool is full, evict LRU agent's blocks to disk
- **WARM → HOT**: When request matches a disk-cached agent, reload blocks
- **WARM → COLD**: If agent hasn't been accessed beyond configured TTL (default: 24h)
- **HOT ← HOT**: Access refreshes LRU position

### 6.3 Prefix Sharing

Multiple Claude Code sessions with the same system prompt share the prefix cache:

```
Session 1: [system_prompt(18KB) + tools + msg_A1 + ...]
Session 2: [system_prompt(18KB) + tools + msg_B1 + ...]
                                    ↑
                          Shared prefix blocks
                    (reference counted, not duplicated)
```

The system prompt + tools typically tokenize to ~4K tokens = ~16 blocks. Sharing these blocks across sessions saves significant memory.

---

## 7. Compatibility Requirements

### 7.1 Must Support (Claude Code Critical Path)

| Feature | Priority | Notes |
|---------|----------|-------|
| `POST /v1/messages` (non-streaming) | P0 | Basic generation |
| `POST /v1/messages` (streaming SSE) | P0 | Claude Code always streams |
| System prompt (string or array) | P0 | Array with cache_control blocks |
| Tool definitions (`tools` array) | P0 | 18+ tool definitions |
| Multi-turn messages | P0 | Growing conversation history |
| `tool_use` content blocks | P0 | Model requesting tool execution |
| `tool_result` content blocks | P0 | Tool output back to model |
| `stop_reason: "end_turn"` | P0 | Normal completion |
| `stop_reason: "tool_use"` | P0 | Tool call requested |
| Usage counting (input/output tokens) | P0 | Claude Code tracks usage |
| `x-api-key` header | P1 | Auth (can be dummy for local) |
| `anthropic-version` header | P1 | Version compatibility |

### 7.2 Should Support (Enhanced Experience)

| Feature | Priority | Notes |
|---------|----------|-------|
| `POST /v1/messages/count_tokens` | P1 | Pre-flight token counting |
| Extended thinking (`thinking` blocks) | P1 | For supported models |
| `cache_control` on system blocks | P2 | Optimize cache behavior |
| `anthropic-beta` headers | P2 | Feature flags |
| `stop_reason: "max_tokens"` | P1 | Length limit hit |
| Multiple content blocks per message | P1 | Mixed text + tool_use |

### 7.3 Not Required (Server-Only, No Client Dependency)

| Feature | Notes |
|---------|-------|
| Batch API (`/v1/messages/batches`) | Not used by Claude Code CLI |
| Image/PDF content blocks | Model-dependent, Gemma 3 text-only |
| `top_p`, `top_k` sampling | Can default to temperature-only |
| `metadata` field | Ignored |

---

## 8. Request Serialization for Cache Keys

### 8.1 Canonical Serialization Order

To compute stable cache keys, serialize in Anthropic's cache prefix order:

```python
def serialize_for_cache_key(request: MessagesRequest) -> str:
    """
    Serialize request into canonical form for cache key computation.

    Order: tools → system → messages
    This matches Anthropic's prompt caching prefix order.
    """
    parts = []

    # 1. Tools (stable across conversation)
    if request.tools:
        for tool in sorted(request.tools, key=lambda t: t["name"]):
            parts.append(f"TOOL:{tool['name']}:{json.dumps(tool['input_schema'], sort_keys=True)}")

    # 2. System prompt (stable across conversation)
    if isinstance(request.system, list):
        for block in request.system:
            parts.append(f"SYSTEM:{block['text']}")
    elif request.system:
        parts.append(f"SYSTEM:{request.system}")

    # 3. Messages (grows each turn)
    for msg in request.messages:
        if isinstance(msg.content, str):
            parts.append(f"{msg.role.upper()}:{msg.content}")
        elif isinstance(msg.content, list):
            for block in msg.content:
                parts.append(f"{msg.role.upper()}:{block['type']}:{serialize_block(block)}")

    return "\n".join(parts)
```

### 8.2 Incremental Hashing

For efficiency, we don't re-hash the entire conversation each request. Instead:

```python
class IncrementalCacheKey:
    """Incrementally compute cache keys as conversation grows."""

    def __init__(self):
        self._block_hashes: List[str] = []
        self._current_block_tokens: List[int] = []

    def extend(self, new_tokens: List[int]):
        """Add tokens from new turn, computing block hashes incrementally."""
        self._current_block_tokens.extend(new_tokens)

        while len(self._current_block_tokens) >= BLOCK_SIZE:
            block = self._current_block_tokens[:BLOCK_SIZE]
            self._current_block_tokens = self._current_block_tokens[BLOCK_SIZE:]
            block_hash = hashlib.sha256(bytes(block)).hexdigest()[:16]
            self._block_hashes.append(block_hash)

    @property
    def key(self) -> str:
        return "|".join(self._block_hashes)
```

---

## 9. Error Handling

### 9.1 Error Response Format

Claude Code expects Anthropic-style error responses:

```json
{
  "type": "error",
  "error": {
    "type": "overloaded_error",
    "message": "Server is temporarily overloaded"
  }
}
```

### 9.2 Error Types to Implement

| HTTP Status | Error Type | When |
|-------------|-----------|------|
| 400 | `invalid_request_error` | Malformed request body |
| 401 | `authentication_error` | Invalid/missing API key |
| 429 | `rate_limit_error` | Too many concurrent requests |
| 500 | `api_error` | Internal generation failure |
| 529 | `overloaded_error` | Pool exhausted, all agents busy |

### 9.3 Retry Behavior

Claude Code implements exponential backoff on 429 and 529 errors. Our server should:
- Return 429 when batch is full and queue exceeds max depth
- Return 529 when memory pool is exhausted and can't evict
- Include `retry-after` header with suggested wait time

---

## 10. Security Considerations

### 10.1 Single-User Model

This adapter targets a single user running multiple agents on their own machine:
- **No multi-tenancy**: All agents belong to the same user
- **No API key validation needed**: Can accept any key (or none) for local use
- **No rate limiting needed**: User throttles themselves by launching sessions
- **File system access**: Cache stored in user's home directory with standard permissions

### 10.2 What We DO Protect Against

- **Cache corruption**: Per-agent locks prevent concurrent cache writes
- **Memory exhaustion**: Pool budget prevents OOM
- **Disk bloat**: TTL-based cache expiry prevents unbounded disk growth
- **Port exposure**: Bind to localhost only by default (not 0.0.0.0)

### 10.3 API Key Handling

For local use, accept any API key (or make optional). But if the user configures a key in the server, validate it:

```python
@app.middleware("http")
async def validate_api_key(request, call_next):
    if configured_key:
        provided = request.headers.get("x-api-key") or \
                   request.headers.get("authorization", "").replace("Bearer ", "")
        if provided != configured_key:
            return JSONResponse(status_code=401, content={
                "type": "error",
                "error": {"type": "authentication_error", "message": "Invalid API key"}
            })
    return await call_next(request)
```

---

## 11. Adapter Architecture

### 11.1 Class Design

```python
class AnthropicAdapter:
    """
    Maps Anthropic Messages API requests to block-pool agent backend.

    Responsibilities:
    - Request parsing and validation
    - Content-based agent identification (cache key computation)
    - Token counting and serialization
    - Response formatting (non-streaming and SSE)
    - Error handling and status codes
    """

    def __init__(self, engine: BlockPoolBatchEngine, cache_store: AgentCacheStore):
        self.engine = engine
        self.cache_store = cache_store
        self.tokenizer = engine.tokenizer

    async def handle_messages(self, request: MessagesRequest) -> Union[MessagesResponse, StreamingResponse]:
        # 1. Tokenize and compute cache key
        tokens = self.tokenize_request(request)
        cache_key = self.compute_cache_key(tokens)

        # 2. Find or create agent
        agent_id, remaining_tokens = self.cache_store.find_agent(cache_key)

        # 3. Generate
        if request.stream:
            return StreamingResponse(
                self.stream_generate(agent_id, remaining_tokens, request.max_tokens),
                media_type="text/event-stream"
            )
        else:
            response_text = await self.engine.generate(
                agent_id, remaining_tokens, request.max_tokens
            )
            return self.format_response(response_text, len(tokens))

    def tokenize_request(self, request: MessagesRequest) -> List[int]:
        """Tokenize full request in canonical order: tools → system → messages."""
        ...

    def compute_cache_key(self, tokens: List[int]) -> CacheKey:
        """Block-aligned hash of token sequence."""
        ...
```

### 11.2 Integration with Existing API Server

The adapter replaces the current request handling in `api_server.py`:

```python
# Current (system prompt hash → agent_id):
agent_id = self._system_to_agent_id(system_prompt)  # MD5 of system text only

# New (content-based prefix matching):
tokens = adapter.tokenize_request(request)
cache_key = adapter.compute_cache_key(tokens)
agent_id, remaining = cache_store.find_agent(cache_key)
```

This is a drop-in replacement: the API endpoints stay the same, only the agent identification logic changes.

---

## 12. Implementation Phases

### Phase A: Enhanced Request Parsing
- Support system prompt as array of blocks (not just string)
- Parse content blocks: text, thinking, tool_use, tool_result
- Handle the `tools` array
- Proper token counting

### Phase B: Content-Based Agent Identification
- Replace MD5(system_prompt) with token prefix hashing
- Implement trie-based prefix matching
- Support exact, shorter, and longer prefix matches
- Handle subagent identification (different system prompts)

### Phase C: Full SSE Streaming
- Implement all SSE event types (message_start through message_stop)
- Support tool_use streaming (input_json_delta)
- Support thinking block streaming
- True per-token delivery from batch engine

### Phase D: Additional Endpoints
- `POST /v1/messages/count_tokens`
- `GET /v1/models`
- Error responses matching Anthropic format
- `retry-after` headers

### Phase E: Cache Lifecycle
- Implement TTL-based eviction (5min, 1h, 24h tiers)
- Prefix sharing for common system prompts
- Disk persistence with cache_control hints
- Metrics: cache hit rate, prefix match depth

---

## 13. Expert Debate Notes

### Anthropic Expert
"The `cache_control` markers in the system prompt are hints about what Anthropic considers 'stable' content. The tools and system instructions rarely change within a session - they're the ideal prefix for caching. The messages grow monotonically. This perfectly maps to our trie-based prefix matching."

### System Engineer
"For single-user deployment, we can skip most of the security complexity. The main risk is resource exhaustion - a long-running Claude Code session could accumulate 100K+ tokens of context and exhaust the pool. The adapter must handle this gracefully: either truncate old context or inform the client (via max_tokens reached)."

### LLM Engineer
"The local model (Gemma 3 12B 4-bit) has a much smaller context window than Claude (200K). We need to handle context window overflow: when the conversation history exceeds our model's context limit, we must either (a) summarize and compact, (b) drop old turns, or (c) return an error. Option (b) with the trie is natural: trim the prefix cache to the last N blocks."

### Memory Expert
"Reference counting for shared prefixes is critical. 5 Claude Code sessions with the same system prompt = 5 references to the same ~16 blocks of system prompt cache. When one session ends, decrement the count. Only free when count reaches 0. This saves ~400MB of redundant cache."

### Attention Expert
"The biggest performance win is prefix cache reuse. On a 5-turn conversation, turn 5 only needs to process the NEW tokens (one user message + one tool result). The previous 4 turns are already in the cache. This is exactly what `fetch_nearest_cache` with 'shorter match' gives us. Expected speedup: 80-95% less prefill work on subsequent turns."

---

## 14. Compatibility Testing Plan

### 14.1 Manual Tests

```bash
# Start server
python -m src.api_server

# Test 1: Basic generation
ANTHROPIC_BASE_URL=http://localhost:8000 claude "hello world"

# Test 2: Multi-turn
ANTHROPIC_BASE_URL=http://localhost:8000 claude
> Read /src/main.py
> Fix the bug on line 42
> Run the tests

# Test 3: Concurrent sessions
ANTHROPIC_BASE_URL=http://localhost:8000 claude &  # Session 1
ANTHROPIC_BASE_URL=http://localhost:8000 claude &  # Session 2

# Test 4: Subagent spawning
ANTHROPIC_BASE_URL=http://localhost:8000 claude
> Use the Task tool to search for all Python files
```

### 14.2 Automated Tests

```python
# Test cache reuse across turns
async def test_cache_reuse():
    # Turn 1
    r1 = await client.post("/v1/messages", json={
        "system": "You are helpful.",
        "messages": [{"role": "user", "content": "Hello"}],
        "max_tokens": 50
    })

    # Turn 2 (extends Turn 1's prefix)
    r2 = await client.post("/v1/messages", json={
        "system": "You are helpful.",
        "messages": [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": r1.json()["content"][0]["text"]},
            {"role": "user", "content": "What did I just say?"}
        ],
        "max_tokens": 50
    })

    # Verify Turn 2 used cache from Turn 1 (faster prefill)
    assert r2.status_code == 200
```

---

## 15. Migration from Current System

### 15.1 Backwards Compatibility

The current system uses `MD5(system_prompt)[:12]` as agent_id. The new content-based system will:
- Keep the same API endpoints (no client changes needed)
- Ignore existing disk caches (cache format changes with block pool)
- Automatically create new caches on first request

### 15.2 Migration Steps

1. Deploy new adapter alongside existing code
2. Feature flag: `USE_CONTENT_BASED_CACHE=1` enables new path
3. Run both paths in shadow mode, compare agent identification
4. Once validated, switch default to content-based
5. Remove old MD5 path after transition period

---

---

## 16. Non-Anthropic API: Explicit Agent Identification

### 16.1 The Case for Explicit Agent IDs

For non-Anthropic systems (custom agents, automation scripts, OpenAI-compatible clients), content-based hashing is unnecessary complexity. These clients can simply pass an agent identifier:

**Advantages over content-based**:
- No tokenization needed for identification (faster)
- No trie lookup (O(1) agent resolution)
- Client controls agent lifecycle explicitly
- No risk of hash collision or prefix ambiguity
- Better for later systems where agents have persistent identities

### 16.2 OpenAI-Compatible API with Session Extension

```
POST /v1/chat/completions
```

**Standard OpenAI fields** (unchanged):
```json
{
  "model": "gemma-3-12b-it-4bit",
  "messages": [
    {"role": "system", "content": "You are a coding assistant."},
    {"role": "user", "content": "Help me fix this bug"}
  ],
  "max_tokens": 2048,
  "temperature": 0.7,
  "stream": true
}
```

**Session extension fields** (our additions):
```json
{
  "model": "gemma-3-12b-it-4bit",
  "session_id": "coding-agent-001",
  "messages": [...],
  "max_tokens": 2048,
  "stream": true,
  "cache_mode": "prefix",
  "ttl": 3600
}
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `session_id` | string | null | Explicit agent/session identifier. If null, falls back to content-based hashing |
| `cache_mode` | enum | "full" | `"full"`: reuse entire cached context. `"prefix"`: reuse matching prefix only |
| `ttl` | int | 3600 | Session cache lifetime in seconds. 0 = no persistence |

### 16.3 Direct Agent API

For maximum control, a dedicated agent management API:

```
POST /v1/agents
GET /v1/agents
GET /v1/agents/{agent_id}
DELETE /v1/agents/{agent_id}
POST /v1/agents/{agent_id}/generate
```

**Create Agent**:
```json
POST /v1/agents
{
  "agent_id": "review-agent",
  "model": "qwen2.5-14b-instruct-4bit",
  "system_prompt": "You are a code review expert.",
  "max_context": 8192,
  "persist": true
}
```

**Generate with Agent**:
```json
POST /v1/agents/review-agent/generate
{
  "messages": [
    {"role": "user", "content": "Review this PR"}
  ],
  "max_tokens": 2048,
  "stream": true
}
```

Note: only the NEW messages are sent. Server maintains conversation history internally (unlike Anthropic's stateless approach).

**List Agents**:
```json
GET /v1/agents
→ {
  "agents": [
    {
      "agent_id": "review-agent",
      "model": "qwen2.5-14b-instruct-4bit",
      "status": "hot",
      "context_tokens": 4096,
      "last_accessed": "2026-01-24T12:30:00Z"
    },
    {
      "agent_id": "coding-agent",
      "model": "gemma-3-12b-it-4bit",
      "status": "warm",
      "context_tokens": 8192,
      "last_accessed": "2026-01-24T10:00:00Z"
    }
  ]
}
```

### 16.4 Agent Identification Strategy Selection

```python
class AgentIdentifier:
    """
    Unified agent identification supporting both strategies.
    """

    async def identify(self, request: dict, protocol: str) -> str:
        """
        Resolve request to agent_id.

        Strategy selection:
        1. Explicit session_id/agent_id in request → use directly (preferred)
        2. Anthropic protocol without session_id → content-based hash
        3. OpenAI protocol without session_id → content-based hash (fallback)
        """
        # Strategy 1: Explicit ID (best for non-Anthropic)
        if "session_id" in request and request["session_id"]:
            return request["session_id"]

        # Strategy 2: Content-based (required for Anthropic/Claude Code)
        tokens = self.tokenize_request(request, protocol)
        cache_key = self.compute_cache_key(tokens)
        agent_id, remaining = self.cache_store.find_agent(cache_key)
        return agent_id

    def tokenize_request(self, request: dict, protocol: str) -> List[int]:
        """Tokenize request based on protocol format."""
        if protocol == "anthropic":
            # Anthropic order: tools → system → messages
            return self._tokenize_anthropic(request)
        else:
            # OpenAI order: system → messages (no tools in request body)
            return self._tokenize_openai(request)
```

### 16.5 Model Routing

The `model` field in requests triggers model management:

```python
async def route_model(self, request_model: str) -> ModelCacheSpec:
    """
    Ensure the requested model is loaded.
    Triggers hot-swap if needed.
    """
    # Normalize model name (handle aliases)
    model_id = self.resolve_model_alias(request_model)
    # e.g., "gemma-3" → "mlx-community/gemma-3-12b-it-4bit"
    #        "gpt-oss-20b" → "mlx-community/gpt-oss-20b-MXFP4-Q4"
    #        "qwen2.5" → "mlx-community/Qwen2.5-14B-Instruct-4bit"

    if model_id != self.registry.current_model_id:
        # Hot-swap: evict caches, unload, load new, reconfigure pool
        await self.registry.swap_model(model_id)

    return self.registry.current_spec
```

**Model aliases** (for convenience):
```json
{
  "gemma-3": "mlx-community/gemma-3-12b-it-4bit",
  "gpt-oss-20b": "mlx-community/gpt-oss-20b-MXFP4-Q4",
  "qwen2.5": "mlx-community/Qwen2.5-14B-Instruct-4bit",
  "qwen2.5-coder": "mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
  "llama-8b": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
}
```

---

## 17. Stateful vs Stateless: Protocol Comparison

### 17.1 Protocol Matrix

| Protocol | Agent ID | History Management | Model Field | Best For |
|----------|----------|-------------------|-------------|----------|
| **Anthropic** (`/v1/messages`) | Content-based hash | Client sends full history | Required | Claude Code CLI |
| **OpenAI** (`/v1/chat/completions`) | `session_id` (optional) | Client sends full history | Required | General LLM clients |
| **Direct** (`/v1/agents/{id}/generate`) | Explicit in URL | Server maintains history | In agent config | Custom automation |

### 17.2 History Management Differences

**Anthropic/OpenAI (stateless)**:
- Client sends ALL messages every request
- Server uses prefix matching to avoid re-processing
- Server-side cache is an optimization, not a requirement
- Client can modify/rewrite history freely

**Direct Agent API (stateful)**:
- Client sends only NEW messages
- Server maintains authoritative conversation history
- More efficient (less network bandwidth)
- Client cannot modify past turns (must reset agent)

```python
class DirectAgentHandler:
    """Handles stateful agent requests."""

    async def generate(self, agent_id: str, new_messages: List[dict], max_tokens: int):
        """
        Generate with server-side history.
        1. Append new_messages to agent's stored history
        2. Tokenize ONLY the new messages (cache covers the rest)
        3. Submit to batch engine with existing cache + new tokens
        4. Append assistant response to stored history
        5. Return response
        """
        agent = self.cache_store.get_agent(agent_id)

        # Append user messages to history
        agent.history.extend(new_messages)

        # Only tokenize what's new (cache covers prefix)
        new_tokens = self.tokenize_messages(new_messages)

        # Generate (cache already has prior context)
        result = await self.engine.generate(
            agent_id=agent_id,
            remaining_tokens=new_tokens,
            max_tokens=max_tokens,
            existing_cache=agent.cache_blocks
        )

        # Append assistant response
        agent.history.append({"role": "assistant", "content": result.text})
        agent.cache_blocks = result.updated_blocks

        return result.text
```

### 17.3 Why Both Approaches?

**Content-based (Anthropic)** is necessary because:
- Claude Code CLI sends no session ID
- Can't modify Claude Code's request format
- Must work transparently as a drop-in for `ANTHROPIC_BASE_URL`

**Explicit agent_id (non-Anthropic)** is better because:
- O(1) agent resolution (no trie traversal, no tokenization for matching)
- Client has explicit control over agent lifecycle
- No risk of cache key collision
- Server can maintain history (stateful mode) → less bandwidth
- Better for multi-agent orchestration frameworks
- **Much better for later systems running locally** where agents have persistent identities

---

## 18. Multi-Model Agent Persistence

### 18.1 Agent Identity vs. Model Cache

Agents have identity independent of the model they use:

```
Agent "coding-helper":
  ├── Identity: "coding-helper" (persistent across model swaps)
  ├── Metadata: {created: ..., system_prompt: "...", ...}
  ├── History: [{role: user, ...}, {role: assistant, ...}, ...]
  └── Caches:
      ├── gemma-3-12b-it-4bit/   → KV cache for this model (if exists)
      ├── gpt-oss-20b-MXFP4-Q4/  → KV cache for this model (if exists)
      └── Qwen2.5-14B-4bit/      → KV cache for this model (if exists)
```

When an agent switches models:
- Agent identity and history are preserved
- Old model's cache remains on disk (warm)
- New model's cache starts fresh (or loaded from disk if previously used)
- Conversation can continue with different model (re-tokenized)

### 18.2 Model-Agnostic Agent Store

```python
class AgentStore:
    """
    Persistent agent store independent of model.
    Manages agent identity, history, and model-specific caches.
    """

    def get_agent(self, agent_id: str) -> AgentState:
        """Get agent state (identity + history). Model-independent."""
        ...

    def get_cache(self, agent_id: str, model_id: str) -> Optional[AgentBlocks]:
        """Get model-specific cache for an agent. None if no cache for this model."""
        ...

    def save_cache(self, agent_id: str, model_id: str, blocks: AgentBlocks):
        """Save model-specific cache for an agent."""
        ...

    def list_agents(self) -> List[AgentSummary]:
        """List all known agents with status per model."""
        ...
```

---

## 19. Streaming Response Format (Model-Agnostic)

### 19.1 Anthropic SSE Format

(Unchanged from Section 4 above - required for Claude Code compatibility)

### 19.2 OpenAI SSE Format

For OpenAI-compatible clients:

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1706000000,"model":"gemma-3-12b-it-4bit","choices":[{"index":0,"delta":{"role":"assistant","content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1706000000,"model":"gemma-3-12b-it-4bit","choices":[{"index":0,"delta":{"content":" world"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1706000000,"model":"gemma-3-12b-it-4bit","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### 19.3 Protocol Detection

```python
@app.post("/v1/messages")
async def anthropic_endpoint(request: Request):
    """Anthropic Messages API - content-based agent ID."""
    return await adapter.handle_anthropic(request)

@app.post("/v1/chat/completions")
async def openai_endpoint(request: Request):
    """OpenAI-compatible - explicit session_id or content-based fallback."""
    return await adapter.handle_openai(request)

@app.post("/v1/agents/{agent_id}/generate")
async def direct_endpoint(agent_id: str, request: Request):
    """Direct agent API - stateful, server-side history."""
    return await adapter.handle_direct(agent_id, request)
```

---

## 20. Expert Debate: Multi-Protocol Design

### System Engineer
"The explicit agent_id path is the 80/20 solution. Content-based hashing is clever but adds complexity (tokenization, trie, hash collisions). For any non-Claude-Code client, just pass an ID. The content-based path exists solely for Anthropic compatibility."

### LLM Engineer
"Model routing via the `model` field is clean. The key insight: agent identity is orthogonal to model choice. The same 'coding-agent' can use Gemma 3 today and Qwen 2.5 tomorrow. Its conversation history persists; only the KV cache is model-specific."

### Anthropic Expert
"We must keep the Anthropic path pristine - Claude Code expects exact protocol compliance. But the OpenAI and Direct APIs are our own design, so we can optimize them freely. The `session_id` extension to OpenAI format is non-breaking (ignored by clients that don't know about it)."

### Memory Expert
"Stateful mode (Direct API) is memory-optimal: client sends only new messages, no redundant re-transmission. But it shifts history management to the server. For a single-user system, this is fine - no multi-tenancy concerns. For multi-user later, the stateless approaches scale better."

### Attention Expert
"The model-agnostic tokenization is the tricky part. Anthropic format has `tools → system → messages` ordering. OpenAI has `system (in messages) → messages`. These produce DIFFERENT token sequences even for the same logical content. Content-based cache keys are protocol-specific by necessity."

---

## 21. Implementation Phases (Updated)

### Phase A: Enhanced Request Parsing (Multi-Protocol)
- Support Anthropic system prompt as array of blocks
- Support OpenAI `messages` format (system as first message)
- Parse the `model` field and resolve aliases
- Route to appropriate handler by endpoint

### Phase B: Dual Agent Identification
- Implement `AgentIdentifier` with both strategies
- Explicit `session_id` for OpenAI-compatible requests
- Content-based prefix hashing for Anthropic requests
- Trie-based prefix matching for content-based path

### Phase C: Model Routing + Hot-Swap Integration
- Model alias resolution
- Hot-swap trigger on model field mismatch
- Cache invalidation and preservation on swap
- Agent-to-model-cache mapping

### Phase D: Full SSE Streaming (Both Formats)
- Anthropic SSE (message_start → content_block_delta → message_stop)
- OpenAI SSE (chat.completion.chunk → [DONE])
- True per-token delivery from batch engine

### Phase E: Direct Agent API
- CRUD for agents (create, list, get, delete)
- Server-side history management
- Stateful generate endpoint
- Agent status reporting (hot/warm/cold)

### Phase F: Cache Lifecycle + Metrics
- TTL-based eviction with per-protocol defaults
- Prefix sharing for common system prompts
- Metrics: cache hit rate, prefix match depth, model swap count
- Health endpoint with pool utilization

---

## References

- [Anthropic Messages API](https://docs.anthropic.com/en/api/messages)
- [Prompt Caching](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
- [Extended Thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
- [Claude Code System Prompts](https://github.com/Piebald-AI/claude-code-system-prompts)
- [Count Tokens API](https://docs.anthropic.com/en/api/messages-count-tokens)
- [mlx-lm Server CacheManager](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/server.py)
- [vLLM PagedAttention](https://docs.vllm.ai/en/stable/design/paged_attention/)
- [vllm-mlx](https://github.com/waybarrios/vllm-mlx)
- [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat)
- [LMDeploy Interactive API](https://lmdeploy.readthedocs.io/en/v0.5.0/serving/api_server.html)
- [llama.cpp Slot-Based Sessions](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md)
- [Ollama Context Persistence](https://github.com/ollama/ollama/issues/7032)
- [GPT-OSS-20B on HuggingFace](https://huggingface.co/openai/gpt-oss-20b)
- [Qwen 2.5-14B on HuggingFace](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
- [vLLM Hybrid KV Cache Manager](https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/)
