Bayesian inference ( BAY-zee-ən or BAY-zhən) is a method of statistical inference in which Bayes' theorem is used to calculate a probability of a hypothesis, given prior evidence, and update it as more information becomes available. Fundamentally, Bayesian inference uses a prior distribution to estimate posterior probabilities. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability". == Introduction to Bayes' rule == === Formal explanation === Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a "likelihood function" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem: P ( H ∣ E ) = P ( E ∣ H ) ⋅ P ( H ) P ( E ) , {\displaystyle P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)}},} where H {\displaystyle H} stands for any hypothesis whose probability may be affected by data (called evidence below). Often there are competing hypotheses, and the task is to determine which is the most probable. P ( H ) {\displaystyle P(H)} , the prior probability, is the estimate of the probability of the hypothesis H {\displaystyle H} before the data E {\displaystyle E} , the current evidence, is observed. E {\displaystyle E} , the evidence, corresponds to new data that were not used in computing the prior probability. P ( H ∣ E ) {\displaystyle P(H\mid E)} , the posterior probability, is the probability of H {\displaystyle H} given E {\displaystyle E} , i.e., after E {\displaystyle E} is observed. This is what we want to know: the probability of a hypothesis given the observed evidence. P ( E ∣ H ) {\displaystyle P(E\mid H)} is the probability of observing E {\displaystyle E} given H {\displaystyle H} and is called the likelihood. As a function of E {\displaystyle E} with H {\displaystyle H} fixed, it indicates the compatibility of the evidence with the given hypothesis. The likelihood function is a function of the evidence, E {\displaystyle E} , while the posterior probability is a function of the hypothesis, H {\displaystyle H} . P ( E ) {\displaystyle P(E)} is sometimes termed the marginal likelihood or "model evidence". This factor is the same for all possible hypotheses being considered (as is evident from the fact that the hypothesis H {\displaystyle H} does not appear anywhere in the symbol, unlike for all the other factors) and hence does not factor into determining the relative probabilities of different hypotheses. P ( E ) > 0 {\displaystyle P(E)>0} (Else one has 0 / 0 {\displaystyle 0/0} .) For different values of H {\displaystyle H} , only the factors P ( H ) {\displaystyle P(H)} and P ( E ∣ H ) {\displaystyle P(E\mid H)} , both in the numerator, affect the value of P ( H ∣ E ) {\displaystyle P(H\mid E)} – the posterior probability of a hypothesis is proportional to its prior probability (its inherent likeliness) and the newly acquired likelihood (its compatibility with the new observed evidence). In cases where ¬ H {\displaystyle \neg H} ("not H {\displaystyle H} "), the logical negation of H {\displaystyle H} , is a valid likelihood, Bayes' rule can be rewritten as follows: P ( H ∣ E ) = P ( E ∣ H ) P ( H ) P ( E ) = P ( E ∣ H ) P ( H ) P ( E ∣ H ) P ( H ) + P ( E ∣ ¬ H ) P ( ¬ H ) = 1 1 + ( 1 P ( H ) − 1 ) P ( E ∣ ¬ H ) P ( E ∣ H ) {\displaystyle {\begin{aligned}P(H\mid E)&={\frac {P(E\mid H)P(H)}{P(E)}}\\\\&={\frac {P(E\mid H)P(H)}{P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}}\\\\&={\frac {1}{1+\left({\frac {1}{P(H)}}-1\right){\frac {P(E\mid \neg H)}{P(E\mid H)}}}}\\\end{aligned}}} because P ( E ) = P ( E ∣ H ) P ( H ) + P ( E ∣ ¬ H ) P ( ¬ H ) {\displaystyle P(E)=P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)} and P ( H ) + P ( ¬ H ) = 1. {\displaystyle P(H)+P(\neg H)=1.} This focuses attention on the term ( 1 P ( H ) − 1 ) P ( E ∣ ¬ H ) P ( E ∣ H ) . {\displaystyle \left({\tfrac {1}{P(H)}}-1\right){\tfrac {P(E\mid \neg H)}{P(E\mid H)}}.} If that term is approximately 1, then the probability of the hypothesis given the evidence, P ( H ∣ E ) {\displaystyle P(H\mid E)} , is about 1 2 {\displaystyle {\tfrac {1}{2}}} , about 50% likely - equally likely or not likely. If that term is very small, close to zero, then the probability of the hypothesis, given the evidence, P ( H ∣ E ) {\displaystyle P(H\mid E)} is close to 1 or the conditional hypothesis is quite likely. If that term is very large, much larger than 1, then the hypothesis, given the evidence, is quite unlikely. If the hypothesis (without consideration of evidence) is unlikely, then P ( H ) {\displaystyle P(H)} is small (but not necessarily astronomically small) and 1 P ( H ) {\displaystyle {\tfrac {1}{P(H)}}} is much larger than 1 and this term can be approximated as P ( E ∣ ¬ H ) P ( E ∣ H ) ⋅ P ( H ) {\displaystyle {\tfrac {P(E\mid \neg H)}{P(E\mid H)\cdot P(H)}}} and relevant probabilities can be compared directly to each other. One quick and easy way to remember the equation would be to use rule of multiplication: P ( E ∩ H ) = P ( E ∣ H ) P ( H ) = P ( H ∣ E ) P ( E ) . {\displaystyle P(E\cap H)=P(E\mid H)P(H)=P(H\mid E)P(E).} === Alternatives to Bayesian updating === Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational. Ian Hacking noted that traditional "Dutch book" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote: "And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour." Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics") following the publication of Richard C. Jeffrey's rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability. The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory. == Inference over exclusive and exhaustive possibilities == If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole. === General formulation === Suppose a process is generating independent and identically distributed events E n , n = 1 , 2 , 3 , … {\displaystyle E_{n},\ n=1,2,3,\ldots } , but the probability distribution is unknown. Let the event space Ω {\displaystyle \Omega } represent the current state of belief for this process. Each model is represented by event M m {\displaystyle M_{m}} . The conditional probabilities P ( E n ∣ M m ) {\displaystyle P(E_{n}\mid M_{m})} are specified to define the models. P ( M m ) {\displaystyle P(M_{m})} is the degree of belief in M m {\displaystyle M_{m}} . Before the first inference step, { P ( M m ) } {\displaystyle \{P(M_{m})\}} is a set of initial prior probabilities. These must sum to 1, but are otherwise arbitrary. Suppose that the process is observed to generate E ∈ { E n } {\displaystyle E\in \{E_{n}\}} . For each M ∈ { M m } {\displaystyle M\in \{M_{m}\}} , the prior P ( M ) {\displaystyle P(M)} is updated to the posterior P ( M ∣ E ) {\displaystyle P(M\mid E)} . From Bayes' theorem: P ( M ∣ E ) = P ( E ∣ M ) ∑ m P ( E ∣ M m ) P ( M m ) ⋅ P ( M ) . {\displaystyle P(M\mid E)={\frac {P(E\mid M)}{\sum _{m}{P(E\mid M_{m})P(M_{m})}}}\cdot P(M).} Upon observation of further evidence, this procedure may be repeated. === Multiple observations === For a sequence of independent and identically distributed observations E = ( e 1 , … , e n ) {\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})} , it can be shown by induction that repeated application of the above is equivalent to P ( M ∣ E ) = P ( E ∣ M ) ∑ m P ( E ∣ M m ) P ( M m ) ⋅ P ( M ) , {\displaystyle P(M\mid \mathbf {E} )={\frac {P(\mathbf {E} \mid M)}{\sum _{m}{P(\mathbf {E} \mid M_{m})P(M_{m})}}}\cdot P(M),} where P ( E ∣ M ) = ∏ k P ( e k ∣ M ) . {\displaystyle P(\mathbf {E} \mid M)=\prod _{k}{P(e_{k}\mid M)}.} === Parametric formulation: motivating the formal description === By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is, however, equally applicable to discrete distributions. Let the vector θ {\displaystyle {\boldsymbol {\theta }}} span the parameter space. Let the initial prior distribution over θ {\displaystyle {\boldsymbol {\theta }}} be p ( θ ∣ α ) {\displaystyle p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})} , where α {\displaystyle {\boldsymbol {\alpha }}} is a set of parameters to the prior itself, or hyperparameters. Let E = ( e 1 , … , e n ) {\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})} be a sequence of independent and identically distributed event observations, where all e i {\displaystyle e_{i}} are distributed as p ( e ∣ θ ) {\displaystyle p(e\mid {\boldsymbol {\theta }})} for some θ {\displaystyle {\boldsymbol {\theta }}} . Bayes' theorem is applied to find the posterior distribution over θ {\displaystyle {\boldsymbol {\theta }}} : p ( θ ∣ E , α ) = p ( E ∣ θ , α ) p ( E ∣ α ) ⋅ p ( θ ∣ α ) = p ( E ∣ θ , α ) ∫ p ( E ∣ θ , α ) p ( θ ∣ α ) d θ ⋅ p ( θ ∣ α ) , {\displaystyle {\begin{aligned}p({\boldsymbol {\theta }}\mid \mathbf {E} ,{\boldsymbol {\alpha }})&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{p(\mathbf {E} \mid {\boldsymbol {\alpha }})}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\\&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{\int p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\,d{\boldsymbol {\theta }}}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }}),\end{aligned}}} where p ( E ∣ θ , α ) = ∏ k p ( e k ∣ θ ) . {\displaystyle p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})=\prod _{k}p(e_{k}\mid {\boldsymbol {\theta }}).} == Formal description of Bayesian inference == === Definitions === x {\displaystyle x} , a data point in general. This may in fact be a vector of values. θ {\displaystyle \theta } , the parameter of the data point's distribution, i.e., x ∼ p ( x ∣ θ ) {\displaystyle x\sim p(x\mid \theta )} . This may be a vector of parameters. α {\displaystyle \alpha } , the hyperparameter of the parameter distribution, i.e., θ ∼ p ( θ ∣ α ) {\displaystyle \theta \sim p(\theta \mid \alpha )} . This may be a vector of hyperparameters. X {\displaystyle \mathbf {X} } is the sample, a set of n {\displaystyle n} observed data points, i.e., x 1 , … , x n {\displaystyle x_{1},\ldots ,x_{n}} . x ~ {\displaystyle {\tilde {x}}} , a new data point whose distribution is to be predicted. === Bayesian inference === The prior distribution is the distribution of the parameter(s) before any data is observed, i.e. p ( θ ∣ α ) {\displaystyle p(\theta \mid \alpha )} . The prior distribution might not be easily determined; in such a case, one possibility may be to use the Jeffreys prior to obtain a prior distribution before updating it with newer observations. The sampling distribution is the distribution of the observed data conditional on its parameters, i.e. p ( X ∣ θ ) {\displaystyle p(\mathbf {X} \mid \theta )} . This is also termed the likelihood, especially when viewed as a function of the parameter(s), sometimes written L ⁡ ( θ ∣ X ) = p ( X ∣ θ ) {\displaystyle \operatorname {L} (\theta \mid \mathbf {X} )=p(\mathbf {X} \mid \theta )} . The marginal likelihood (sometimes also termed the evidence) is the distribution of the observed data marginalized over the parameter(s), i.e. p ( X ∣ α ) = ∫ p ( X ∣ θ ) p ( θ ∣ α ) d θ . {\displaystyle p(\mathbf {X} \mid \alpha )=\int p(\mathbf {X} \mid \theta )p(\theta \mid \alpha )d\theta .} It quantifies the agreement between data and expert opinion, in a geometric sense that can be made precise. If the marginal likelihood is 0 then there is no agreement between the data and expert opinion and Bayes' rule cannot be applied. The posterior distribution is the distribution of the parameter(s) after taking into account the observed data. This is determined by Bayes' rule, which forms the heart of Bayesian inference: p ( θ ∣ X , α ) = p ( θ , X , α ) p ( X , α ) = p ( X ∣ θ , α ) p ( θ , α ) p ( X ∣ α ) p ( α ) = p ( X ∣ θ , α ) p ( θ ∣ α ) p ( X ∣ α ) ∝ p ( X ∣ θ , α ) p ( θ ∣ α ) . {\displaystyle p(\theta \mid \mathbf {X} ,\alpha )={\frac {p(\theta ,\mathbf {X} ,\alpha )}{p(\mathbf {X} ,\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta ,\alpha )}{p(\mathbf {X} \mid \alpha )p(\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha )}{p(\mathbf {X} \mid \alpha )}}\propto p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha ).} This is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence". In practice, for almost all complex Bayesian models used in machine learning, the posterior distribution p ( θ ∣ X , α ) {\displaystyle p(\theta \mid \mathbf {X} ,\alpha )} is not obtained in a closed form distribution, mainly because the parameter space for θ {\displaystyle \theta } can be very high, or the Bayesian model retains certain hierarchical structure formulated from the observations X {\displaystyle \mathbf {X} } and parameter θ {\displaystyle \theta } . In such situations, we need to resort to approximation techniques. General case: Let P Y x {\displaystyle P_{Y}^{x}} be the conditional distribution of Y {\displaystyle Y} given X = x {\displaystyle X=x} and let P X {\displaystyle P_{X}} be the distribution of X {\displaystyle X} . The joint distribution is then P X , Y ( d x , d y ) = P Y x ( d y ) P X ( d x ) {\displaystyle P_{X,Y}(dx,dy)=P_{Y}^{x}(dy)P_{X}(dx)} . The conditional distribution P X y {\displaystyle P_{X}^{y}} of X {\displaystyle X} given Y = y {\displaystyle Y=y} is then determined by P X y ( A ) = E ( 1 A ( X ) | Y = y ) {\displaystyle P_{X}^{y}(A)=E(1_{A}(X)|Y=y)} Existence and uniqueness of the needed conditional expectation is a consequence of the Radon–Nikodym theorem. This was formulated by Kolmogorov in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing "I wish to call attention to ... and especially the theory of conditional probabilities and conditional expectations ..." in the Preface. The Bayes theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions. Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line. Modern Markov chain Monte Carlo methods have boosted the importance of Bayes' theorem including cases with improper priors. === Bayesian prediction === The posterior predictive distribution is the distribution of a new data point, marginalized over the posterior: p ( x ~ ∣ X , α ) = ∫ p ( x ~ ∣ θ ) p ( θ ∣ X , α ) d θ {\displaystyle p({\tilde {x}}\mid \mathbf {X} ,\alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )d\theta } The prior predictive distribution is the distribution of a new data point, marginalized over the prior: p ( x ~ ∣ α ) = ∫ p ( x ~ ∣ θ ) p ( θ ∣ α ) d θ {\displaystyle p({\tilde {x}}\mid \alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \alpha )d\theta } Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned. Only this way is the entire posterior distribution of the parameter(s) used. By comparison, prediction in frequentist statistics often involves finding an optimum point estimate of the parameter(s)—e.g., by maximum likelihood or maximum a posteriori estimation (MAP)—and then plugging this estimate into the formula for the

[Article truncated for benchmark context.]
