{
  "metadata": {
    "timestamp": "2026-02-01T00:12:46.075659+00:00",
    "server": "semantic_b1",
    "base_url": "http://127.0.0.1:8399",
    "context_mode": "full",
    "api": "openai",
    "endpoint": "/v1/chat/completions",
    "machine": {
      "os": "Darwin",
      "os_version": "25.2.0",
      "chip": "arm64"
    },
    "git_sha": "d1035c5",
    "runs_per_scenario": 1,
    "quick": false
  },
  "experiments": {
    "1_cold_start": {
      "cold_short": {
        "runs": [
          {
            "scenario": "cold_short",
            "config": "semantic_b1",
            "ttft_ms": 586.8165839929134,
            "e2e_ms": 1161.7069999920204,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 8.982662749986048,
            "decode_tps": 111.32556434911834,
            "overall_tps": 111.32556434911834,
            "output_tokens": 64,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 9081.9,
            "raw_output": "fixed size and allocated per-layer per-agent from a shared pool. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. Subsequent requests retrieve the cache from disk and reconstruct the quantized KV blocks. This architecture supports efficient memoryy management",
            "error": null
          }
        ],
        "stats": {
          "ttft_ms": {
            "mean": 586.8165839929134,
            "median": 586.8165839929134,
            "p95": 586.8165839929134,
            "p99": 586.8165839929134
          },
          "e2e_ms": {
            "mean": 1161.7069999920204,
            "median": 1161.7069999920204,
            "p95": 1161.7069999920204,
            "p99": 1161.7069999920204
          },
          "decode_tps": {
            "mean": 111.32556434911834,
            "median": 111.32556434911834,
            "p95": 111.32556434911834,
            "p99": 111.32556434911834
          },
          "peak_memory_mb": {
            "mean": 9081.9,
            "median": 9081.9,
            "p95": 9081.9,
            "p99": 9081.9
          }
        }
      },
      "cold_medium": {
        "runs": [
          {
            "scenario": "cold_medium",
            "config": "semantic_b1",
            "ttft_ms": 1519.229625002481,
            "e2e_ms": 2315.4128339956515,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 12.440362640518288,
            "decode_tps": 80.38350881693736,
            "overall_tps": 80.38350881693736,
            "output_tokens": 64,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 9221.7,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensorss serialization",
            "error": null
          }
        ],
        "stats": {
          "ttft_ms": {
            "mean": 1519.229625002481,
            "median": 1519.229625002481,
            "p95": 1519.229625002481,
            "p99": 1519.229625002481
          },
          "e2e_ms": {
            "mean": 2315.4128339956515,
            "median": 2315.4128339956515,
            "p95": 2315.4128339956515,
            "p99": 2315.4128339956515
          },
          "decode_tps": {
            "mean": 80.38350881693736,
            "median": 80.38350881693736,
            "p95": 80.38350881693736,
            "p99": 80.38350881693736
          },
          "peak_memory_mb": {
            "mean": 9221.7,
            "median": 9221.7,
            "p95": 9221.7,
            "p99": 9221.7
          }
        }
      },
      "cold_long": {
        "runs": [
          {
            "scenario": "cold_long",
            "config": "semantic_b1",
            "ttft_ms": 6223.1537500047125,
            "e2e_ms": 7710.78620798653,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 23.244257155965897,
            "decode_tps": 43.02137914281932,
            "overall_tps": 43.02137914281932,
            "output_tokens": 64,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 10920.2,
            "raw_output": "s a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from diskk and",
            "error": null
          }
        ],
        "stats": {
          "ttft_ms": {
            "mean": 6223.1537500047125,
            "median": 6223.1537500047125,
            "p95": 6223.1537500047125,
            "p99": 6223.1537500047125
          },
          "e2e_ms": {
            "mean": 7710.78620798653,
            "median": 7710.78620798653,
            "p95": 7710.78620798653,
            "p99": 7710.78620798653
          },
          "decode_tps": {
            "mean": 43.02137914281932,
            "median": 43.02137914281932,
            "p95": 43.02137914281932,
            "p99": 43.02137914281932
          },
          "peak_memory_mb": {
            "mean": 10920.2,
            "median": 10920.2,
            "p95": 10920.2,
            "p99": 10920.2
          }
        }
      },
      "cold_xl": {
        "runs": [
          {
            "scenario": "cold_xl",
            "config": "semantic_b1",
            "ttft_ms": 35753.58249997953,
            "e2e_ms": 40361.8357499945,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 72.00395703148388,
            "decode_tps": 13.888125614579042,
            "overall_tps": 13.888125614579042,
            "output_tokens": 64,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 11976.3,
            "raw_output": "ization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-basedd KV",
            "error": null
          }
        ],
        "stats": {
          "ttft_ms": {
            "mean": 35753.58249997953,
            "median": 35753.58249997953,
            "p95": 35753.58249997953,
            "p99": 35753.58249997953
          },
          "e2e_ms": {
            "mean": 40361.8357499945,
            "median": 40361.8357499945,
            "p95": 40361.8357499945,
            "p99": 40361.8357499945
          },
          "decode_tps": {
            "mean": 13.888125614579042,
            "median": 13.888125614579042,
            "p95": 13.888125614579042,
            "p99": 13.888125614579042
          },
          "peak_memory_mb": {
            "mean": 11976.3,
            "median": 11976.3,
            "p95": 11976.3,
            "p99": 11976.3
          }
        }
      }
    },
    "4_output_scaling": {
      "output_16": {
        "runs": [
          {
            "scenario": "output_16",
            "config": "semantic_b1",
            "ttft_ms": 2428.620500024408,
            "e2e_ms": 2428.620500024408,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 151.7887812515255,
            "decode_tps": 6.588102175633944,
            "overall_tps": 6.588102175633944,
            "output_tokens": 16,
            "input_tokens": 728,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "implements a block-based KV cache architecture where each block stores a fixed number of",
            "error": null
          }
        ],
        "stats": {
          "e2e_ms": {
            "mean": 2428.620500024408,
            "median": 2428.620500024408,
            "p95": 2428.620500024408,
            "p99": 2428.620500024408
          },
          "tpot_ms": {
            "mean": 151.7887812515255,
            "median": 151.7887812515255,
            "p95": 151.7887812515255,
            "p99": 151.7887812515255
          },
          "decode_tps": {
            "mean": 6.588102175633944,
            "median": 6.588102175633944,
            "p95": 6.588102175633944,
            "p99": 6.588102175633944
          }
        }
      },
      "output_64": {
        "runs": [
          {
            "scenario": "output_64",
            "config": "semantic_b1",
            "ttft_ms": 1675.3236249787733,
            "e2e_ms": 1675.3236249787733,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 26.176931640293333,
            "decode_tps": 38.20157433809894,
            "overall_tps": 38.20157433809894,
            "output_tokens": 64,
            "input_tokens": 728,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors",
            "error": null
          }
        ],
        "stats": {
          "e2e_ms": {
            "mean": 1675.3236249787733,
            "median": 1675.3236249787733,
            "p95": 1675.3236249787733,
            "p99": 1675.3236249787733
          },
          "tpot_ms": {
            "mean": 26.176931640293333,
            "median": 26.176931640293333,
            "p95": 26.176931640293333,
            "p99": 26.176931640293333
          },
          "decode_tps": {
            "mean": 38.20157433809894,
            "median": 38.20157433809894,
            "p95": 38.20157433809894,
            "p99": 38.20157433809894
          }
        }
      },
      "output_128": {
        "runs": [
          {
            "scenario": "output_128",
            "config": "semantic_b1",
            "ttft_ms": 2191.973124979995,
            "e2e_ms": 2191.973124979995,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 18.266442708166625,
            "decode_tps": 54.74519675103004,
            "overall_tps": 54.74519675103004,
            "output_tokens": 120,
            "input_tokens": 728,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "stats": {
          "e2e_ms": {
            "mean": 2191.973124979995,
            "median": 2191.973124979995,
            "p95": 2191.973124979995,
            "p99": 2191.973124979995
          },
          "tpot_ms": {
            "mean": 18.266442708166625,
            "median": 18.266442708166625,
            "p95": 18.266442708166625,
            "p99": 18.266442708166625
          },
          "decode_tps": {
            "mean": 54.74519675103004,
            "median": 54.74519675103004,
            "p95": 54.74519675103004,
            "p99": 54.74519675103004
          }
        }
      },
      "output_256": {
        "runs": [
          {
            "scenario": "output_256",
            "config": "semantic_b1",
            "ttft_ms": 2209.674540965352,
            "e2e_ms": 2209.674540965352,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 18.413954508044604,
            "decode_tps": 54.30664008446011,
            "overall_tps": 54.30664008446011,
            "output_tokens": 120,
            "input_tokens": 728,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "stats": {
          "e2e_ms": {
            "mean": 2209.674540965352,
            "median": 2209.674540965352,
            "p95": 2209.674540965352,
            "p99": 2209.674540965352
          },
          "tpot_ms": {
            "mean": 18.413954508044604,
            "median": 18.413954508044604,
            "p95": 18.413954508044604,
            "p99": 18.413954508044604
          },
          "decode_tps": {
            "mean": 54.30664008446011,
            "median": 54.30664008446011,
            "p95": 54.30664008446011,
            "p99": 54.30664008446011
          }
        }
      },
      "output_512": {
        "runs": [
          {
            "scenario": "output_512",
            "config": "semantic_b1",
            "ttft_ms": 2194.012791966088,
            "e2e_ms": 2194.012791966088,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 18.283439933050737,
            "decode_tps": 54.69430280416286,
            "overall_tps": 54.69430280416286,
            "output_tokens": 120,
            "input_tokens": 728,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "stats": {
          "e2e_ms": {
            "mean": 2194.012791966088,
            "median": 2194.012791966088,
            "p95": 2194.012791966088,
            "p99": 2194.012791966088
          },
          "tpot_ms": {
            "mean": 18.283439933050737,
            "median": 18.283439933050737,
            "p95": 18.283439933050737,
            "p99": 18.283439933050737
          },
          "decode_tps": {
            "mean": 54.69430280416286,
            "median": 54.69430280416286,
            "p95": 54.69430280416286,
            "p99": 54.69430280416286
          }
        }
      }
    }
  }
}