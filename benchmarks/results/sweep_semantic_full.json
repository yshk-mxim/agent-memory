{
  "metadata": {
    "timestamp": "2026-02-01T01:54:14.967507+00:00",
    "server": "semantic",
    "base_url": "http://127.0.0.1:8399",
    "model": "default",
    "machine": {
      "os": "Darwin",
      "os_version": "25.2.0",
      "chip": "arm64"
    },
    "git_sha": "d1035c5",
    "contexts": [
      50000,
      25000,
      10000,
      5000,
      2000
    ],
    "outputs": [
      5000,
      1000,
      500,
      100
    ],
    "batch_sizes": [
      1,
      2,
      3
    ],
    "prealloc_tokens": 64000
  },
  "sweeps": {
    "batch_1": {
      "cold_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 119527.37958403304,
        "e2e_ms": 135341.4386670338,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 126.51247266400605,
        "decode_tps": 7.904358984871134,
        "overall_tps": 7.904358984871134,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "warm_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 108849.39387498889,
        "e2e_ms": 118987.47887497302,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 81.10467999987304,
        "decode_tps": 12.32974472005272,
        "overall_tps": 12.32974472005272,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "hot_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 33572.511875012424,
        "e2e_ms": 55908.21841702564,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 111.67853271006607,
        "decode_tps": 8.954272372078414,
        "overall_tps": 8.954272372078414,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "cold_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 122781.86312498292,
        "e2e_ms": 140984.8253329983,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 145.62369766412303,
        "decode_tps": 6.86701420194996,
        "overall_tps": 6.86701420194996,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "warm_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 115384.60079202196,
        "e2e_ms": 127373.69700003183,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 95.91276966407895,
        "decode_tps": 10.426140372156492,
        "overall_tps": 10.426140372156492,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "hot_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 23015.237332961988,
        "e2e_ms": 50915.34620797029,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 139.5005443750415,
        "decode_tps": 7.168430807779079,
        "overall_tps": 7.168430807779079,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "cold_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 118072.70670798607,
        "e2e_ms": 136649.38379096566,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 148.6134166638367,
        "decode_tps": 6.72886757096769,
        "overall_tps": 6.72886757096769,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "warm_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 113321.09604100697,
        "e2e_ms": 124336.68116602348,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 88.12468100013211,
        "decode_tps": 11.34755880703274,
        "overall_tps": 11.34755880703274,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "hot_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 36172.50924999826,
        "e2e_ms": 54761.52125000954,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 92.9450600000564,
        "decode_tps": 10.759044106264422,
        "overall_tps": 10.759044106264422,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "cold_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 116880.38441602839,
        "e2e_ms": 133577.670083032,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 166.97285667003598,
        "decode_tps": 5.988997373244644,
        "overall_tps": 5.988997373244644,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "warm_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 106927.75254201842,
        "e2e_ms": 121678.33587498171,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 147.50583332963288,
        "decode_tps": 6.779392905535398,
        "overall_tps": 6.779392905535398,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "hot_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 30246.681291027926,
        "e2e_ms": 50010.00837498577,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 197.63327083957847,
        "decode_tps": 5.059876789731994,
        "overall_tps": 5.059876789731994,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "cold_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 65589.31800001301,
        "e2e_ms": 202726.0280419723,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.42734200839186,
        "decode_tps": 36.45996756426609,
        "overall_tps": 36.45996756426609,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "warm_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 64444.12941701012,
        "e2e_ms": 200692.57212500088,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.249688541598154,
        "decode_tps": 36.69766714850501,
        "overall_tps": 36.69766714850501,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "hot_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1179.0217499947175,
        "e2e_ms": 8086.8446250096895,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 34.19714294561867,
        "decode_tps": 29.24220896436378,
        "overall_tps": 29.24220896436378,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "cold_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31587.791749974713,
        "e2e_ms": 63338.833208952565,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 31.751041458977852,
        "decode_tps": 31.495029896641146,
        "overall_tps": 31.495029896641146,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "warm_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 32821.345083008055,
        "e2e_ms": 59400.17241699388,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 26.578827333985828,
        "decode_tps": 37.62393229144912,
        "overall_tps": 37.62393229144912,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "hot_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1194.893583015073,
        "e2e_ms": 8091.799457964953,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 34.14309839084099,
        "decode_tps": 29.288495981028294,
        "overall_tps": 29.288495981028294,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "cold_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 32254.016084014438,
        "e2e_ms": 46514.4317499944,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 28.520831331959926,
        "decode_tps": 35.06209157653193,
        "overall_tps": 35.06209157653193,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "warm_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31171.292083046865,
        "e2e_ms": 45373.112875036895,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 28.40364158398006,
        "decode_tps": 35.206753227164015,
        "overall_tps": 35.206753227164015,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "hot_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 905.2462090039626,
        "e2e_ms": 7733.451917010825,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 33.80299855448942,
        "decode_tps": 29.58317435620482,
        "overall_tps": 29.58317435620482,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "cold_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31586.18691697484,
        "e2e_ms": 36389.54083400313,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 48.03353917028289,
        "decode_tps": 20.8187865660891,
        "overall_tps": 20.8187865660891,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "warm_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 29697.753124986775,
        "e2e_ms": 34809.62220800575,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 51.118690830189735,
        "decode_tps": 19.56231632226033,
        "overall_tps": 19.56231632226033,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "hot_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 968.364417029079,
        "e2e_ms": 5416.000708006322,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 44.47636290977243,
        "decode_tps": 22.483852873236586,
        "overall_tps": 22.483852873236586,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inferencee on",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "cold_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 10640.975791960955,
        "e2e_ms": 97044.89191697212,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.28078322500223,
        "decode_tps": 57.8677474845687,
        "overall_tps": 57.8677474845687,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "warm_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 12188.292833045125,
        "e2e_ms": 99095.68191703875,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.381477816798725,
        "decode_tps": 57.532507335683924,
        "overall_tps": 57.532507335683924,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "hot_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 577.3828749661334,
        "e2e_ms": 8091.883874963969,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.19565446223761,
        "decode_tps": 58.15422740646729,
        "overall_tps": 58.15422740646729,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "cold_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9936.399709025864,
        "e2e_ms": 26236.290959001053,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.29989124997519,
        "decode_tps": 61.35010256596173,
        "overall_tps": 61.35010256596173,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "warm_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9009.541416948196,
        "e2e_ms": 25269.92429199163,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.260382875043433,
        "decode_tps": 61.49916688215307,
        "overall_tps": 61.49916688215307,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "hot_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 556.0550420195796,
        "e2e_ms": 8054.527750005946,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.158976448481386,
        "decode_tps": 58.27853444536329,
        "overall_tps": 58.27853444536329,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "cold_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9940.000707982108,
        "e2e_ms": 18338.343249983154,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.796685084002092,
        "decode_tps": 59.53555686725617,
        "overall_tps": 59.53555686725617,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "warm_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 8982.174791046418,
        "e2e_ms": 17396.261708054226,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.828173834015615,
        "decode_tps": 59.424154389150104,
        "overall_tps": 59.424154389150104,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "hot_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 572.2911669872701,
        "e2e_ms": 8077.6825830107555,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.17480873231919,
        "decode_tps": 58.22481144248328,
        "overall_tps": 58.22481144248328,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "cold_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9939.507666043937,
        "e2e_ms": 12161.43475001445,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.219270839705132,
        "decode_tps": 45.005977343461325,
        "overall_tps": 45.005977343461325,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "warm_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 8989.343249995727,
        "e2e_ms": 11222.217041999102,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.328737920033745,
        "decode_tps": 44.78533464727453,
        "overall_tps": 44.78533464727453,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "hot_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 559.0262500336394,
        "e2e_ms": 2841.3005000329576,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.822742499993183,
        "decode_tps": 43.8159436798754,
        "overall_tps": 43.8159436798754,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n\n",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "cold_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4605.306125013158,
        "e2e_ms": 6348.227499984205,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.42408296434555,
        "decode_tps": 64.833676161598,
        "overall_tps": 64.833676161598,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "warm_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4187.735290965065,
        "e2e_ms": 5921.888665994629,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.346490044509416,
        "decode_tps": 65.16147973247959,
        "overall_tps": 65.16147973247959,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "hot_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 509.2992080026306,
        "e2e_ms": 7296.937833016273,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.309095343164003,
        "decode_tps": 75.1365869892602,
        "overall_tps": 75.1365869892602,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "cold_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4597.095459001139,
        "e2e_ms": 6334.24274995923,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.372984875735325,
        "decode_tps": 65.04917607629974,
        "overall_tps": 65.04917607629974,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "warm_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4170.511959004216,
        "e2e_ms": 5905.732584011275,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.355934734575747,
        "decode_tps": 65.12140206928458,
        "overall_tps": 65.12140206928458,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "hot_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 499.57279197406024,
        "e2e_ms": 7296.681749983691,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.327664623548294,
        "decode_tps": 75.03190005495236,
        "overall_tps": 75.03190005495236,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "cold_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4595.235084008891,
        "e2e_ms": 6326.336333993822,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.319480088362223,
        "decode_tps": 65.27636670644402,
        "overall_tps": 65.27636670644402,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "warm_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4177.0000840188,
        "e2e_ms": 5908.860708994325,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.326200221022344,
        "decode_tps": 65.24774474943499,
        "overall_tps": 65.24774474943499,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "hot_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 496.0736249922775,
        "e2e_ms": 7159.170917002484,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.326194584020413,
        "decode_tps": 75.0401769758871,
        "overall_tps": 75.0401769758871,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "cold_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4604.158958012704,
        "e2e_ms": 6179.904292046558,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.757453340338543,
        "decode_tps": 63.46203148449338,
        "overall_tps": 63.46203148449338,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "warm_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4210.47308400739,
        "e2e_ms": 5783.400666958187,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.72927582950797,
        "decode_tps": 63.57571771511627,
        "overall_tps": 63.57571771511627,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "hot_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 505.0544579862617,
        "e2e_ms": 2130.617832997814,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.25563375011552,
        "decode_tps": 61.5171340208679,
        "overall_tps": 61.5171340208679,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memoryy resources",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "cold_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1594.9420409742743,
        "e2e_ms": 3050.0534580205567,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.125928475385686,
        "decode_tps": 82.46791179989978,
        "overall_tps": 82.46791179989978,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "warm_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1715.5906669795513,
        "e2e_ms": 3160.050958977081,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.037169099979414,
        "decode_tps": 83.07601161818938,
        "overall_tps": 83.07601161818938,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "hot_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 461.953625024762,
        "e2e_ms": 5002.087958040647,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.210208229668853,
        "decode_tps": 89.20440900940694,
        "overall_tps": 89.20440900940694,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "cold_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1574.3874159525149,
        "e2e_ms": 3013.8033329858445,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.995132641944414,
        "decode_tps": 83.36714814667525,
        "overall_tps": 83.36714814667525,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "warm_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1723.6184580251575,
        "e2e_ms": 3165.235083026346,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.013471875009904,
        "decode_tps": 83.23988355773928,
        "overall_tps": 83.23988355773928,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "hot_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 465.08633298799396,
        "e2e_ms": 5019.709957996383,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.245984259279973,
        "decode_tps": 88.92062952825306,
        "overall_tps": 88.92062952825306,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "cold_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1578.4456669935025,
        "e2e_ms": 3014.67770896852,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.968600349791814,
        "decode_tps": 83.55195852265167,
        "overall_tps": 83.55195852265167,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "warm_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1719.2553749773651,
        "e2e_ms": 3189.23962500412,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.249868750222959,
        "decode_tps": 81.6335277046784,
        "overall_tps": 81.6335277046784,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "hot_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 478.3063340000808,
        "e2e_ms": 5017.139917006716,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.206996501250952,
        "decode_tps": 89.22997342672299,
        "overall_tps": 89.22997342672299,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "cold_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1575.6332909804769,
        "e2e_ms": 2798.796166025568,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.23162875045091,
        "decode_tps": 81.7552609224782,
        "overall_tps": 81.7552609224782,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "warm_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1715.835832990706,
        "e2e_ms": 2940.5718749621883,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.247360419714823,
        "decode_tps": 81.65024672502328,
        "overall_tps": 81.65024672502328,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "hot_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 470.9298750385642,
        "e2e_ms": 1704.1821670136414,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.332522919750772,
        "decode_tps": 81.08640920492276,
        "overall_tps": 81.08640920492276,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especiallyy during",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 100
      }
    },
    "batch_2": {
      "cold_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 118489.24220900517,
        "e2e_ms": 135556.538291974,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 136.5383686637506,
        "decode_tps": 7.323948643788717,
        "overall_tps": 7.323948643788717,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "warm_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 118226.10941698076,
        "e2e_ms": 129144.59066698328,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 87.34785000002012,
        "decode_tps": 11.44847869752684,
        "overall_tps": 11.44847869752684,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "hot_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 43034.941165999044,
        "e2e_ms": 74484.1082909843,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 157.24583562492626,
        "decode_tps": 6.35946889166127,
        "overall_tps": 6.35946889166127,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "cold_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 129752.27820896544,
        "e2e_ms": 147892.17745896894,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 145.11919400002807,
        "decode_tps": 6.890887224744416,
        "overall_tps": 6.890887224744416,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "warm_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 104283.8732090313,
        "e2e_ms": 116982.88374999538,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 101.59208432771266,
        "decode_tps": 9.843286577074553,
        "overall_tps": 9.843286577074553,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "hot_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 25312.106457946356,
        "e2e_ms": 45912.92379196966,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 103.00408667011652,
        "decode_tps": 9.708352671507347,
        "overall_tps": 9.708352671507347,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "cold_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 121557.94037500164,
        "e2e_ms": 139340.3366249986,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 142.2591699999757,
        "decode_tps": 7.029423832573821,
        "overall_tps": 7.029423832573821,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "warm_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 102000.52183395019,
        "e2e_ms": 113052.31366696535,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 88.41433466412127,
        "decode_tps": 11.3103831386496,
        "overall_tps": 11.3103831386496,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "hot_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 25591.486124962103,
        "e2e_ms": 46625.27020799462,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 105.16892041516257,
        "decode_tps": 9.508512553446602,
        "overall_tps": 9.508512553446602,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "cold_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 118579.06079100212,
        "e2e_ms": 135794.63220800972,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 172.15571417007595,
        "decode_tps": 5.808694790183268,
        "overall_tps": 5.808694790183268,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "warm_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 111620.95941702137,
        "e2e_ms": 121962.70595898386,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 103.41746541962493,
        "decode_tps": 9.669546589083547,
        "overall_tps": 9.669546589083547,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "hot_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 39208.05075002136,
        "e2e_ms": 54635.963916021865,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 154.27913166000508,
        "decode_tps": 6.481758026767773,
        "overall_tps": 6.481758026767773,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "cold_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 62253.53979197098,
        "e2e_ms": 201252.4097500136,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.799773991608525,
        "decode_tps": 35.97151546274635,
        "overall_tps": 35.97151546274635,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "warm_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 60626.17633299669,
        "e2e_ms": 198074.05858300626,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.489576450001913,
        "decode_tps": 36.37742479658796,
        "overall_tps": 36.37742479658796,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "hot_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1037.1436669956893,
        "e2e_ms": 7810.394791944418,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 33.53094616311252,
        "decode_tps": 29.823196611734822,
        "overall_tps": 29.823196611734822,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "cold_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31769.207250035834,
        "e2e_ms": 59411.844125017524,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.64263687498169,
        "decode_tps": 36.17599885722416,
        "overall_tps": 36.17599885722416,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "warm_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31739.767874998506,
        "e2e_ms": 58058.72158304555,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 26.318953708047047,
        "decode_tps": 37.995431394913275,
        "overall_tps": 37.995431394913275,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "hot_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1035.785958985798,
        "e2e_ms": 7889.590499980841,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 33.92972545047051,
        "decode_tps": 29.472681748037335,
        "overall_tps": 29.472681748037335,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "cold_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 33184.683415980544,
        "e2e_ms": 47705.78829100123,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 29.042209750041366,
        "decode_tps": 34.43264161393834,
        "overall_tps": 34.43264161393834,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "warm_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 29872.417416016106,
        "e2e_ms": 44658.27762504341,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 29.57172041805461,
        "decode_tps": 33.81609138268004,
        "overall_tps": 33.81609138268004,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "hot_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1293.1079170084558,
        "e2e_ms": 8239.380249986425,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 34.38748679692064,
        "decode_tps": 29.080345589243493,
        "overall_tps": 29.080345589243493,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "cold_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 32647.26541697746,
        "e2e_ms": 37197.868999966886,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 45.506035829894245,
        "decode_tps": 21.97510685699128,
        "overall_tps": 21.97510685699128,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "warm_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 29639.28974996088,
        "e2e_ms": 34098.816624959,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 44.59526874998119,
        "decode_tps": 22.42390343259052,
        "overall_tps": 22.42390343259052,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "hot_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1286.965083971154,
        "e2e_ms": 5789.481291954871,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 45.02516207983717,
        "decode_tps": 22.209803447832837,
        "overall_tps": 22.209803447832837,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inferencee on",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "cold_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 10773.070459021255,
        "e2e_ms": 97303.99304203456,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.306184516602663,
        "decode_tps": 57.78281163249193,
        "overall_tps": 57.78281163249193,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "warm_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 20122.89833300747,
        "e2e_ms": 107540.31208303059,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.483482750004622,
        "decode_tps": 57.19684197358993,
        "overall_tps": 57.19684197358993,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "hot_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 588.3934999583289,
        "e2e_ms": 8190.214374975767,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.395471109879722,
        "decode_tps": 57.48622694283066,
        "overall_tps": 57.48622694283066,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "cold_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9933.297375042457,
        "e2e_ms": 26249.666167015675,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.31636879197322,
        "decode_tps": 61.28814644664973,
        "overall_tps": 61.28814644664973,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "warm_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9031.087500043213,
        "e2e_ms": 25322.92495801812,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.291837457974907,
        "decode_tps": 61.380430696016845,
        "overall_tps": 61.380430696016845,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "hot_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 561.3816670374945,
        "e2e_ms": 8100.670833024196,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.25237795420298,
        "decode_tps": 57.96302414974526,
        "overall_tps": 57.96302414974526,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "cold_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9942.92670802679,
        "e2e_ms": 18359.74799998803,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.833642583922483,
        "decode_tps": 59.40484924843791,
        "overall_tps": 59.40484924843791,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "warm_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 8982.189542031847,
        "e2e_ms": 17386.354417016264,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.808329749968834,
        "decode_tps": 59.49431114664169,
        "overall_tps": 59.49431114664169,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "hot_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 564.8806249955669,
        "e2e_ms": 8083.886042004451,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.20596205265191,
        "decode_tps": 58.11938890367788,
        "overall_tps": 58.11938890367788,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "cold_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9941.802499990445,
        "e2e_ms": 12177.255832997616,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.354533330071718,
        "decode_tps": 44.7336558198145,
        "overall_tps": 44.7336558198145,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "warm_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 8967.33995800605,
        "e2e_ms": 11195.233958016615,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.27894000010565,
        "decode_tps": 44.8854388940972,
        "overall_tps": 44.8854388940972,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "hot_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 561.2799170194194,
        "e2e_ms": 2858.1390000181273,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.96859082998708,
        "decode_tps": 43.5377166758716,
        "overall_tps": 43.5377166758716,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n\n",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "cold_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4616.593540995382,
        "e2e_ms": 6360.973208036739,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.43698820390582,
        "decode_tps": 64.77947555514638,
        "overall_tps": 64.77947555514638,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "warm_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4167.075041972566,
        "e2e_ms": 5909.651957976166,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.421034654899113,
        "decode_tps": 64.84649197531695,
        "overall_tps": 64.84649197531695,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "hot_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 500.18854200607166,
        "e2e_ms": 7351.1392080108635,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.433236600009396,
        "decode_tps": 74.44222340275765,
        "overall_tps": 74.44222340275765,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "cold_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4595.956458011642,
        "e2e_ms": 6346.186415990815,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.488760690081172,
        "decode_tps": 64.56294470611769,
        "overall_tps": 64.56294470611769,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "warm_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4157.055667019449,
        "e2e_ms": 5915.515917004086,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.561595132607406,
        "decode_tps": 64.26076449609097,
        "overall_tps": 64.26076449609097,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "hot_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 504.8769590212032,
        "e2e_ms": 7308.152959041763,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.339756862785412,
        "decode_tps": 74.96388504574249,
        "overall_tps": 74.96388504574249,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "cold_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4594.089167017955,
        "e2e_ms": 6334.160041995347,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.398857300684877,
        "decode_tps": 64.93988355587425,
        "overall_tps": 64.93988355587425,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "warm_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4176.162207964808,
        "e2e_ms": 5920.717332977802,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.43854092931853,
        "decode_tps": 64.77296038390209,
        "overall_tps": 64.77296038390209,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "hot_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 502.15958297485486,
        "e2e_ms": 7164.02070800541,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.32372225006111,
        "decode_tps": 75.05410134134351,
        "overall_tps": 75.05410134134351,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "cold_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4595.605499984231,
        "e2e_ms": 6178.929874964524,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.833243749802932,
        "decode_tps": 63.15825208037023,
        "overall_tps": 63.15825208037023,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "warm_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4168.625999998767,
        "e2e_ms": 5755.145749950316,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.865197499515489,
        "decode_tps": 63.0310464165693,
        "overall_tps": 63.0310464165693,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "hot_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 495.50745903979987,
        "e2e_ms": 2122.0845840289257,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.26577124989126,
        "decode_tps": 61.47879400472236,
        "overall_tps": 61.47879400472236,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memoryy resources",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "cold_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1581.8915420095436,
        "e2e_ms": 3044.5089169661514,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.188478124638399,
        "decode_tps": 82.04469744081914,
        "overall_tps": 82.04469744081914,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "warm_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1711.3117079716176,
        "e2e_ms": 3192.5874169683084,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.34396424163909,
        "decode_tps": 81.01125217349262,
        "overall_tps": 81.01125217349262,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "hot_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 462.7219589892775,
        "e2e_ms": 5011.193667014595,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.230794340803254,
        "decode_tps": 89.04089681055255,
        "overall_tps": 89.04089681055255,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "cold_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1577.3890410200693,
        "e2e_ms": 3025.626791000832,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.068647916506356,
        "decode_tps": 82.859323340794,
        "overall_tps": 82.859323340794,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "warm_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1709.4263749895617,
        "e2e_ms": 3153.8202079827897,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.036615274943566,
        "decode_tps": 83.0798340860561,
        "overall_tps": 83.0798340860561,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "hot_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 465.790249989368,
        "e2e_ms": 5007.381457951851,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.213805451759217,
        "decode_tps": 89.17579356106275,
        "overall_tps": 89.17579356106275,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "cold_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1571.177291974891,
        "e2e_ms": 3018.646082957275,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.062239924853202,
        "decode_tps": 82.90334185275046,
        "overall_tps": 82.90334185275046,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "warm_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1716.5584579925053,
        "e2e_ms": 3182.4953330215067,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.216140625241678,
        "decode_tps": 81.85891360269247,
        "overall_tps": 81.85891360269247,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "hot_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 467.6334169926122,
        "e2e_ms": 5019.327541987877,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.238750925914236,
        "decode_tps": 88.97785942512587,
        "overall_tps": 88.97785942512587,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "cold_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1573.9397500292398,
        "e2e_ms": 2799.986208032351,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.260464580031112,
        "decode_tps": 81.56297777073814,
        "overall_tps": 81.56297777073814,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "warm_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1713.6729170451872,
        "e2e_ms": 2971.1016670335084,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.574287499883212,
        "decode_tps": 79.52736884768126,
        "overall_tps": 79.52736884768126,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "hot_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 468.9564169966616,
        "e2e_ms": 1699.0523329586722,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.300959159620106,
        "decode_tps": 81.29447362793157,
        "overall_tps": 81.29447362793157,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especiallyy during",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "concurrent_2x_50000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 216500.998582982,
        "system_tps": 0.0,
        "total_output_tokens": 0,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 216499.084375042,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 216496.79041601485,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          }
        ],
        "error": "Server disconnected without sending a response.",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "concurrent_2x_50000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 214242.61066695908,
        "system_tps": 0.0,
        "total_output_tokens": 0,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 214235.1109170122,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 214231.32883303333,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          }
        ],
        "error": "Server disconnected without sending a response.",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "concurrent_2x_25000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 109675.3964999807,
        "system_tps": 1.8235630449718518,
        "total_output_tokens": 200,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 109673.40129200602,
            "e2e_ms": 109673.40129200602,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 1096.7340129200602,
            "decode_tps": 0.9117981098602884,
            "overall_tps": 0.9117981098602884,
            "output_tokens": 100,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 109668.06120797992,
            "e2e_ms": 109668.06120797992,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 1096.6806120797992,
            "decode_tps": 0.9118425081880044,
            "overall_tps": 0.9118425081880044,
            "output_tokens": 100,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool",
            "error": null
          }
        ],
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "concurrent_2x_25000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 135854.73704099422,
        "system_tps": 7.360803324055234,
        "total_output_tokens": 1000,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 135850.57800001232,
            "e2e_ms": 135850.57800001232,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 271.70115600002464,
            "decode_tps": 3.6805143368617443,
            "overall_tps": 3.6805143368617443,
            "output_tokens": 500,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 135840.03462496912,
            "e2e_ms": 135840.03462496912,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 271.68006924993824,
            "decode_tps": 3.6808000040666484,
            "overall_tps": 3.6808000040666484,
            "output_tokens": 500,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the",
            "error": null
          }
        ],
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "concurrent_2x_10000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 64197.6622910006,
        "system_tps": 3.115378237503775,
        "total_output_tokens": 200,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 64194.731707975734,
            "e2e_ms": 64194.731707975734,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 641.9473170797573,
            "decode_tps": 1.5577602295294852,
            "overall_tps": 1.5577602295294852,
            "output_tokens": 100,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 64196.2690409855,
            "e2e_ms": 64196.2690409855,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 641.962690409855,
            "decode_tps": 1.55772292523972,
            "overall_tps": 1.55772292523972,
            "output_tokens": 100,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple",
            "error": null
          }
        ],
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "concurrent_2x_10000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 72668.17333397921,
        "system_tps": 13.761182566184113,
        "total_output_tokens": 1000,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 72666.09200002858,
            "e2e_ms": 72666.09200002858,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 145.33218400005717,
            "decode_tps": 6.880788359993314,
            "overall_tps": 6.880788359993314,
            "output_tokens": 500,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 72667.06883296138,
            "e2e_ms": 72667.06883296138,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 145.33413766592275,
            "decode_tps": 6.880695864440906,
            "overall_tps": 6.880695864440906,
            "output_tokens": 500,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-",
            "error": null
          }
        ],
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "concurrent_2x_5000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 52874.9189999653,
        "system_tps": 3.7825117046539822,
        "total_output_tokens": 200,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 52872.76383303106,
            "e2e_ms": 52872.76383303106,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 528.7276383303106,
            "decode_tps": 1.8913329425296144,
            "overall_tps": 1.8913329425296144,
            "output_tokens": 100,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 52872.95137497131,
            "e2e_ms": 52872.95137497131,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 528.7295137497131,
            "decode_tps": 1.891326233915087,
            "overall_tps": 1.891326233915087,
            "output_tokens": 100,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound",
            "error": null
          }
        ],
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "concurrent_2x_5000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 51987.04362497665,
        "system_tps": 4.308765884359338,
        "total_output_tokens": 224,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 51986.529832996894,
            "e2e_ms": 51986.529832996894,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 464.1654449374723,
            "decode_tps": 2.154404234323626,
            "overall_tps": 2.154404234323626,
            "output_tokens": 112,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 51983.02662500646,
            "e2e_ms": 51983.02662500646,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 464.13416629470055,
            "decode_tps": 2.154549422601768,
            "overall_tps": 2.154549422601768,
            "output_tokens": 112,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "concurrent_2x_2000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 45969.19654199155,
        "system_tps": 4.350739517870531,
        "total_output_tokens": 200,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 45961.47358400049,
            "e2e_ms": 45961.47358400049,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 459.61473584000487,
            "decode_tps": 2.17573528875738,
            "overall_tps": 2.17573528875738,
            "output_tokens": 100,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 45964.733084023464,
            "e2e_ms": 45964.733084023464,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 459.64733084023464,
            "decode_tps": 2.1755810007033034,
            "overall_tps": 2.1755810007033034,
            "output_tokens": 100,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes",
            "error": null
          }
        ],
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "concurrent_2x_2000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 2,
        "wall_ms": 40677.19154100632,
        "system_tps": 5.850944742831477,
        "total_output_tokens": 238,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 40675.2927499474,
            "e2e_ms": 40675.2927499474,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 341.80918277266727,
            "decode_tps": 2.9256089373850633,
            "overall_tps": 2.9256089373850633,
            "output_tokens": 119,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 40675.70591601543,
            "e2e_ms": 40675.70591601543,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 341.8126547564322,
            "decode_tps": 2.925579220326342,
            "overall_tps": 2.925579220326342,
            "output_tokens": 119,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "context_tokens": 2000,
        "max_output_tokens": 500
      }
    },
    "batch_3": {
      "cold_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 122246.345749998,
        "e2e_ms": 136957.4720840319,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 117.68901067227125,
        "decode_tps": 8.496970059377093,
        "overall_tps": 8.496970059377093,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "warm_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 108565.64908300061,
        "e2e_ms": 123026.33983298438,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 115.68552599987015,
        "decode_tps": 8.644123725565482,
        "overall_tps": 8.644123725565482,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "hot_50000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 27583.229917043354,
        "e2e_ms": 60266.02970901877,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 163.41399895987706,
        "decode_tps": 6.119426771053619,
        "overall_tps": 6.119426771053619,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 5000
      },
      "cold_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 117357.87454101956,
        "e2e_ms": 136154.12770799594,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 150.37002533581108,
        "decode_tps": 6.650261564874838,
        "overall_tps": 6.650261564874838,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "warm_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 111986.70524999034,
        "e2e_ms": 129601.76066699205,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 140.9204433360137,
        "decode_tps": 7.096202483663628,
        "overall_tps": 7.096202483663628,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "hot_50000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31131.250291946344,
        "e2e_ms": 51593.375624972396,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 102.31062666513026,
        "decode_tps": 9.774155750927703,
        "overall_tps": 9.774155750927703,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 1000
      },
      "cold_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 115170.31479097204,
        "e2e_ms": 130300.4364160006,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 121.0409730002284,
        "decode_tps": 8.261665246181662,
        "overall_tps": 8.261665246181662,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "warm_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 113936.5659590112,
        "e2e_ms": 127990.56700000074,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 112.43200832791626,
        "decode_tps": 8.894264319137893,
        "overall_tps": 8.894264319137893,
        "output_tokens": 125,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "hot_50000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 28638.57354200445,
        "e2e_ms": 47078.71895795688,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 92.20072707976215,
        "decode_tps": 10.84590145514696,
        "overall_tps": 10.84590145514696,
        "output_tokens": 200,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capable of preserving semantic context across conversations, which is crucial for maintaining the integrity of the information being processed. The system's ability to efficiently manage memory usage is further demonstrated by its capability to handle the hot tier capacity of an agent's cache, ensuring that the least recently used agent's cache is evicted to disk via safetensors serialization when the capacity is exceeded. This architecture is a key component in enhancing the performance of the system, particularly in scenarios where memory usage is a critical factor..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "cold_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 119165.39108299185,
        "e2e_ms": 133557.86424997495,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 143.9247316698311,
        "decode_tps": 6.9480761811947565,
        "overall_tps": 6.9480761811947565,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "warm_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 99842.68091700505,
        "e2e_ms": 110438.77591699129,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 105.96094999986235,
        "decode_tps": 9.437438981070848,
        "overall_tps": 9.437438981070848,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests thee cache",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "hot_50000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 41795.25658302009,
        "e2e_ms": 67841.47775004385,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 260.4622116702376,
        "decode_tps": 3.8393285290307917,
        "overall_tps": 3.8393285290307917,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated cache architecture designed for efficient memory management during long-context inference tasks on Apple Silicon devices. It employs a block-based KV cache where each block stores a fixed number of token key-value pairs, which are allocated from a shared pool and assigned per-layer per-agent. The system is capable of handling the high memory usage during long-context inference by processing input tokens in adaptive chunks, ensuring that the memory usage stays within acceptable limits. The architecture is capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "cold_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 63292.801542032976,
        "e2e_ms": 206434.60212502396,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 28.628360116598195,
        "decode_tps": 34.930397547298504,
        "overall_tps": 34.930397547298504,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "warm_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 77066.39745796565,
        "e2e_ms": 216687.39666597685,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.92419984160224,
        "decode_tps": 35.81123203788896,
        "overall_tps": 35.81123203788896,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecturee where",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "hot_25000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1242.5149999908172,
        "e2e_ms": 8267.577833961695,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 34.77753878203405,
        "decode_tps": 28.754191211386022,
        "overall_tps": 28.754191211386022,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 5000
      },
      "cold_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 32172.2437919816,
        "e2e_ms": 59970.298500033095,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.798054708051495,
        "decode_tps": 35.97374026716904,
        "overall_tps": 35.97374026716904,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "warm_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 37123.88737499714,
        "e2e_ms": 63475.07979202783,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 26.35119241703069,
        "decode_tps": 37.94894683223911,
        "overall_tps": 37.94894683223911,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management whilee preserving",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "hot_25000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 2635.390584007837,
        "e2e_ms": 9640.689667023253,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 34.679698430769385,
        "decode_tps": 28.835314182338884,
        "overall_tps": 28.835314182338884,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 1000
      },
      "cold_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 31941.030791960657,
        "e2e_ms": 46887.135458993725,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 29.892209334066138,
        "decode_tps": 33.45353261862673,
        "overall_tps": 33.45353261862673,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "warm_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 29859.991499979515,
        "e2e_ms": 43796.62904201541,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 27.873275084071793,
        "decode_tps": 35.87665952363994,
        "overall_tps": 35.87665952363994,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requestss the",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "hot_25000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1255.757624981925,
        "e2e_ms": 8553.41595801292,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 36.12702145064849,
        "decode_tps": 27.68011200054384,
        "overall_tps": 27.68011200054384,
        "output_tokens": 202,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inference on Apple Silicon. In this phase, input tokens are processed in adaptive chunks to bound peak memory usage. This approach ensures that memory usage remains under control, preventing potential overflow during the inference process.\n\nThe system's architecture is highly efficient, allowing for seamless integration of the KV cache with other components, such as the prefill phase, to maintain optimal performance and context preservation. This architecture is particularly beneficial for applications requiring complex interactions and data management, ensuring that the system can handle various scenarios with ease..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "cold_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 32829.476916987915,
        "e2e_ms": 37711.13474998856,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 48.81657833000645,
        "decode_tps": 20.48484416994303,
        "overall_tps": 20.48484416994303,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "warm_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 29746.55379197793,
        "e2e_ms": 34816.35391700547,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 50.69800125027541,
        "decode_tps": 19.724643483742224,
        "overall_tps": 19.724643483742224,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a sharedd pool",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "hot_25000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1124.1969589609653,
        "e2e_ms": 5578.276708954945,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 44.5407974999398,
        "decode_tps": 22.45132678644453,
        "overall_tps": 22.45132678644453,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and the prefill phase:\n\nThe block-based KV cache architecture is a sophisticated system designed to optimize memory usage by storing data in fixed-size blocks. Each block is allocated from a shared pool, allowing multiple agents or layers to access the same data, which is crucial for maintaining semantic context across conversations.\n\nThe prefill phase is a critical component of the system, particularly during long-context inferencee on",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "cold_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 10244.27512503462,
        "e2e_ms": 96851.79233300732,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.32150344159454,
        "decode_tps": 57.73170922326962,
        "overall_tps": 57.73170922326962,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "warm_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 11508.94079200225,
        "e2e_ms": 98380.21191698499,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.374254224996548,
        "decode_tps": 57.55642728890705,
        "overall_tps": 57.55642728890705,
        "output_tokens": 5000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. Thee pref",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "hot_10000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 592.7515419898555,
        "e2e_ms": 8112.545916985255,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.207767448502057,
        "decode_tps": 58.11329116300037,
        "overall_tps": 58.11329116300037,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 5000
      },
      "cold_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9960.836458019912,
        "e2e_ms": 26261.799458006863,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.30096299998695,
        "decode_tps": 61.34606894088408,
        "overall_tps": 61.34606894088408,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "warm_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9085.57216601912,
        "e2e_ms": 25381.454124988522,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.295881958969403,
        "decode_tps": 61.36519658879775,
        "overall_tps": 61.36519658879775,
        "output_tokens": 1000,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent iss ev",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "hot_10000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 587.7502500079572,
        "e2e_ms": 8109.485458000563,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.21220871394189,
        "decode_tps": 58.098296193096935,
        "overall_tps": 58.098296193096935,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 1000
      },
      "cold_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9935.962249990553,
        "e2e_ms": 18357.659791014157,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.84339508204721,
        "decode_tps": 59.37045323278472,
        "overall_tps": 59.37045323278472,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "warm_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9066.167958022561,
        "e2e_ms": 17455.84016700741,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.779344417969696,
        "decode_tps": 59.597084074933136,
        "overall_tps": 59.597084074933136,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer perr-",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "hot_10000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 546.3363329763524,
        "e2e_ms": 8048.608749988489,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 17.16767143481038,
        "decode_tps": 58.24901785878366,
        "overall_tps": 58.24901785878366,
        "output_tokens": 437,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n### Implementation on Apple Silicon\nFor the large language model running on Apple Silicon, the architecture ensures that the memory usage is optimized by allocating blocks of data that are specific to each layer and agent. This allows for efficient memory management, as the least recently used (LRU) agent's cache can be evicted to disk via safetensors serialization when the cache exceeds the hot tier capacity.\n\n### Adaptive Chunking for Prefill Phase\nDuring the prefill phase, the system processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. This adaptive chunking ensures that the memory usage stays within the predefined limits, preventing potential crashes or slowdowns due to excessive memory consumption.\n\n### Efficient Memory Management\nThe system's design enables efficient memory management by loading the cache from disk and reconstructing it into quantized KV blocks when needed, which is crucial for preserving semantic context across conversations. This design ensures that the system can efficiently handle requests for data that has been previously cached, thus enhancing performance and responsiveness.\n\n### Enhancing Performance and Responsiveness\nThe implementation of the block-based KV cache architecture on Apple Silicon contributes to enhancing performance and responsiveness in large language models by optimizing memory usage and efficiently handling requests for cached data. This architecture ensures that the system can handle long-context inference with optimal memory usage, making it suitable for complex language processing tasks.\n\n### Conclusion\nThe system's block-based KV cache architecture, when implemented on Apple Silicon, ensures efficient memory management and optimal performance for large language models, enhancing the system's ability to handle complex language processing tasks while maintaining efficient memory usage..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "cold_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9977.258457976859,
        "e2e_ms": 12257.062332995702,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.798038750188425,
        "decode_tps": 43.86342224248281,
        "overall_tps": 43.86342224248281,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "warm_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 9052.281333017163,
        "e2e_ms": 11284.93345802417,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.326521250070073,
        "decode_tps": 44.78978112171691,
        "overall_tps": 44.78978112171691,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference onn Apple",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "hot_10000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 570.3525839489885,
        "e2e_ms": 2866.3477919762954,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 22.95995208027307,
        "decode_tps": 43.55409787022982,
        "overall_tps": 43.55409787022982,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "Certainly! Here's a comprehensive summary of the block-based KV cache architecture and its implementation in a large language model running on Apple Silicon:\n\n### Block-Based KV Cache Architecture\nThe system's architecture is designed to efficiently manage memory usage by allocating fixed-size blocks of data, each containing a specific number of token key-value pairs. These blocks are dynamically allocated from a shared pool, ensuring that each layer and agent has access to a specific set of cached data.\n\n\n",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "cold_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4642.94291700935,
        "e2e_ms": 6377.616499958094,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.351093654413667,
        "decode_tps": 65.14193858184724,
        "overall_tps": 65.14193858184724,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "warm_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4206.596667005215,
        "e2e_ms": 5957.146000000648,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.491587017658702,
        "decode_tps": 64.55116566560355,
        "overall_tps": 64.55116566560355,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "hot_5000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 489.91679097525775,
        "e2e_ms": 7300.871415995061,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.35481299023491,
        "decode_tps": 74.87937125972516,
        "overall_tps": 74.87937125972516,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 5000
      },
      "cold_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4633.4332919796,
        "e2e_ms": 6376.644333009608,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.426646380796534,
        "decode_tps": 64.82290287309783,
        "overall_tps": 64.82290287309783,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "warm_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4227.327250002418,
        "e2e_ms": 5953.434291004669,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.275283548692489,
        "decode_tps": 65.4652332189013,
        "overall_tps": 65.4652332189013,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "hot_5000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 514.3674580031075,
        "e2e_ms": 7311.241457995493,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.327203921553696,
        "decode_tps": 75.03449379826246,
        "overall_tps": 75.03449379826246,
        "output_tokens": 510,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capable of handling long-running tasks without performance degradation..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 1000
      },
      "cold_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4614.804874989204,
        "e2e_ms": 6352.913333976176,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.381490787495324,
        "decode_tps": 65.01320410457022,
        "overall_tps": 65.01320410457022,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "warm_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4206.642583012581,
        "e2e_ms": 5958.863541018218,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.50638015934192,
        "decode_tps": 64.48958362455363,
        "overall_tps": 64.48958362455363,
        "output_tokens": 113,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "hot_5000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 517.659957986325,
        "e2e_ms": 7189.0244169626385,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 13.342728917952627,
        "decode_tps": 74.94718705215551,
        "overall_tps": 74.94718705215551,
        "output_tokens": 500,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memory resources.\n\n2. **Fixed Number of Token Key-Value Pairs**: Each block is pre-allocated to store a specific number of key-value pairs, ensuring that the system can handle a predefined amount of data efficiently.\n\n3. **Cache Allocation**: Blocks are allocated from a shared pool, which allows for flexible allocation of memory space based on the system's needs. This dynamic allocation helps in optimizing the use of memory by the system.\n\n4. **Least Recently Used (LRU) Block Eviction**: When an agent's cache exceeds the hot tier capacity, the least recently used agent's block is evicted to disk via safetensors serialization. This mechanism ensures that the cache is periodically refreshed and optimized, preventing memory overload.\n\n5. **Reconstruction on Subsequent Requests**: On subsequent requests, the evicted blocks are loaded from disk and reconstructed into quantized KV blocks. This process ensures that the data remains accessible and usable, even after being temporarily removed from the cache.\n\n6. **Efficient Memory Management**: The system's architecture promotes efficient memory management by pre-allocating memory for each block, preventing memory fragmentation and overuse.\n\n7. **Adaptive Chunking for Prefill Phase**: During the prefill phase, input tokens are processed in adaptive chunks to bound memory usage during long-context inference tasks. This feature ensures that the system can handle large data inputs efficiently, optimizing performance during computationally intensive tasks.\n\n### Summary:\nThe system's architecture is tailored to efficiently handle long sequences of tokens, particularly in natural language processing tasks, by using a block-based KV cache. The system's design ensures that memory is efficiently managed by pre-allocating memory for each block and pre-fetching data based on system needs. The system's ability to handle large data inputs and process them efficiently makes it suitable for complex computations, ensuring that the system remains responsive and capablee of",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "cold_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4599.041167006362,
        "e2e_ms": 6177.4580419878475,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.784168749814853,
        "decode_tps": 63.35461916622818,
        "overall_tps": 63.35461916622818,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "warm_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 4210.352124995552,
        "e2e_ms": 5793.889708002098,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 15.835375830065459,
        "decode_tps": 63.14974843232794,
        "overall_tps": 63.14974843232794,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks too bound",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "hot_5000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 504.70654101809487,
        "e2e_ms": 2135.5616659857333,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 16.308551249676384,
        "decode_tps": 61.31752506341379,
        "overall_tps": 61.31752506341379,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system described is a sophisticated block-based key-value (KV) cache architecture, specifically designed for memory management during complex computations, particularly in the context of natural language processing tasks where long sequences of tokens need to be processed.\n\n### Key Features:\n1. **Block-based Design**: Each block in the system is designed to store a fixed number of token key-value pairs. This design ensures that the memory usage is efficiently managed, preventing overuse or underuse of memoryy resources",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "cold_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1602.9566660290584,
        "e2e_ms": 3048.52574999677,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.046409033064265,
        "decode_tps": 83.01228999075656,
        "overall_tps": 83.01228999075656,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "warm_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1723.5044169938192,
        "e2e_ms": 3191.7827089782804,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.235652433203844,
        "decode_tps": 81.72837578209592,
        "overall_tps": 81.72837578209592,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "hot_2000in_5000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 465.3404999990016,
        "e2e_ms": 5050.104249967262,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.320404320909285,
        "decode_tps": 88.33606748064254,
        "overall_tps": 88.33606748064254,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 5000
      },
      "cold_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1583.045166975353,
        "e2e_ms": 3033.301916962955,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.085472916563353,
        "decode_tps": 82.74396930131567,
        "overall_tps": 82.74396930131567,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "warm_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1746.6027089976706,
        "e2e_ms": 3192.381959001068,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.048160416694978,
        "decode_tps": 83.00022288998684,
        "overall_tps": 83.00022288998684,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "hot_2000in_1000out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 471.58383298665285,
        "e2e_ms": 5060.776499973144,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.331339918485165,
        "decode_tps": 88.25081651364718,
        "overall_tps": 88.25081651364718,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 1000
      },
      "cold_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1591.444291989319,
        "e2e_ms": 3034.6243340172805,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.026500350233013,
        "decode_tps": 83.14970863328708,
        "overall_tps": 83.14970863328708,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "warm_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1721.6025000088848,
        "e2e_ms": 3179.6421249746345,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.150330208047913,
        "decode_tps": 82.30228996884696,
        "overall_tps": 82.30228996884696,
        "output_tokens": 120,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon..",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "hot_2000in_500out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 482.61775000719354,
        "e2e_ms": 5047.131458006334,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 11.270404217281827,
        "decode_tps": 88.72796225592501,
        "overall_tps": 88.72796225592501,
        "output_tokens": 405,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especially during processes that require extensive data processing, such as long-context inference tasks. To achieve this, the system employs a pre-fetching mechanism that involves caching data in manageable chunks called \"blocks.\" These blocks are allocated from a shared pool and assigned to specific layers and agents within the system.\n\nThe assignment of these blocks to specific layers and agents ensures that each agent has access to a fixed number of pre-allocated blocks, which helps in efficiently managing memory usage. This architecture is particularly beneficial in scenarios where memory usage is a critical factor, such as during complex data processing tasks where large amounts of data need to be handled efficiently.\n\nFurthermore, the system's block-based KV cache architecture ensures that the least recently used data is evicted from memory to make space for new data, thus maintaining an optimal memory usage for efficient processing. This mechanism helps in maintaining the system's performance and ensures that the system can handle large amounts of data efficiently during long-context inference tasks.\n\nIn summary, the system's block-based KV cache architecture is a sophisticated system designed to handle large volumes of data efficiently during long-context inference tasks. The architecture ensures that data is pre-fetched and cached in manageable chunks called \"blocks,\" which are allocated from a shared pool and assigned to specific layers and agents within the system. This architecture helps in maintaining optimal memory usage, especially during complex data processing tasks, and ensures that the system can handle large amounts of data efficiently during long-context inference tasks..",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 500
      },
      "cold_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1580.547958961688,
        "e2e_ms": 2807.0148749975488,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.264669160358608,
        "decode_tps": 81.53501630783173,
        "overall_tps": 81.53501630783173,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "cold",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "warm_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 1726.5817500301637,
        "e2e_ms": 2953.883083013352,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.273013329831883,
        "decode_tps": 81.47958232631514,
        "overall_tps": 81.47958232631514,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phasee processes",
        "error": null,
        "cache_state": "warm",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "hot_2000in_100out": {
        "scenario": "",
        "config": "",
        "ttft_ms": 481.7672919598408,
        "e2e_ms": 1713.499499950558,
        "itl_mean_ms": 0.0,
        "itl_p95_ms": 0.0,
        "itl_p99_ms": 0.0,
        "tpot_ms": 12.317322079907171,
        "decode_tps": 81.18647815755878,
        "overall_tps": 81.18647815755878,
        "output_tokens": 100,
        "input_tokens": 0,
        "cache_created": 0,
        "cache_read": 0,
        "memory_before_mb": 0.0,
        "memory_after_mb": 0.0,
        "peak_memory_mb": 0.0,
        "raw_output": "The system's architecture is a sophisticated block-based Key-Value (KV) cache that is designed to handle large volumes of data efficiently. This architecture is tailored to work with specific layers and agents, ensuring that each agent has access to a fixed number of pre-allocated blocks, which contain token key-value pairs. These blocks are sourced from a shared pool and assigned to various layers and agents within the system.\n\nThe primary function of this architecture is to optimize memory usage, especiallyy during",
        "error": null,
        "cache_state": "hot",
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "concurrent_3x_50000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 300071.27845898503,
        "system_tps": 0.0,
        "total_output_tokens": 0,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 300068.3987090015,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": ""
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 300069.26424999256,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": ""
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 300069.12929198006,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": ""
          }
        ],
        "context_tokens": 50000,
        "max_output_tokens": 100
      },
      "concurrent_3x_50000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 12558.989500044845,
        "system_tps": 0.0,
        "total_output_tokens": 0,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 12558.80887497915,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 12557.66287498409,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 0.0,
            "e2e_ms": 12556.649957958143,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 0.0,
            "decode_tps": 0.0,
            "overall_tps": 0.0,
            "output_tokens": 0,
            "input_tokens": 0,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "",
            "error": "Server disconnected without sending a response."
          }
        ],
        "error": "Server disconnected without sending a response.",
        "context_tokens": 50000,
        "max_output_tokens": 500
      },
      "concurrent_3x_25000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 129470.71933397092,
        "system_tps": 2.3171262316551067,
        "total_output_tokens": 300,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 129469.63350003352,
            "e2e_ms": 129469.63350003352,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 1294.6963350003352,
            "decode_tps": 0.7723818882979545,
            "overall_tps": 0.7723818882979545,
            "output_tokens": 100,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 129469.32862501126,
            "e2e_ms": 129469.32862501126,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 1294.6932862501126,
            "decode_tps": 0.7723837071066862,
            "overall_tps": 0.7723837071066862,
            "output_tokens": 100,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 129464.87604203867,
            "e2e_ms": 129464.87604203867,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 1294.6487604203867,
            "decode_tps": 0.7724102710879582,
            "overall_tps": 0.7724102710879582,
            "output_tokens": 100,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool",
            "error": null
          }
        ],
        "context_tokens": 25000,
        "max_output_tokens": 100
      },
      "concurrent_3x_25000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 176712.50449999934,
        "system_tps": 8.488363651707543,
        "total_output_tokens": 1500,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 176710.42924997164,
            "e2e_ms": 176710.42924997164,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 353.4208584999433,
            "decode_tps": 2.829487779086928,
            "overall_tps": 2.829487779086928,
            "output_tokens": 500,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 176708.94641702762,
            "e2e_ms": 176708.94641702762,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 353.41789283405524,
            "decode_tps": 2.829511522410504,
            "overall_tps": 2.829511522410504,
            "output_tokens": 500,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 176710.8823750168,
            "e2e_ms": 176710.8823750168,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 353.4217647500336,
            "decode_tps": 2.82948052366632,
            "overall_tps": 2.82948052366632,
            "output_tokens": 500,
            "input_tokens": 18058,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "he least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the",
            "error": null
          }
        ],
        "context_tokens": 25000,
        "max_output_tokens": 500
      },
      "concurrent_3x_10000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 38663.549957971554,
        "system_tps": 7.759246120082327,
        "total_output_tokens": 300,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 38662.8854169976,
            "e2e_ms": 38662.8854169976,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 386.62885416997597,
            "decode_tps": 2.586459828888932,
            "overall_tps": 2.586459828888932,
            "output_tokens": 100,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 38663.14133402193,
            "e2e_ms": 38663.14133402193,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 386.6314133402193,
            "decode_tps": 2.586442708730556,
            "overall_tps": 2.586442708730556,
            "output_tokens": 100,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 38663.04616595153,
            "e2e_ms": 38663.04616595153,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 386.6304616595153,
            "decode_tps": 2.5864490751912,
            "overall_tps": 2.5864490751912,
            "output_tokens": 100,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple",
            "error": null
          }
        ],
        "context_tokens": 10000,
        "max_output_tokens": 100
      },
      "concurrent_3x_10000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 59284.46941700531,
        "system_tps": 25.30173609970331,
        "total_output_tokens": 1500,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 59283.237333002035,
            "e2e_ms": 59283.237333002035,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 118.56647466600407,
            "decode_tps": 8.434087315296763,
            "overall_tps": 8.434087315296763,
            "output_tokens": 500,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 59282.77687501395,
            "e2e_ms": 59282.77687501395,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 118.5655537500279,
            "decode_tps": 8.434152824085002,
            "overall_tps": 8.434152824085002,
            "output_tokens": 500,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 59281.980749976356,
            "e2e_ms": 59281.980749976356,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 118.56396149995271,
            "decode_tps": 8.434266090209873,
            "overall_tps": 8.434266090209873,
            "output_tokens": 500,
            "input_tokens": 7225,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon. The system implements a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared",
            "error": null
          }
        ],
        "context_tokens": 10000,
        "max_output_tokens": 500
      },
      "concurrent_3x_5000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 18252.431125030853,
        "system_tps": 16.436166664318417,
        "total_output_tokens": 300,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18252.338666992728,
            "e2e_ms": 18252.338666992728,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 182.52338666992728,
            "decode_tps": 5.478749974152002,
            "overall_tps": 5.478749974152002,
            "output_tokens": 100,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18251.78316695383,
            "e2e_ms": 18251.78316695383,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 182.51783166953828,
            "decode_tps": 5.478916722014166,
            "overall_tps": 5.478916722014166,
            "output_tokens": 100,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18250.983666977845,
            "e2e_ms": 18250.983666977845,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 182.50983666977845,
            "decode_tps": 5.479156730655212,
            "overall_tps": 5.479156730655212,
            "output_tokens": 100,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound",
            "error": null
          }
        ],
        "context_tokens": 5000,
        "max_output_tokens": 100
      },
      "concurrent_3x_5000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 18276.120541035198,
        "system_tps": 18.384645649802014,
        "total_output_tokens": 336,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18276.02566697169,
            "e2e_ms": 18276.02566697169,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 163.17880059796153,
            "decode_tps": 6.128247029243652,
            "overall_tps": 6.128247029243652,
            "output_tokens": 112,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18274.961707997136,
            "e2e_ms": 18274.961707997136,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 163.16930096426015,
            "decode_tps": 6.128603812668386,
            "overall_tps": 6.128603812668386,
            "output_tokens": 112,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 18274.907500017434,
            "e2e_ms": 18274.907500017434,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 163.1688169644414,
            "decode_tps": 6.128621991651293,
            "overall_tps": 6.128621991651293,
            "output_tokens": 112,
            "input_tokens": 3617,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "tecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "context_tokens": 5000,
        "max_output_tokens": 500
      },
      "concurrent_3x_2000in_100out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 45949.41379199736,
        "system_tps": 6.528918983777081,
        "total_output_tokens": 300,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 45948.29633302288,
            "e2e_ms": 45948.29633302288,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 459.4829633302288,
            "decode_tps": 2.1763592555254405,
            "overall_tps": 2.1763592555254405,
            "output_tokens": 100,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 45945.06779202493,
            "e2e_ms": 45945.06779202493,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 459.4506779202493,
            "decode_tps": 2.176512187394309,
            "overall_tps": 2.176512187394309,
            "output_tokens": 100,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 45947.65066698892,
            "e2e_ms": 45947.65066698892,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 459.4765066698892,
            "decode_tps": 2.1763898381826294,
            "overall_tps": 2.1763898381826294,
            "output_tokens": 100,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes",
            "error": null
          }
        ],
        "context_tokens": 2000,
        "max_output_tokens": 100
      },
      "concurrent_3x_2000in_500out": {
        "cache_state": "concurrent",
        "n_concurrent": 3,
        "wall_ms": 7583.213166974019,
        "system_tps": 47.0776690750019,
        "total_output_tokens": 357,
        "per_request": [
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 7582.9439999652095,
            "e2e_ms": 7582.9439999652095,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 63.72221848710261,
            "decode_tps": 15.693113387168093,
            "overall_tps": 15.693113387168093,
            "output_tokens": 119,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 7582.93000003323,
            "e2e_ms": 7582.93000003323,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 63.722100840615376,
            "decode_tps": 15.693142360469967,
            "overall_tps": 15.693142360469967,
            "output_tokens": 119,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          },
          {
            "scenario": "",
            "config": "",
            "ttft_ms": 7582.327291020192,
            "e2e_ms": 7582.327291020192,
            "itl_mean_ms": 0.0,
            "itl_p95_ms": 0.0,
            "itl_p99_ms": 0.0,
            "tpot_ms": 63.717036058993216,
            "decode_tps": 15.694389787279771,
            "overall_tps": 15.694389787279771,
            "output_tokens": 119,
            "input_tokens": 1449,
            "cache_created": 0,
            "cache_read": 0,
            "memory_before_mb": 0.0,
            "memory_after_mb": 0.0,
            "peak_memory_mb": 0.0,
            "raw_output": "a block-based KV cache architecture where each block stores a fixed number of token key-value pairs. Blocks are allocated from a shared pool and assigned per-layer per-agent. When an agent's cache exceeds the hot tier capacity, the least recently used agent is evicted to disk via safetensors serialization. On subsequent requests, the cache is loaded from disk and reconstructed into quantized KV blocks. This design enables efficient memory management while preserving semantic context across conversations. The prefill phase processes input tokens in adaptive chunks to bound peak memory usage during long-context inference on Apple Silicon.",
            "error": null
          }
        ],
        "context_tokens": 2000,
        "max_output_tokens": 500
      }
    }
  }
}