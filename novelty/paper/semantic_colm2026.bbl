\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2024)Agrawal, Kedia, Panwar, Mohan, Kwatra, Gulavani,
  Tumanov, and Ramjee]{agrawal2024sarathi}
Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra,
  Bhargav~S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee.
\newblock Taming throughput-latency tradeoff in llm inference with
  sarathi-serve.
\newblock In \emph{18th USENIX Symposium on Operating Systems Design and
  Implementation}, 2024.

\bibitem[Anonymous(2026)]{memart2026iclr}
Anonymous.
\newblock Kvcache-centric memory for llm agents.
\newblock In \emph{International Conference on Learning Representations}, 2026.
\newblock URL \url{https://openreview.net/forum?id=YolJOZOGhI}.
\newblock Submitted to ICLR 2026.

\bibitem[Barrios(2026)]{barrios2026vllmmlx}
Wayner Barrios.
\newblock Native llm and mllm inference at scale on apple silicon.
\newblock \emph{arXiv preprint arXiv:2601.19139}, 2026.

\bibitem[Chang et~al.(2025)]{chang2025sagallm}
Yifan Chang et~al.
\newblock Sagallm: Multi-agent orchestration with transaction-based context
  management.
\newblock \emph{Proceedings of the VLDB Endowment}, 2025.

\bibitem[Guo et~al.(2024)Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, and
  Zhang]{guo2024multiagent}
Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh~V.
  Chawla, Olaf Wiest, and Xiangliang Zhang.
\newblock Large language model based multi-agents: A survey of progress and
  challenges.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2024.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer,
  and Gholami]{hooper2024kvquant}
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael~W. Mahoney,
  Yakun~Sophia Shao, Kurt Keutzer, and Amir Gholami.
\newblock Kvquant: Towards 10 million context length llm inference with kv
  cache quantization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~37, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin,
  Yang, and Qiu]{jiang2024minference}
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin
  Ahn, Zhenhua Han, Amir~H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and
  Lili Qiu.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via
  dynamic sparse attention.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2024.
\newblock Spotlight.

\bibitem[Jiang et~al.(2025)]{jiang2025emllm}
Yi~Jiang et~al.
\newblock Em-llm: Human-inspired episodic memory for infinite context llms.
\newblock In \emph{International Conference on Learning Representations}, 2025.

\bibitem[Jin et~al.(2024)Jin, Zhang, Jiang, Fan, Zhu, Luo, and
  Jin]{jin2024ragcache}
Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Fan, Xin Zhu, Xuanzhe Luo, and Xin
  Jin.
\newblock Ragcache: Efficient knowledge caching for retrieval-augmented
  generation.
\newblock \emph{ACM Transactions on Computer Systems}, 2024.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023pagedattention}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems
  Principles}, pages 611--626, 2023.

\bibitem[Li et~al.(2025{\natexlab{a}})Li, Mang, He, Zhang, Mao, Chen, Zhou,
  Cheung, Gonzalez, and Stoica]{li2025continuum}
Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen,
  Hangrui Zhou, Alvin Cheung, Joseph Gonzalez, and Ion Stoica.
\newblock Continuum: Efficient and robust multi-turn llm agent scheduling with
  kv cache time-to-live.
\newblock \emph{arXiv preprint arXiv:2511.02230}, 2025{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2511.02230}.

\bibitem[Li et~al.(2025{\natexlab{b}})Li, Tomar, and Gholami]{li2025quantspec}
Minjae Li, Aditya Tomar, and Amir Gholami.
\newblock Quantspec: Self-speculative decoding with hierarchical quantized kv
  cache.
\newblock In \emph{Proceedings of the 42nd International Conference on Machine
  Learning}, PMLR, 2025{\natexlab{b}}.

\bibitem[Li et~al.(2025{\natexlab{c}})Li, Zhang, Zhao, Zheng, Yang, and
  Ding]{li2025commvq}
Shikai Li, Yuke Zhang, Xin Zhao, Zhijie Zheng, Xiaopeng Yang, and Xiaolong
  Ding.
\newblock Commvq: Commutative vector quantization for kv cache compression.
\newblock In \emph{Proceedings of the 42nd International Conference on Machine
  Learning}, PMLR, 2025{\natexlab{c}}.

\bibitem[Li et~al.(2025{\natexlab{d}})]{fusionragcache2025}
Zhenyu Li et~al.
\newblock Fusion rag cache: Optimizing retrieval-augmented generation with
  persistent kv states.
\newblock \emph{arXiv preprint}, 2025{\natexlab{d}}.
\newblock Reports prefill = 95.53\% of RAG inference time.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua,
  Petroni, and Liang]{liu2024lost}
Nelson~F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
  Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  12:\penalty0 157--173, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yuan, Jin, Zhong, Xu, Braverman,
  Chen, and Hu]{liu2024kivi}
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
  Braverman, Beidi Chen, and Xia Hu.
\newblock Kivi: A tuning-free asymmetric 2bit quantization for kv cache.
\newblock In \emph{Proceedings of the 41st International Conference on Machine
  Learning}, volume 235 of \emph{Proceedings of Machine Learning Research},
  pages 32332--32344. PMLR, 2024{\natexlab{b}}.

\bibitem[{LMCache Team}(2025)]{lmcache2025}
{LMCache Team}.
\newblock Lmcache: Engine-agnostic persistent kv store for llm serving.
\newblock \url{https://lmcache.ai/tech_report.pdf}, 2025.

\bibitem[{MICRO 2025 Authors}(2025)]{kelle2025micro}
{MICRO 2025 Authors}.
\newblock Kelle: Co-design kv caching and edram for edge computing.
\newblock In \emph{IEEE/ACM International Symposium on Microarchitecture},
  2025.
\newblock URL \url{https://dl.acm.org/doi/10.1145/3725843.3756071}.

\bibitem[{NDSS 2025 Authors}(2025)]{promptpeek2025}
{NDSS 2025 Authors}.
\newblock I know what you asked: Prompt leakage via kv-cache sharing in
  multi-tenant llm serving.
\newblock In \emph{Network and Distributed System Security Symposium (NDSS)},
  2025.

\bibitem[Nielsen(1993)]{nielsen1993}
Jakob Nielsen.
\newblock \emph{Usability Engineering}.
\newblock Academic Press, Boston, MA, 1993.
\newblock Chapter 5: Response Times: The 3 Important Limits (0.1s, 1.0s, 10s).

\bibitem[Nielsen(2024)]{nielsen2024speed}
Jakob Nielsen.
\newblock The need for speed in the age of ai.
\newblock Nielsen Norman Group, 2024.
\newblock Argues no current AI system meets the 1-second response time
  threshold.

\bibitem[Pan et~al.(2025)Pan, Patel, Hu, Shen, Guan, Li, Qin, Wang, and
  Ding]{pan2025kvflow}
Zaifeng Pan, Ajjkumar Patel, Zhengding Hu, Yipeng Shen, Yue Guan, Wan-Lu Li,
  Lianhui Qin, Yida Wang, and Yufei Ding.
\newblock Kvflow: Efficient prefix caching for accelerating llm-based
  multi-agent workflows.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2025.
\newblock URL \url{https://arxiv.org/abs/2507.07400}.

\bibitem[Perez et~al.(2025)]{perez2025localllm}
Daniel Perez et~al.
\newblock Production-grade local llm inference on apple silicon.
\newblock \emph{arXiv preprint arXiv:2511.05502}, 2025.

\bibitem[Tao et~al.(2025)]{rotatekv2025}
Mingming Tao et~al.
\newblock Rotatekv: Accurate and robust 2-bit kv cache quantization for llms
  via outlier-aware adaptive rotations.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2025.
\newblock URL \url{https://arxiv.org/abs/2501.16383}.

\bibitem[Wen et~al.(2025)]{wen2025krul}
Wei Wen et~al.
\newblock Krul: Optimizing on-device large language model deployment.
\newblock \emph{arXiv preprint}, 2025.

\bibitem[Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Li, Zhu, Jiang, Zhang, Zhang,
  Liu, Awadallah, White, Burger, and Wang]{wu2023autogen}
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu,
  Li~Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed~Hassan Awadallah,
  Ryen~W. White, Doug Burger, and Chi Wang.
\newblock Autogen: Enabling next-gen llm applications via multi-agent
  conversation.
\newblock \emph{arXiv preprint arXiv:2308.08155}, 2023.

\bibitem[Xu et~al.(2025)Xu, Zhang, et~al.]{xu2025amem}
Wujiang Xu, Zujie Zhang, et~al.
\newblock A-mem: Agentic memory for llm agents.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.12110}.

\bibitem[Ye et~al.(2025)Ye, Gao, Ma, Wang, Fu, Chung, Lin, Liu, Zhang, Zhuo,
  and Chen]{ye2025kvcomm}
Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung,
  Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, and Yiran Chen.
\newblock Kvcomm: Online cross-context kv-cache communication for efficient
  llm-based multi-agent systems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2025.
\newblock URL \url{https://arxiv.org/abs/2510.12872}.

\bibitem[Zhang et~al.(2024)Zhang, Xia, and Wang]{zhang2024kvswap}
Huawei Zhang, Chunwei Xia, and Zheng Wang.
\newblock Kvswap: Disk-aware kv cache offloading for long-context on-device
  inference.
\newblock \emph{arXiv preprint arXiv:2511.11907}, 2024.

\bibitem[Zheng et~al.(2024)Zheng, Yin, Xie, Huang, Sun, Yu, Cao, Kozyrakis,
  Stoica, Gonzalez, Barrett, and Sheng]{zheng2024sglang}
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody~Hao
  Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph~E. Gonzalez, Clark
  Barrett, and Ying Sheng.
\newblock Sglang: Efficient execution of structured language model programs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Zhong et~al.(2024)Zhong, Liu, Chen, Hu, Zhu, Liu, Jin, and
  Zhang]{zhong2024distserve}
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin
  Jin, and Hao Zhang.
\newblock Distserve: Disaggregating prefill and decoding for goodput-optimized
  large language model serving.
\newblock In \emph{18th USENIX Symposium on Operating Systems Design and
  Implementation}, 2024.

\end{thebibliography}
