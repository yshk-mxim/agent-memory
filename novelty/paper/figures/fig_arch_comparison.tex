% Figure: Architecture Comparison â€” Gemma 3 vs DeepSeek-Coder-V2-Lite
% Side-by-side layer stacks feeding shared block pool

\begin{figure}[t]
\centering
\begin{tikzpicture}[
    node distance=0.4cm,
    layer/.style={rectangle, draw=black, minimum width=2.4cm, minimum height=0.5cm, align=center, font=\scriptsize},
    pool/.style={rectangle, draw=black, thick, fill=green!15, minimum width=3.2cm, minimum height=1.2cm, align=center},
    spec/.style={rectangle, draw=gray, dashed, minimum width=2.6cm, minimum height=0.7cm, align=center, font=\tiny},
    arrow/.style={->, >=stealth, thick},
    label/.style={font=\scriptsize}
]

% Gemma 3 stack (left)
\node[font=\small\bfseries] (gemma_title) at (-2.8, 4.5) {Gemma 3 12B};

\node[layer, fill=blue!15] (g_global) at (-2.8, 3.6) {8 Global Attn layers};
\node[layer, fill=cyan!15, below=0.15cm of g_global] (g_slide) {40 Sliding Window layers};
\node[spec, below=0.15cm of g_slide] (g_spec) {GQA: 8 KV / 16 Q heads\\K=256, V=256 (symmetric)\\Window = 1024 tokens};

% DeepSeek stack (right)
\node[font=\small\bfseries] (ds_title) at (2.8, 4.5) {DeepSeek-V2-Lite 16B};

\node[layer, fill=blue!15] (d_global) at (2.8, 3.6) {27 Global Attn layers};
\node[layer, fill=yellow!20, below=0.15cm of d_global] (d_moe) {MoE routing (2/6 active)};
\node[spec, below=0.15cm of d_moe] (d_spec) {MLA: 16 KV heads\\K=192, V=128 (asymmetric)\\No sliding window};

% Shared Block Pool (center bottom)
\node[pool] (bp) at (0, 0) {Model-Agnostic\\Block Pool\\(256-token blocks)};

% ModelCacheSpec arrows
\draw[arrow, blue!70!black] (g_spec.south) -- ++(0,-0.3) -| node[pos=0.25, above, font=\tiny] {ModelCacheSpec} (bp.north west);
\draw[arrow, red!70!black] (d_spec.south) -- ++(0,-0.3) -| node[pos=0.25, above, font=\tiny] {ModelCacheSpec} (bp.north east);

% Disk below
\node[cylinder, draw=black, fill=yellow!20, minimum width=1.5cm, minimum height=0.8cm, shape border rotate=90, font=\scriptsize, below=0.6cm of bp] (disk) {safetensors};
\draw[arrow] (bp) -- (disk);

% Key differences annotations
\node[font=\tiny, align=left, text=blue!70!black, anchor=north west] at (-4.5, -1.2) {
    Gemma challenges:\\
    -- 5D mask for GQA broadcast\\
    -- Sliding window in chunked prefill\\
    -- clip\_residual monkeypatch
};

\node[font=\tiny, align=left, text=red!70!black, anchor=north east] at (4.5, -1.2) {
    DeepSeek challenges:\\
    -- Asymmetric K/V dimensions\\
    -- MoE intermediate memory\\
    -- EOS template injection
};

\end{tikzpicture}
\caption{Architecture comparison. The block pool abstracts away architectural differences through ModelCacheSpec. Gemma 3 uses grouped-query attention with hybrid sliding-window layers, requiring 5D mask expansion and window-aware chunked prefill. DeepSeek uses multi-latent attention with asymmetric K/V dimensions (192 vs 128) and MoE routing, requiring larger memory budgets for intermediate tensors.}
\label{fig:archcomp}
\end{figure}
