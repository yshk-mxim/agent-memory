\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nvidia2024a100bench}
\citation{kwon2023pagedattention,zheng2024sglang}
\citation{sarthi2024raptor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Cold-Start Problem and Key Insight}{1}{subsection.1.1}\protected@file@percent }
\citation{apple2024m4pro}
\citation{nvidia2025dgxspark}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Contributions}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{sec:background}{{2}{2}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Re-Prefill Problem}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Unified Memory Architecture Opportunity}{2}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Memory architecture comparison. Discrete GPUs isolate model weights in VRAM, requiring explicit PCIe transfers (32 GB/s bottleneck). Unified Memory Architecture (UMA) provides zero-copy access to a shared DRAM pool. M4 Pro and DGX Spark converge at 273 GB/s bandwidth, though datacenter accelerators still dominate compute throughput.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:uma}{{1}{3}{Memory architecture comparison. Discrete GPUs isolate model weights in VRAM, requiring explicit PCIe transfers (32 GB/s bottleneck). Unified Memory Architecture (UMA) provides zero-copy access to a shared DRAM pool. M4 Pro and DGX Spark converge at 273 GB/s bandwidth, though datacenter accelerators still dominate compute throughput}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Design}{3}{section.3}\protected@file@percent }
\newlabel{sec:design}{{3}{3}{System Design}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Block Pool with Per-Agent Isolation}{3}{subsection.3.1}\protected@file@percent }
\citation{kwon2023pagedattention,zheng2024sglang}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces System architecture. Multiple agents maintain isolated KV caches in a persistent block pool. The Q4 pipeline quantizes cache data on save and operates directly on quantized tensors during attention. Disk persistence enables sub-100ms reload (warm) vs seconds of re-prefill (cold).}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:architecture}{{2}{4}{System architecture. Multiple agents maintain isolated KV caches in a persistent block pool. The Q4 pipeline quantizes cache data on save and operates directly on quantized tensors during attention. Disk persistence enables sub-100ms reload (warm) vs seconds of re-prefill (cold)}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q4 Quantization Pipeline}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Prefix Matching: Character-Level vs Token-Level}{4}{subsection.3.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Character-Level Prefix Matching}}{5}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Batched Quantized Inference}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Cross-Phase Context Injection}{5}{subsection.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces TTFT scaling across cache states (Gemma 3 12B). Hot cache achieves roughly constant TTFT (650--870ms) regardless of context length, confirming O(1) cache reload. Warm (disk reload) provides 10.5$\times $ speedup at 16K. Cold start exhibits O(n) prefill scaling.}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:ttft}{{3}{6}{TTFT scaling across cache states (Gemma 3 12B). Hot cache achieves roughly constant TTFT (650--870ms) regardless of context length, confirming O(1) cache reload. Warm (disk reload) provides 10.5$\times $ speedup at 16K. Cold start exhibits O(n) prefill scaling}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Evaluation}{6}{section.4}\protected@file@percent }
\newlabel{sec:eval}{{4}{6}{Evaluation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Setup}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}TTFT Scaling: Cold, Warm, Hot}{6}{subsection.4.2}\protected@file@percent }
\citation{kwon2023pagedattention}
\citation{zheng2024sglang}
\citation{zhang2024kvswap}
\citation{ye2025kvcomm}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces TTFT (ms) across context lengths and cache states}}{7}{table.1}\protected@file@percent }
\newlabel{tab:ttft}{{1}{7}{TTFT (ms) across context lengths and cache states}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Batched Throughput}{7}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Batched vs sequential serving (Gemma 3 12B, 1K context)}}{7}{table.2}\protected@file@percent }
\newlabel{tab:batch}{{2}{7}{Batched vs sequential serving (Gemma 3 12B, 1K context)}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Staggered Arrivals}{7}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Staggered request arrivals (both users 4K context). User A submits at t=0, User B at t=2s. Sequential serving forces User B to wait for User A's completion (24.5s TTFT). Batched serving provides 2.6$\times $ speedup for User B at minimal cost to User A (4\% penalty). Net total TTFT improves 1.86$\times $.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:staggered}{{4}{8}{Staggered request arrivals (both users 4K context). User A submits at t=0, User B at t=2s. Sequential serving forces User B to wait for User A's completion (24.5s TTFT). Batched serving provides 2.6$\times $ speedup for User B at minimal cost to User A (4\% penalty). Net total TTFT improves 1.86$\times $}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{8}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{8}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Contributions and Comparison with Related Systems}{8}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Novelty comparison with related systems}}{8}{table.3}\protected@file@percent }
\newlabel{tab:novelty}{{3}{8}{Novelty comparison with related systems}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Working Memory vs RAG vs Message Passing}{8}{subsection.5.2}\protected@file@percent }
\citation{liu2024kivi,hooper2024kvquant}
\citation{kwon2023pagedattention}
\citation{zheng2024sglang}
\citation{barrios2026vllmmlx}
\citation{liu2024kivi}
\citation{hooper2024kvquant}
\citation{li2025commvq}
\citation{liu2024cachegen}
\citation{jiang2025emllm}
\citation{yang2025memory3}
\citation{memart2026iclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Limitations}{9}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{9}{section.6}\protected@file@percent }
\newlabel{sec:related}{{6}{9}{Related Work}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}KV Cache Management Systems}{9}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}KV Cache Compression}{9}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Agent Memory}{9}{subsection.6.3}\protected@file@percent }
\citation{ye2025kvcomm}
\citation{pan2025kvflow}
\citation{liu2024droidspeak}
\citation{zhang2024kvswap}
\citation{feng2024evicpress}
\citation{bui2024trimkv}
\citation{bui2024trimkv}
\bibstyle{colm2026_conference}
\bibdata{semantic_colm2026}
\bibcite{memart2026iclr}{{1}{2026}{{Anonymous}}{{}}}
\bibcite{apple2024m4pro}{{2}{2024}{{Apple Inc.}}{{}}}
\bibcite{barrios2026vllmmlx}{{3}{2026}{{Barrios}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Multi-Agent KV Cache Systems}{10}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Edge/On-Device Systems}{10}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{10}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{10}{Conclusion}{section.7}{}}
\bibcite{bui2024trimkv}{{4}{2024}{{Bui et~al.}}{{Bui, Sharma, Lamba, Mishra, and Ying}}}
\bibcite{feng2024evicpress}{{5}{2024}{{Feng et~al.}}{{Feng, Liu, Li, Chen, Shen, Du, Gu, Zhang, Huang, Cheng, Yao, Zhang, Ananthanarayanan, and Jiang}}}
\bibcite{hooper2024kvquant}{{6}{2024}{{Hooper et~al.}}{{Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami}}}
\bibcite{nvidia2024a100bench}{{7}{2024}{{Hyperstack}}{{}}}
\bibcite{jiang2025emllm}{{8}{2025}{{Jiang et~al.}}{{}}}
\bibcite{kwon2023pagedattention}{{9}{2023}{{Kwon et~al.}}{{Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica}}}
\bibcite{li2025commvq}{{10}{2025}{{Li et~al.}}{{Li, Zhang, Zhao, Zheng, Yang, and Ding}}}
\bibcite{liu2024droidspeak}{{11}{2024{}}{{Liu et~al.}}{{Liu, Huang, Yao, Feng, Gu, Du, Li, Cheng, Jiang, Lu, Musuvathi, and Choukse}}}
\bibcite{liu2024cachegen}{{12}{2024{}}{{Liu et~al.}}{{Liu, Li, Cheng, Ray, Huang, Zhang, Du, Yao, Lu, Ananthanarayanan, Maire, Hoffmann, Holtzman, and Jiang}}}
\bibcite{liu2024kivi}{{13}{2024{}}{{Liu et~al.}}{{Liu, Yuan, Jin, Zhong, Xu, Braverman, Chen, and Hu}}}
\bibcite{nvidia2025dgxspark}{{14}{2025}{{NVIDIA Corporation}}{{}}}
\bibcite{pan2025kvflow}{{15}{2025}{{Pan et~al.}}{{Pan, Patel, Hu, Shen, Guan, Li, Qin, Wang, and Ding}}}
\bibcite{sarthi2024raptor}{{16}{2024}{{Sarthi et~al.}}{{Sarthi, Abdullah, Tuli, Khanna, Goldie, and Manning}}}
\bibcite{yang2025memory3}{{17}{2024}{{Yang et~al.}}{{Yang, Lin, et~al.}}}
\bibcite{ye2025kvcomm}{{18}{2025}{{Ye et~al.}}{{Ye, Gao, Ma, Wang, Fu, Chung, Lin, Liu, Zhang, Zhuo, and Chen}}}
\bibcite{zhang2024kvswap}{{19}{2024}{{Zhang et~al.}}{{Zhang, Xia, and Wang}}}
\bibcite{zheng2024sglang}{{20}{2024}{{Zheng et~al.}}{{Zheng, Yin, Xie, Huang, Sun, Yu, Cao, Kozyrakis, Stoica, Gonzalez, Barrett, and Sheng}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}safetensors Q4 Format}{13}{appendix.A}\protected@file@percent }
\newlabel{app:safetensors}{{A}{13}{safetensors Q4 Format}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}MLX Lazy Evaluation Pitfalls}{13}{appendix.B}\protected@file@percent }
\newlabel{app:mlx}{{B}{13}{MLX Lazy Evaluation Pitfalls}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Common MLX lazy evaluation pitfalls}}{13}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Benchmark Configuration}{14}{appendix.C}\protected@file@percent }
\newlabel{app:benchmark}{{C}{14}{Benchmark Configuration}{appendix.C}{}}
\gdef \@abspage@last{14}
