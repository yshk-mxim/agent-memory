\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{liu2024lost}
\citation{kwon2023pagedattention}
\citation{zheng2024sglang}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{liu2024lost,jiang2024minference}
\citation{guo2024multiagent}
\citation{chang2025sagallm}
\citation{promptpeek2025}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Edge device memory and bandwidth. Unified memory devices share RAM between CPU and GPU. Discrete GPUs (RTX) have separate VRAM; KV cache offload to host RAM drops to PCIe bandwidth.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:hardware}{{1}{2}{Edge device memory and bandwidth. Unified memory devices share RAM between CPU and GPU. Discrete GPUs (RTX) have separate VRAM; KV cache offload to host RAM drops to PCIe bandwidth}{table.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Contributions.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{sec:background}{{2}{2}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Multi-Agent Memory Problem}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:memory_problem}{{2.1}{2}{The Multi-Agent Memory Problem}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Edge Device Constraints}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:edge_constraints}{{2.2}{2}{Edge Device Constraints}{subsection.2.2}{}}
\citation{nielsen1993}
\citation{nielsen2024speed}
\citation{fusionragcache2025}
\citation{jin2024ragcache}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces System architecture. Multiple agents maintain isolated KV caches in a persistent block pool. The Q4 pipeline quantizes cache data on save and operates directly on quantized tensors during attention. Disk persistence enables sub-100ms reload (warm) vs seconds of re-prefill (cold).}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:architecture}{{1}{3}{System architecture. Multiple agents maintain isolated KV caches in a persistent block pool. The Q4 pipeline quantizes cache data on save and operates directly on quantized tensors during attention. Disk persistence enables sub-100ms reload (warm) vs seconds of re-prefill (cold)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Interactivity and TTFT}{3}{subsection.2.3}\protected@file@percent }
\newlabel{sec:interactivity}{{2.3}{3}{Interactivity and TTFT}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Design}{3}{section.3}\protected@file@percent }
\newlabel{sec:design}{{3}{3}{System Design}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Block Pool with Per-Agent Isolation}{3}{subsection.3.1}\protected@file@percent }
\citation{kwon2023pagedattention,zheng2024sglang}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Agent capacity on M4~Pro (10.2\,GB cache budget). Gemma~3 12B, 48 layers, 8 KV heads, head dim 256.}}{4}{table.2}\protected@file@percent }
\newlabel{tab:fp16}{{2}{4}{Agent capacity on M4~Pro (10.2\,GB cache budget). Gemma~3 12B, 48 layers, 8 KV heads, head dim 256}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q4 Quantization Pipeline}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:q4_pipeline}{{3.2}{4}{Q4 Quantization Pipeline}{subsection.3.2}{}}
\newlabel{sec:fp16_analysis}{{3.2}{4}{Why Q4, not FP16}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Q4, not FP16.}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Prefix Matching}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Batched Quantized Inference}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concurrency model.}{4}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Cross-Phase Context Injection}{5}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Architectural Coverage}{5}{subsection.3.6}\protected@file@percent }
\newlabel{sec:archcoverage}{{3.6}{5}{Architectural Coverage}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Evaluation}{5}{section.4}\protected@file@percent }
\newlabel{sec:eval}{{4}{5}{Evaluation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}TTFT Scaling}{5}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces TTFT scaling across cache states for both models. Cold prefill scales linearly with context length. Hot and warm caches reduce TTFT by 69--130$\times $ at 32K tokens, with sub-second reload up to 16K context. DeepSeek's smaller layer count (27 vs 48) yields faster cold prefill but both models converge at similar warm/hot latencies relative to their cold baselines.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:ttft}{{2}{6}{TTFT scaling across cache states for both models. Cold prefill scales linearly with context length. Hot and warm caches reduce TTFT by 69--130$\times $ at 32K tokens, with sub-second reload up to 16K context. DeepSeek's smaller layer count (27 vs 48) yields faster cold prefill but both models converge at similar warm/hot latencies relative to their cold baselines}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces TTFT (ms) by cache state. Streaming, batch=1, median of 3 passes.}}{6}{table.3}\protected@file@percent }
\newlabel{tab:ttft}{{3}{6}{TTFT (ms) by cache state. Streaming, batch=1, median of 3 passes}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Batched Throughput}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ablation Analysis}{6}{subsection.4.4}\protected@file@percent }
\newlabel{sec:ablation}{{4.4}{6}{Ablation Analysis}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Single vs concurrent throughput (non-streaming, median of 3 passes). Single: batch=1. Concurrent: batch=2, SysTPS = total tokens/second across both agents.}}{7}{table.4}\protected@file@percent }
\newlabel{tab:batch}{{4}{7}{Single vs concurrent throughput (non-streaming, median of 3 passes). Single: batch=1. Concurrent: batch=2, SysTPS = total tokens/second across both agents}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Component contributions. Each row compares the system with vs without one component, holding others constant.}}{7}{table.5}\protected@file@percent }
\newlabel{tab:ablation}{{5}{7}{Component contributions. Each row compares the system with vs without one component, holding others constant}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multi-Phase Cache Persistence}{7}{subsection.4.5}\protected@file@percent }
\citation{liu2024kivi}
\citation{hooper2024kvquant}
\citation{liu2024kivi,hooper2024kvquant,li2025quantspec}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Measured per-phase average TTFT (ms). Cold: caches cleared each phase. Persistent: caches accumulate. 25 turns total per run.}}{8}{table.6}\protected@file@percent }
\newlabel{tab:phase}{{6}{8}{Measured per-phase average TTFT (ms). Cold: caches cleared each phase. Persistent: caches accumulate. 25 turns total per run}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Wikipedia routing TTFT (ms) by phase. 10 experts, 5 queries, 3 repeated. Articles are 3K words (${\sim }$4K tokens) each.}}{8}{table.7}\protected@file@percent }
\newlabel{tab:wiki}{{7}{8}{Wikipedia routing TTFT (ms) by phase. 10 experts, 5 queries, 3 repeated. Articles are 3K words (${\sim }$4K tokens) each}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Multi-Agent Routing}{8}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Q4 Cache Quality}{8}{subsection.4.7}\protected@file@percent }
\newlabel{sec:perplexity}{{4.7}{8}{Q4 Cache Quality}{subsection.4.7}{}}
\citation{wu2023autogen}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Perplexity with actual Q4 KV caches. FP16 baseline uses standard KV cache; Q4 uses the production \texttt  {QuantizedKVCache} (group size 64). 7{,}935 tokens, 512-token windows, 256-token stride.}}{9}{table.8}\protected@file@percent }
\newlabel{tab:perplexity_main}{{8}{9}{Perplexity with actual Q4 KV caches. FP16 baseline uses standard KV cache; Q4 uses the production \texttt {QuantizedKVCache} (group size 64). 7{,}935 tokens, 512-token windows, 256-token stride}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Context restoration approaches for multi-turn agents.}}{9}{table.9}\protected@file@percent }
\newlabel{tab:approaches}{{9}{9}{Context restoration approaches for multi-turn agents}{table.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{9}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{9}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Infrastructure Layer for Agentic Systems}{9}{subsection.5.1}\protected@file@percent }
\newlabel{sec:framework_layer}{{5.1}{9}{Infrastructure Layer for Agentic Systems}{subsection.5.1}{}}
\citation{kwon2023pagedattention}
\citation{zheng2024sglang}
\citation{barrios2026vllmmlx}
\citation{zhang2024kvswap}
\citation{ye2025kvcomm}
\citation{pan2025kvflow}
\citation{memart2026iclr}
\citation{li2025continuum}
\citation{li2025commvq}
\citation{lmcache2025}
\citation{barrios2026vllmmlx}
\citation{memart2026iclr}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Feature comparison with related systems. Pool: per-agent cache isolation. BQ4: batched Q4 inference. WM: cross-phase KV persistence. Edge: UMA device support. Multi: dense + MoE architectures.}}{10}{table.10}\protected@file@percent }
\newlabel{tab:novelty}{{10}{10}{Feature comparison with related systems. Pool: per-agent cache isolation. BQ4: batched Q4 inference. WM: cross-phase KV persistence. Edge: UMA device support. Multi: dense + MoE architectures}{table.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Persistent Cache vs RAG vs Message Passing}{10}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Comparison with Related Systems}{10}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Portability}{10}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Limitations}{10}{subsection.5.5}\protected@file@percent }
\citation{kwon2023pagedattention}
\citation{zheng2024sglang}
\citation{lmcache2025}
\citation{barrios2026vllmmlx}
\citation{li2025continuum}
\citation{zhong2024distserve}
\citation{agrawal2024sarathi}
\citation{liu2024kivi}
\citation{hooper2024kvquant}
\citation{li2025commvq}
\citation{li2025quantspec}
\citation{jiang2025emllm}
\citation{xu2025amem}
\citation{memart2026iclr}
\citation{ye2025kvcomm}
\citation{pan2025kvflow}
\citation{promptpeek2025}
\citation{zhang2024kvswap}
\citation{kelle2025micro}
\citation{perez2025localllm}
\citation{wen2025krul}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{11}{section.6}\protected@file@percent }
\newlabel{sec:related}{{6}{11}{Related Work}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{11}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{11}{Conclusion}{section.7}{}}
\bibstyle{plainnat}
\bibdata{semantic_colm2026}
\bibcite{agrawal2024sarathi}{{1}{2024}{{Agrawal et~al.}}{{Agrawal, Kedia, Panwar, Mohan, Kwatra, Gulavani, Tumanov, and Ramjee}}}
\bibcite{memart2026iclr}{{2}{2026}{{Anonymous}}{{}}}
\bibcite{barrios2026vllmmlx}{{3}{2026}{{Barrios}}{{}}}
\bibcite{chang2025sagallm}{{4}{2025}{{Chang et~al.}}{{}}}
\bibcite{guo2024multiagent}{{5}{2024}{{Guo et~al.}}{{Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, and Zhang}}}
\bibcite{hooper2024kvquant}{{6}{2024}{{Hooper et~al.}}{{Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami}}}
\bibcite{jiang2024minference}{{7}{2024}{{Jiang et~al.}}{{Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin, Yang, and Qiu}}}
\bibcite{jiang2025emllm}{{8}{2025}{{Jiang et~al.}}{{}}}
\bibcite{jin2024ragcache}{{9}{2024}{{Jin et~al.}}{{Jin, Zhang, Jiang, Fan, Zhu, Luo, and Jin}}}
\bibcite{kwon2023pagedattention}{{10}{2023}{{Kwon et~al.}}{{Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica}}}
\bibcite{li2025continuum}{{11}{2025{}}{{Li et~al.}}{{Li, Mang, He, Zhang, Mao, Chen, Zhou, Cheung, Gonzalez, and Stoica}}}
\bibcite{li2025quantspec}{{12}{2025{}}{{Li et~al.}}{{Li, Tomar, and Gholami}}}
\bibcite{li2025commvq}{{13}{2025{}}{{Li et~al.}}{{Li, Zhang, Zhao, Zheng, Yang, and Ding}}}
\bibcite{fusionragcache2025}{{14}{2025{}}{{Li et~al.}}{{}}}
\bibcite{liu2024lost}{{15}{2024{}}{{Liu et~al.}}{{Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}}}
\bibcite{liu2024kivi}{{16}{2024{}}{{Liu et~al.}}{{Liu, Yuan, Jin, Zhong, Xu, Braverman, Chen, and Hu}}}
\bibcite{lmcache2025}{{17}{2025}{{LMCache Team}}{{}}}
\bibcite{kelle2025micro}{{18}{2025}{{MICRO 2025 Authors}}{{}}}
\bibcite{promptpeek2025}{{19}{2025}{{NDSS 2025 Authors}}{{}}}
\bibcite{nielsen1993}{{20}{1993}{{Nielsen}}{{}}}
\bibcite{nielsen2024speed}{{21}{2024}{{Nielsen}}{{}}}
\bibcite{pan2025kvflow}{{22}{2025}{{Pan et~al.}}{{Pan, Patel, Hu, Shen, Guan, Li, Qin, Wang, and Ding}}}
\bibcite{perez2025localllm}{{23}{2025}{{Perez et~al.}}{{}}}
\bibcite{rotatekv2025}{{24}{2025}{{Tao et~al.}}{{}}}
\bibcite{wen2025krul}{{25}{2025}{{Wen et~al.}}{{}}}
\bibcite{wu2023autogen}{{26}{2023}{{Wu et~al.}}{{Wu, Bansal, Zhang, Wu, Li, Zhu, Jiang, Zhang, Zhang, Liu, Awadallah, White, Burger, and Wang}}}
\bibcite{xu2025amem}{{27}{2025}{{Xu et~al.}}{{Xu, Zhang, et~al.}}}
\bibcite{ye2025kvcomm}{{28}{2025}{{Ye et~al.}}{{Ye, Gao, Ma, Wang, Fu, Chung, Lin, Liu, Zhang, Zhuo, and Chen}}}
\bibcite{zhang2024kvswap}{{29}{2024}{{Zhang et~al.}}{{Zhang, Xia, and Wang}}}
\bibcite{zheng2024sglang}{{30}{2024}{{Zheng et~al.}}{{Zheng, Yin, Xie, Huang, Sun, Yu, Cao, Kozyrakis, Stoica, Gonzalez, Barrett, and Sheng}}}
\bibcite{zhong2024distserve}{{31}{2024}{{Zhong et~al.}}{{Zhong, Liu, Chen, Hu, Zhu, Liu, Jin, and Zhang}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}safetensors Q4 Format}{15}{appendix.A}\protected@file@percent }
\newlabel{app:safetensors}{{A}{15}{safetensors Q4 Format}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}MLX Engineering Notes}{15}{appendix.B}\protected@file@percent }
\newlabel{app:mlx}{{B}{15}{MLX Engineering Notes}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces MLX lazy evaluation failure modes relevant to KV cache persistence.}}{15}{table.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Benchmark Configuration}{15}{appendix.C}\protected@file@percent }
\newlabel{app:benchmark}{{C}{15}{Benchmark Configuration}{appendix.C}{}}
\citation{liu2024kivi}
\citation{hooper2024kvquant}
\citation{li2025quantspec}
\citation{rotatekv2025}
\@writefile{toc}{\contentsline {section}{\numberline {D}FP16 vs Q4 Memory Analysis}{16}{appendix.D}\protected@file@percent }
\newlabel{app:fp16_analysis}{{D}{16}{FP16 vs Q4 Memory Analysis}{appendix.D}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Agent capacity comparison, both models. M4~Pro, 10.2\,GB cache budget.}}{16}{table.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Perplexity Methodology}{16}{appendix.E}\protected@file@percent }
\newlabel{app:perplexity}{{E}{16}{Perplexity Methodology}{appendix.E}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Staggered request arrivals (4K cold context, Agent B arrives 2s after Agent A). Total wall time is similar between sequential and batched modes for both models. Agent B benefits from batched scheduling because it begins prefill immediately rather than waiting for Agent A's decode phase to complete. The effect is larger on DeepSeek due to shorter prefill relative to decode.}}{17}{figure.3}\protected@file@percent }
\newlabel{fig:staggered}{{3}{17}{Staggered request arrivals (4K cold context, Agent B arrives 2s after Agent A). Total wall time is similar between sequential and batched modes for both models. Agent B benefits from batched scheduling because it begins prefill immediately rather than waiting for Agent A's decode phase to complete. The effect is larger on DeepSeek due to shorter prefill relative to decode}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Staggered Arrivals}{17}{appendix.F}\protected@file@percent }
\newlabel{app:staggered}{{F}{17}{Staggered Arrivals}{appendix.F}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Hardware Landscape}{17}{appendix.G}\protected@file@percent }
\newlabel{app:hardware}{{G}{17}{Hardware Landscape}{appendix.G}{}}
\@writefile{toc}{\contentsline {section}{\numberline {H}Detailed Figures}{17}{appendix.H}\protected@file@percent }
\newlabel{app:figures}{{H}{17}{Detailed Figures}{appendix.H}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}Architectural Comparison}{17}{subsection.H.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Phase Timeline}{17}{subsection.H.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architecture comparison. The block pool abstracts away architectural differences through ModelCacheSpec. Gemma 3 uses grouped-query attention with hybrid sliding-window layers, requiring 5D mask expansion and window-aware chunked prefill. DeepSeek uses multi-latent attention with asymmetric K/V dimensions (192 vs 128) and MoE routing, requiring larger memory budgets for intermediate tensors.}}{18}{figure.4}\protected@file@percent }
\newlabel{fig:archcomp}{{4}{18}{Architecture comparison. The block pool abstracts away architectural differences through ModelCacheSpec. Gemma 3 uses grouped-query attention with hybrid sliding-window layers, requiring 5D mask expansion and window-aware chunked prefill. DeepSeek uses multi-latent attention with asymmetric K/V dimensions (192 vs 128) and MoE routing, requiring larger memory budgets for intermediate tensors}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.3}Wikipedia Routing Diagram}{18}{subsection.H.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Agent cache state across prisoner's dilemma phases. Permanent agents (Warden, Marco, Danny) start cold and transition to warm/hot as context accumulates via cross-phase injection. Each phase extends the cached prefix rather than re-computing. The Analyst appears only in Phase~5 (cold start). TTFT annotations show projected latency from Table~\ref {tab:ttft} at equivalent context lengths.}}{19}{figure.5}\protected@file@percent }
\newlabel{fig:timeline}{{5}{19}{Agent cache state across prisoner's dilemma phases. Permanent agents (Warden, Marco, Danny) start cold and transition to warm/hot as context accumulates via cross-phase injection. Each phase extends the cached prefix rather than re-computing. The Analyst appears only in Phase~5 (cold start). TTFT annotations show projected latency from Table~\ref {tab:ttft} at equivalent context lengths}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Wikipedia multi-agent routing. Ten expert agents are primed with article content (cold prefill). Cross-topic queries route to 2--3 relevant experts whose caches are warm/hot from priming. A reporter agent synthesizes responses. Repeated queries to the same experts benefit from hot cache (projected 10--30$\times $ TTFT reduction vs cold).}}{19}{figure.6}\protected@file@percent }
\newlabel{fig:wikirouting}{{6}{19}{Wikipedia multi-agent routing. Ten expert agents are primed with article content (cold prefill). Cross-topic queries route to 2--3 relevant experts whose caches are warm/hot from priming. A reporter agent synthesizes responses. Repeated queries to the same experts benefit from hot cache (projected 10--30$\times $ TTFT reduction vs cold)}{figure.6}{}}
\gdef \@abspage@last{19}
