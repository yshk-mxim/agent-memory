<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs</title>
<!--Generated on Wed Dec  3 00:27:24 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2512.03324v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S1" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S2" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.SS1" title="In 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Transformers with Self-Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.SS2" title="In 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Revisiting KV Cache Eviction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.SS1" title="In 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Selective In-Context Memory via Retention-Gated Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.SS2" title="In 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.SS3" title="In 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1" title="In 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Long Generation Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1.SSS1" title="In 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Quantitative Result</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1.SSS2" title="In 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Qualitative Result</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS2" title="In 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Long-Context Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS3" title="In 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S6" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A1" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A1.SS1" title="In Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Inference Algorithm.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A1.SS2" title="In Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Complexity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.SS1" title="In Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Long Generation Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.SS2" title="In Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Long-Context Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.SS3" title="In Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Chunked-Prefill Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.SS4" title="In Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Additional Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3" title="In Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Qualitative Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="g1" src="figs/trimkv_icon.png" width="23"/>Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ngoc Bui<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#003A70;">Y</span></sup>Â , Shubham Sharma<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#996C48;">J</span></sup>, Simran Lamba<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#996C48;">J</span></sup>, Saumitra Mishra<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#996C48;">J</span></sup>, Rex Ying<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#003A70;">Y</span></sup>
<br class="ltx_break"/><sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#003A70;">Y</span></sup>Department of Computer Science, Yale University,
<sup class="ltx_sup"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#996C48;">J</span></sup>JPMorganChase AI Research
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">{ngoc.bui,rex.ying}@yale.edu</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">{shubham.x2.sharma,simran.lamba,saumitra.mishra}@jpmchase.com</span>
<br class="ltx_break"/>Â <span class="ltx_text ltx_font_bold">Source Code:</span> <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/ngocbh/trimkv" title="">https://github.com/ngocbh/trimkv</a>
</span><span class="ltx_author_notes">Work done during an internship at JPMorganChase AI Research</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each tokenâ€™s intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBenchV2 and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Modern large language models (LLMs) can, in principle, handle extremely long input contexts â€“ some recent models support context windows of 128k tokens or moreÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2025qwen3</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2024train</span>)</cite>. Yet, extending context length comes with steep computational costs. The self-attention mechanism has quadratic time complexity in sequence length, and storing the key-value (KV) cache for thousands of tokens can quickly exhaust GPU memoryÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025llms</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024survey</span>)</cite>. In practical deployments, the KV cache, which saves past key and value vectors to avoid re-computation, becomes a major memory and latency bottleneck for long-context inference. Decoupling resource usage from context length is therefore critical for enabling efficient and scalable applications such as long-horizon reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025towards</span>)</cite> and lifelong agentsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2025lifelong</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024larm</span>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">To address this challenge, recent work has explored memory-bounded LLMs that can operate effectively under constrained KV budgetsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024survey</span>)</cite>. One line of research focuses on compression and quantization, aiming to reduce memory footprint by learning compact representations of past tokens rather than storing all keys and values explicitlyÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">hooper2024kvquant</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">saxena2024eigen</span>)</cite>. These techniques are mostly effective during the prefill phase but scale poorly with generation length. Another line leverages attention sparsity to offload most of the cache to CPU or secondary storage, and retrieve only relevant segments on demand via similarity searchÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">tang2024quest</span>)</cite> or learned indicesÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>. While offloading lowers the on-GPU footprint, it incurs nontrivial orchestration overhead that accumulates over long generations, undermining end-to-end throughput.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">A more common and direct approach to enforce a fixed memory budget is KV cache eviction, which directly drops certain tokens from the KV cacheÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2023efficient</span>)</cite>. Many KV eviction strategies have been proposed to decide which tokens to remove. However, most of them are attention-guided heuristics: they track attention from new queries to cached tokens and retain those that are recently or frequently attended, adapting the cache to the current focusÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023h2o</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024snapkv</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025llms</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2025can</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ghadia2025dialogue</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>)</cite>. While being efficient, these methods assume that recent attention is a reliable proxy for future importance. This assumption often breaks for long-horizon generation and reasoning tasks: a token might be crucial much later, even if it has not been attended to in the recent pastÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">jiang2024minference</span>)</cite>. Moreover, attention-based eviction can suffer from attention bias, <span class="ltx_text ltx_font_italic">e.g.,</span> the model might temporarily overlook a needed token due to a distracting contextÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">shi2023large</span>)</cite>, causing it to be evicted prematurely. While some recent studies have attempted to learn better eviction decisionsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2024nacl</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zeng2024context</span>)</cite>, these methods typically scale poorly with sequence length and are therefore limited to the prefilling stage.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">In this work</span>, we take a new perspective on the KV eviction problem. Rather than relying on the attention-guided importance, we propose to learn each tokenâ€™s intrinsic importance at the time of its creation and use that as the basis for eviction. Intuitively, not all tokens are created equal: some carry significant semantic or task-related weight (<span class="ltx_text ltx_font_italic">e.g.</span> a critical fact, a question being answered, or the first few â€œsink" tokens that often encode the topic or instructions), while others are relatively inconsequential (<span class="ltx_text ltx_font_italic">e.g.</span> filler words, stopwords, or trivial arithmetic steps in a chain-of-thought). Moreover, the importance of tokens is not uniform across the network, but it varies systematically by layers and heads, reflecting their functional specializationsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">voita2019analyzing</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wu2024retrieval</span>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p">We posit that the contextual embedding of a token already encodes much of its long-term utility. We therefore introduce a retention gate that maps the tokenâ€™s embedding and produces a scalar retention score <math alttext="\beta\in[0,1]" class="ltx_Math" display="inline" id="S1.p5.m1" intent=":literal"><semantics><mrow><mi>Î²</mi><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\beta\in[0,1]</annotation></semantics></math> reflecting the tokenâ€™s inherent importance for a specific layer and head. Especially, we design this retention score to decay exponentially as the context grows, mimicking the gradual forgetting of old information in human brainsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ebbinghaus2013image</span>)</cite>. Thus, a highly important token will have <math alttext="\beta\approx 1" class="ltx_Math" display="inline" id="S1.p5.m2" intent=":literal"><semantics><mrow><mi>Î²</mi><mo>â‰ˆ</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\beta\approx 1</annotation></semantics></math> and retain a high score for a long time, whereas a token deemed unimportant will have <math alttext="\beta" class="ltx_Math" display="inline" id="S1.p5.m3" intent=":literal"><semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> closer to 0 and its influence will vanish quickly. We leverage this score to drive a simple eviction policy: whenever the number of cached tokens exceeds the budget <math alttext="M" class="ltx_Math" display="inline" id="S1.p5.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>, we evict the token with the smallest current retention score. This approach, which we call <span class="ltx_text ltx_font_bold">T</span>oken <span class="ltx_text ltx_font_bold">R</span>etent<span class="ltx_text ltx_font_bold">I</span>on for <span class="ltx_text ltx_font_bold">M</span>emory-bounded <span class="ltx_text ltx_font_bold">KV</span> CacheÂ (<span class="ltx_text ltx_font_smallcaps">TRIM-KV</span>), ensures that at all times, the cache is filled with the <math alttext="M" class="ltx_Math" display="inline" id="S1.p5.m5" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> tokens judged most intrinsically important, with a preference toward more recently generated tokens.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p">Implementing retention-based caching in an existing LLM only requires adding a few lightweight components. We integrate the retention gates into each self-attention layer of a pretrained model to modulate attention weights by token importance during training. We then train only the gates with a two-part loss: a distillation loss that compels the modified model to mimic the original modelâ€™s outputs, thus preserving quality, and a capacity loss that penalizes exceeding the target memory budget, thus encouraging sparseness in attention via eviction. Importantly, by training the gates across all layers jointly, the model can learn a coordinated, globally optimal caching policy rather than greedy layer-wise decisions. At inference time, the learned retention gates produce per-token scores on the fly, and eviction is implemented with a simple score comparison, adding minimal overhead.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results and Contributions.</span> Through extensive experiments on long-context and long-generation benchmarks, we demonstrate that our learnable token retention approach substantially improves the performance of memory-bounded LLMs. On challenging mathematical reasoning datasets, GSM8K, MATH, AIME, a long procedural generation benchmark, LongProc, and a long-memory chat assistant benchmark, LongMemEval, our method consistently outperforms eviction baselines, even when those baselines use <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S1.p7.m1" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics></math> more KV budget, and deliver 58.4% pass@1 gain compared to the SOTA learnable KV retrieval baselineÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>, especially in low-memory regimes. Remarkably, in several settings, TRIM-KV even surpasses a full-cache model, suggesting that selective retention can function as an effective regularizer by suppressing noise from uninformative tokens.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p">We also present qualitative evidence that learned retention scores align with human intuition: the model tends to assign high scores to initial tokens and problem descriptions, and low scores to less meaningful punctuation. Notably, many behaviors reminiscent of common heuristics, such as keeping sink tokens, sliding windows, and gist compressionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mu2023learning</span>)</cite>, emerge naturally and adaptively from our learned policy, without being hard-coded. Finally, we show that these learned retention scores can also act as a diagnostic tool for probing layer- and head-specific dynamics, providing a lightweight means to analyze and ultimately improve the interpretability of attention patterns.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">KV Cache Compression.</span> As model sizes and context windows grow, optimizing KV-cache memory is increasingly critical. Prior work largely falls into three directions: (i) token eviction/mergingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2023efficient</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024snapkv</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023h2o</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">nawrot2024dynamic</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2024cam</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">qin2025cake</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025llms</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2025can</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">park2025keydiff</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">park2025keydiff</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">kim2024infinipot</span>)</cite>, (ii) vector compression/quantizationÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">hooper2024kvquant</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2024kivi</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">yue2024wkvquant</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sun2024shadowkv</span>)</cite>, and (iii) token retrievalÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">tang2024quest</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2024retrievalattention</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>. While effective in many settings, vector compression and retrieval either discard fine-grained information or introduce nontrivial systems overhead (e.g., coordination and data movement)Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024survey</span>)</cite>. Moreover, their memory and computation still scale with sequence length, making them inefficient for long-horizon generation applications. Token eviction offers a simple, memory-bounded alternative; however, most existing policies are heuristic and can significantly degrade performance, especially on long reasoning trajectories. Recent work has introduced learnable eviction policiesÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2024nacl</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zeng2024context</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2024locret</span>)</cite>, but these are primarily designed for the pre-filling stage and thus are not well suited to sustained long-horizon generation. We bridge this gap by introducing a learnable and efficient eviction policy designed for long-horizon LLM inference under fixed memory budgets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Forgetting in Language Models.</span> A key limitation of vanilla self-attention is the lack of an explicit forgetting mechanism, forcing the model to carry potentially irrelevant information and making long-context processing inefficient. Early work tackled this by replacing quadratic attention with linearized and recurrent variantsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">katharopoulos2020transformers</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2020linformer</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sun2023retentive</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2023gated</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2024parallelizing</span>)</cite> that summarize the past into a fixed-size state, often a single vector. While computationally attractive, such heavy compression can degrade performance on tasks requiring long-range memory. <span class="ltx_text" style="--ltx-fg-color:#000000;">Follow-up studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">behrouz2024titans</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sun2024learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">karamitrellis</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">karami2025lattice</span>)</cite> increase memory capacity by replacing this hidden vector with a more expressive neural state. Complementary lines of work retain softmax attention but enforce forgetting by modifying attention logitsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">lin2025forgetting</span>)</cite> or imposing trainable sparsity patternsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yuan2025native</span>)</cite>. However, these approaches typically alter attention dynamics substantially and thus require training models from scratch. This incurs significant training cost and leaves their scalability to contemporary LLM sizes uncertain. In contrast, we introduce a <span class="ltx_text ltx_font_italic">plug-in</span> forgetting mechanism for pretrained LLMs that converts them into memory-bounded models, providing long-context efficiency without retraining from scratch.</span></p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transformers with Self-Attention</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">Given a sequence of <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-dimensional input vectors <math alttext="\mathbf{x}_{1},\dots,\mathbf{x}_{T}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mrow><msub><mi>ğ±</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><msub><mi>ğ±</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_{1},\dots,\mathbf{x}_{T}</annotation></semantics></math>, a (causal) self-attention layer attends only to past positions. For each <math alttext="t=1,\ldots,T" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\ldots,T</annotation></semantics></math>, the attention output <math alttext="\mathbf{o}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><msub><mi>ğ¨</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{o}_{t}</annotation></semantics></math> is computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{q}_{t}=\mathbf{W}_{Q}\mathbf{x}_{t},\mathbf{k}_{t}=\mathbf{W}_{K}\mathbf{x}_{t},\mathbf{v}_{t}=\mathbf{W}_{V}\mathbf{x}_{t},\quad\mathbf{o}_{t}=\sum_{i=1}^{t}\frac{\exp\left(\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left(\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i}," class="ltx_Math" display="block" id="S3.Ex1.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ğª</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>Q</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>ğ¤</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>K</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>ğ¯</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>V</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>ğ¨</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¯</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{q}_{t}=\mathbf{W}_{Q}\mathbf{x}_{t},\mathbf{k}_{t}=\mathbf{W}_{K}\mathbf{x}_{t},\mathbf{v}_{t}=\mathbf{W}_{V}\mathbf{x}_{t},\quad\mathbf{o}_{t}=\sum_{i=1}^{t}\frac{\exp\left(\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left(\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{q},\mathbf{k},\mathbf{v}" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mrow><mi>ğª</mi><mo>,</mo><mi>ğ¤</mi><mo>,</mo><mi>ğ¯</mi></mrow><annotation encoding="application/x-tex">\mathbf{q},\mathbf{k},\mathbf{v}</annotation></semantics></math> are query, key, and value states, respectively, and <math alttext="\mathbf{W}_{Q},\mathbf{W}_{K},\mathbf{W}_{V}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>ğ–</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>ğ–</mi><mi>K</mi></msub><mo>,</mo><msub><mi>ğ–</mi><mi>V</mi></msub></mrow><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">Ã—</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}_{Q},\mathbf{W}_{K},\mathbf{W}_{V}\in\mathbb{R}^{d\times d}</annotation></semantics></math> are linear transformation weights. Here, we assume a single-head attention layer and omit the scaling factor <math alttext="1/\sqrt{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">1/\sqrt{d}</annotation></semantics></math> for simplicity. The sequence of key-value pairs <math alttext="\{(\mathbf{k}_{i},\mathbf{v}_{i})\}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><msub><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ¤</mi><mi>i</mi></msub><mo>,</mo><msub><mi>ğ¯</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">\{(\mathbf{k}_{i},\mathbf{v}_{i})\}_{i}</annotation></semantics></math> is the in-context memory of the LLM. During the autoregressive decoding, we typically generate one token at a time and cache the running key-value pair <math alttext="(\mathbf{k}_{t},\mathbf{v}_{t})" class="ltx_Math" display="inline" id="S3.SS1.p1.m9" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>ğ¤</mi><mi>t</mi></msub><mo>,</mo><msub><mi>ğ¯</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{k}_{t},\mathbf{v}_{t})</annotation></semantics></math> to our in-context memory to avoid recomputation. However, this vanilla caching approach leads to a linear increase in memory footprint with the sequence length, while computation grows quadraticallyÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">keles2023computational</span>)</cite>. This reduces efficiency when handling long-context inputs and extended generation tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Revisiting KV Cache Eviction</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">A common method to address the linear growth in the memory is to prune or compress the running key-value pairs into fixed-size (slot) memory. As new tokens arrive, we evict <span class="ltx_text ltx_font_italic">un-(or less-)important</span> tokens from our memory and append the new ones. To understand this procedure, we revisit and rewrite the attention computation with eviction at inference step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{o}_{t}^{\prime}=\sum_{i=1}^{t}\frac{\exp\left({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\alpha_{ti}}\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\alpha_{tj}}\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i}\quad\text{where}\quad~\alpha_{ti}\in\{0,1\}~\text{and}~\alpha_{ti}\geq\alpha_{t+1,i},~~\forall i,t.\vskip-11.38109pt" class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><msubsup><mi>ğ¨</mi><mi>t</mi><mo>â€²</mo></msubsup><mo rspace="0.111em">=</mo><mrow><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msub><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">Î±</mi><mrow><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">i</mi></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msub><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">Î±</mi><mrow><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">j</mi></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¯</mi><mi>i</mi></msub></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mtext>where</mtext></mrow></mrow><mspace style="width:1.33em;" width="1.33em"></mspace><mrow><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>âˆˆ</mo><mrow><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><mo lspace="0.330em" rspace="0em">â€‹</mo><mtext>and</mtext><mo lspace="0.330em" rspace="0em">â€‹</mo><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow><mo>â‰¥</mo><msub><mi>Î±</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow><mo rspace="0.827em">,</mo><mrow><mrow><mo rspace="0.167em">âˆ€</mo><mi>i</mi></mrow><mo>,</mo><mi>t</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{o}_{t}^{\prime}=\sum_{i=1}^{t}\frac{\exp\left({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\alpha_{ti}}\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\alpha_{tj}}\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i}\quad\text{where}\quad~\alpha_{ti}\in\{0,1\}~\text{and}~\alpha_{ti}\geq\alpha_{t+1,i},~~\forall i,t.\vskip-11.38109pt</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="604" id="S3.F1.g1" src="figs/eviction_example.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Attention w/ eviction (<math alttext="M=3" class="ltx_Math" display="inline" id="S3.F1.m2" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">M=3</annotation></semantics></math>).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p">In EquationÂ (<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.E1" title="In 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>), we introduce a binary variable <math alttext="\alpha_{ti}\in{0,1}" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>âˆˆ</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\alpha_{ti}\in{0,1}</annotation></semantics></math> indicating whether keyâ€“value pair <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> has been evicted at time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and the monotonicity constraint <math alttext="\alpha_{ti}\geq\alpha_{t+1,i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m4" intent=":literal"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>â‰¥</mo><msub><mi>Î±</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ti}\geq\alpha_{t+1,i}</annotation></semantics></math> ensures that we cannot retrieve a token once it is evicted (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.F1" title="Figure 1 â€£ 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>). The goal is to choose a decision variable <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.p2.m5" intent=":literal"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> so that the attention output deviates as little as possible from the full KV cache (all <math alttext="\alpha_{ti}=1,\ \forall i,t" class="ltx_Math" display="inline" id="S3.SS2.p2.m6" intent=":literal"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo rspace="0.667em">,</mo><mrow><mo rspace="0.167em">âˆ€</mo><mi>i</mi></mrow><mo>,</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">\alpha_{ti}=1,\ \forall i,t</annotation></semantics></math>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vskip-2.84526pt\min_{\alpha}~~\mathcal{L}_{\mathrm{base}}(\mathbf{o}_{t}^{\prime};\mathbf{o}_{t})\quad\operatorname{s.t.}\sum_{i=1}^{t}\alpha_{ti}\leq M.\vskip-2.84526pt" class="ltx_math_unparsed" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>Î±</mi></munder><mo lspace="0.827em">â¡</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>base</mi></msub></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>ğ¨</mi><mi>t</mi><mo>â€²</mo></msubsup><mo>;</mo><msub><mi>ğ¨</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><mrow><mi mathvariant="normal">s</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">t</mi><mo lspace="0em">.</mo></mrow><mo lspace="0.167em" rspace="0em">â€‹</mo><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><mo>â‰¤</mo><mi>M</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\vskip-2.84526pt\min_{\alpha}~~\mathcal{L}_{\mathrm{base}}(\mathbf{o}_{t}^{\prime};\mathbf{o}_{t})\quad\operatorname{s.t.}\sum_{i=1}^{t}\alpha_{ti}\leq M.\vskip-2.84526pt</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S3.SS2.p2.m7" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">â„’</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> penalizes differences between attention with and without eviction, and the constraint enforces keeping at most <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p2.m8" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> tokens at any inference step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.m9" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p">Solving the above constrained optimization at every time step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p3.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is impractical due to its combinatorial nature and efficiency requirements of LLM inference in real-world applications. Most existing approaches Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2023efficient</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">han2023lm</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023h2o</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024snapkv</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ghadia2025dialogue</span>)</cite> opt to determine <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.p3.m2" intent=":literal"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> heuristically while we focus on a learnable eviction method.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">In this section, we propose a learning-based eviction policy that prunes the KV cache based on the <span class="ltx_text ltx_font_italic">intrinsic importance</span> of the tokens at each layer and head. The policy ranks tokens by relative importance to decide which should be evicted from the KV memory. To learn token importance, we introduce a small neural network that takes token embeddings as input and produces a scalar retention score. We then integrate this retention score into the attention computation to modulate the attention weights. We term this proxy attention mechanism a <span class="ltx_text ltx_font_italic">retention-gated attention</span>. We train the LLM with retention-gated attention against a baseline model with standard attention, using a combination of distillation and hinge-like regularization losses to enforce memory capacity constraints while preserving response quality.
A visualization is shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.F2" title="Figure 2 â€£ 4.2 Training â€£ 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Selective In-Context Memory via Retention-Gated Attention</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p">We introduce retention-gated attention, a trainable mechanism that mimics the information loss induced by inference-time eviction. From the formulationÂ (<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.E1" title="In 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>), the sequence <math alttext="\alpha_{ii},\alpha_{(i+1)i},\ldots,\alpha_{ti}" class="ltx_Math" display="inline" id="S4.SS1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>Î±</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ii},\alpha_{(i+1)i},\ldots,\alpha_{ti}</annotation></semantics></math> represents how token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p1.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> is retained in the attention computation over time. Retention begins at <math alttext="1" class="ltx_Math" display="inline" id="S4.SS1.p1.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> and then abruptly drops to <math alttext="0" class="ltx_Math" display="inline" id="S4.SS1.p1.m4" intent=":literal"><mn>0</mn></math> once the token is evicted. While this binary behavior matches the inference stage, it poses challenges for learning: the signal is discrete, non-differentiable, thus providing no gradients for optimization. To remedy this, we replace the hard binary variable <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.p1.m5" intent=":literal"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> with a smooth, monotonically decreasing function that models the gradual decay of importance while enabling gradient-based training. A natural candidate is the sigmoid function, <math alttext="\mathrm{bar}{\alpha}_{ti}=1/(1+\exp(f(\mathbf{x}_{i},t)))" class="ltx_Math" display="inline" id="S4.SS1.p1.m6" intent=":literal"><semantics><mrow><mrow><mi>bar</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ±</mi><mi>i</mi></msub><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{bar}{\alpha}_{ti}=1/(1+\exp(f(\mathbf{x}_{i},t)))</annotation></semantics></math>, which predicts the time at which the token is evicted. However, this design suffers from two drawbacks: (i) the domain of <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.p1.m7" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is unnormalized since the sequence length is unknown during decoding, and (ii) the sigmoid flattens across most of its range, producing negligible variation between steps and leading to vanishing gradients during training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p">To overcome these limitations, we adopt an exponential decay formulation, <math alttext="\mathrm{bar}\alpha_{ti}=\beta_{i}^{t-i}" class="ltx_Math" display="inline" id="S4.SS1.p2.m1" intent=":literal"><semantics><mrow><mrow><mi>bar</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow><mo>=</mo><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathrm{bar}\alpha_{ti}=\beta_{i}^{t-i}</annotation></semantics></math> where <math alttext="\beta_{i}\in[0,1]" class="ltx_Math" display="inline" id="S4.SS1.p2.m2" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mi>i</mi></msub><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\beta_{i}\in[0,1]</annotation></semantics></math>, to model the retention rate of token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p2.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> over time. Larger values of <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="S4.SS1.p2.m4" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math> correspond to higher intrinsic importance, implying slower decay and stronger memory retention. Substituting this design for <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.p2.m5" intent=":literal"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> in EquationÂ (<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.E1" title="In 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>) yields our proposed <span class="ltx_text ltx_font_italic">retention-gated attention</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{q}_{t}=\mathbf{W}_{Q}\mathbf{x}_{t},\mathbf{k}_{t}=\mathbf{W}_{K}\mathbf{x}_{t},\mathbf{v}_{t}=\mathbf{W}_{V}\mathbf{x}_{t},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{t}=g(\mathbf{x}_{t})},~~\mathbf{o}_{t}=\sum_{i=1}^{t}\frac{\exp\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{i}^{t-i}}\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{j}^{t-j}}\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i}." class="ltx_Math" display="block" id="S4.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ğª</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>Q</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>ğ¤</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>K</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>ğ¯</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ğ–</mi><mi>V</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">=</mo><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">(</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ±</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">)</mo></mrow></mrow></mrow><mo rspace="0.827em">,</mo><mrow><msub><mi>ğ¨</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msubsup><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">i</mi><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ’</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo>(</mo><mrow><msubsup><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ’</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi></mrow></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>ğª</mi><mi>t</mi><mo>âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¤</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ¯</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{q}_{t}=\mathbf{W}_{Q}\mathbf{x}_{t},\mathbf{k}_{t}=\mathbf{W}_{K}\mathbf{x}_{t},\mathbf{v}_{t}=\mathbf{W}_{V}\mathbf{x}_{t},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{t}=g(\mathbf{x}_{t})},~~\mathbf{o}_{t}=\sum_{i=1}^{t}\frac{\exp\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{i}^{t-i}}\mathbf{q}_{t}^{\top}\mathbf{k}_{i}\right)}{\sum_{j=1}^{t}\exp\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\beta_{j}^{t-j}}\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\right)}\mathbf{v}_{i}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, we propose a <span class="ltx_text ltx_font_italic">retention gate</span> <math alttext="g" class="ltx_Math" display="inline" id="S4.SS1.p2.m6" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>, which is a lightweight network, to parametrize the token importance <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S4.SS1.p2.m7" intent=":literal"><semantics><msub><mi>Î²</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math>. The retention gate can be a linear projection, <span class="ltx_text ltx_font_italic">i.e.</span>, <math alttext="g(\mathbf{x})=\sigma(\mathbf{W}_{\beta}\mathbf{x}_{t}+b),\mathbf{W}_{\beta}\in\mathbb{R}^{1\times d}" class="ltx_Math" display="inline" id="S4.SS1.p2.m8" intent=":literal"><semantics><mrow><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğ±</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>Ïƒ</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>ğ–</mi><mi>Î²</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><msub><mi>ğ–</mi><mi>Î²</mi></msub><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">Ã—</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">g(\mathbf{x})=\sigma(\mathbf{W}_{\beta}\mathbf{x}_{t}+b),\mathbf{W}_{\beta}\in\mathbb{R}^{1\times d}</annotation></semantics></math>, or a simple MLP, <span class="ltx_text ltx_font_italic">i.e.</span>, <math alttext="g(\mathbf{x})=\sigma(\mathrm{MLP}(\mathbf{x})+b)" class="ltx_Math" display="inline" id="S4.SS1.p2.m9" intent=":literal"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğ±</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>Ïƒ</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>MLP</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğ±</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">g(\mathbf{x})=\sigma(\mathrm{MLP}(\mathbf{x})+b)</annotation></semantics></math>. The sigmoid function <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS1.p2.m10" intent=":literal"><semantics><mi>Ïƒ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> squashes the output of <math alttext="g" class="ltx_Math" display="inline" id="S4.SS1.p2.m11" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> to the range <math alttext="[0,1]" class="ltx_Math" display="inline" id="S4.SS1.p2.m12" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math>, while <math alttext="b" class="ltx_Math" display="inline" id="S4.SS1.p2.m13" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> is a learnable bias. When all <math alttext="\beta_{t}=1,\forall t" class="ltx_Math" display="inline" id="S4.SS1.p2.m14" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mi>t</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mrow><mo rspace="0.167em">âˆ€</mo><mi>t</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\beta_{t}=1,\forall t</annotation></semantics></math>, our retention-gated attention recovers the vanilla attention. Our ablation studies show that an MLP with a single hidden layer provides a more powerful retention estimation than a linear projection.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">Token Retention vs. Attention Scores.<span class="ltx_text ltx_font_medium">
In standard self-attention, the importance of a past token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p3.m1" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> at decoding step <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p3.m2" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is given by
<math alttext="a_{ti}\propto\exp(\mathbf{q}_{t}^{\top}\mathbf{k}_{i})," class="ltx_Math" display="inline" id="S4.SS1.p3.m3" intent=":literal"><semantics><mrow><mrow><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">a</mi><mrow><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi></mrow></msub><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">âˆ</mo><mrow><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">exp</mi><mo>â¡</mo><mrow><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">(</mo><mrow><msubsup><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğª</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğ¤</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi></msub></mrow><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">)</mo></mrow></mrow></mrow><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">,</mo></mrow><annotation encoding="application/x-tex">a_{ti}\propto\exp(\mathbf{q}_{t}^{\top}\mathbf{k}_{i}),</annotation></semantics></math>
which depends explicitly on the <em class="ltx_emph ltx_font_italic">current</em> query <math alttext="\mathbf{q}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p3.m4" intent=":literal"><semantics><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğª</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi></msub><annotation encoding="application/x-tex">\mathbf{q}_{t}</annotation></semantics></math>.
These scores capture <em class="ltx_emph ltx_font_italic">short-term</em> utility for predicting the next token and are recomputed at every step, making them local, myopic, and highly dependent on the transient decoding state.</span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">KV cache eviction, in contrast, is a <em class="ltx_emph ltx_font_italic">long-horizon</em> decision: once a token is dropped, it cannot influence <em class="ltx_emph ltx_font_italic">any</em> future prediction.
An effective eviction policy should depend on a tokenâ€™s <em class="ltx_emph ltx_font_italic">intrinsic long-term utility</em> that reflects how useful it is expected to be over the remainder of the sequence and how long it has already stayed in the cache, rather than on the current query alone.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">Token retention provides a more suitable abstraction.
Instead of asking â€œhow much should token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p5.m1" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> contribute <em class="ltx_emph ltx_font_italic">now</em>?â€ it asks â€œhow important is token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p5.m2" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> for the long run, and for how long should it stay in the cache?â€
Concretely, each token <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p5.m3" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> receives a scalar retention score <math alttext="\beta_{i}\in[0,1]" class="ltx_Math" display="inline" id="S4.SS1.p5.m4" intent=":literal"><semantics><mrow><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">Î²</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi></msub><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">âˆˆ</mo><mrow><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">[</mo><mn mathcolor="#000000" style="--ltx-fg-color:#000000;">0</mn><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">,</mo><mn mathcolor="#000000" style="--ltx-fg-color:#000000;">1</mn><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">]</mo></mrow></mrow><annotation encoding="application/x-tex">\beta_{i}\in[0,1]</annotation></semantics></math> based only on information available at creation time (its representation, layer, and head), and its effective contribution at future step <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p5.m5" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> decays as <math alttext="\beta_{i}^{t-i}" class="ltx_Math" display="inline" id="S4.SS1.p5.m6" intent=":literal"><semantics><msubsup><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">Î²</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi><mrow><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">t</mi><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">âˆ’</mo><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">i</mi></mrow></msubsup><annotation encoding="application/x-tex">\beta_{i}^{t-i}</annotation></semantics></math>. This yields a smooth, exponentially decaying retention curve that is aligned naturally with long-term utility under memory constraints.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Brain-inspired Interpretation.</span> Our proposed retention-gated attention bears a natural connection to the classical Ebbinghausâ€™s forgetting curve theoryÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ebbinghaus2013image</span>)</cite>, which models human memory retention as an exponential decay over time. A common approximation of human retention rate is <math alttext="R=\exp(-tS)" class="ltx_Math" display="inline" id="S4.SS1.p6.m1" intent=":literal"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>âˆ’</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>S</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">R=\exp(-tS)</annotation></semantics></math>, where <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p6.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is the time and <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.p6.m3" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> is the memory strength determining how fast <math alttext="R" class="ltx_Math" display="inline" id="S4.SS1.p6.m4" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> decays over time in the absence of trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wozniak1995two</span>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p7">
<p class="ltx_p">In a similar spirit, our retention-gated attention models the contribution of past tokens as an exponentially decreasing function of their temporal distance from the current step, <span class="ltx_text ltx_font_italic">i.e.</span>, <math alttext="\exp((t-i)\log\beta_{i})" class="ltx_Math" display="inline" id="S4.SS1.p7.m1" intent=":literal"><semantics><mrow><mi>exp</mi><mo>â¡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">â€‹</mo><mrow><mi>log</mi><mo lspace="0.167em">â¡</mo><msub><mi>Î²</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\exp((t-i)\log\beta_{i})</annotation></semantics></math>. Each token begins with full weight (<math alttext="\mathrm{bar}\alpha_{ii}=1" class="ltx_Math" display="inline" id="S4.SS1.p7.m2" intent=":literal"><semantics><mrow><mrow><mi>bar</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î±</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathrm{bar}\alpha_{ii}=1</annotation></semantics></math>), akin to a newly encoded memory, and its influence decays as more tokens arrive, mirroring the drop in recall probability described by the forgetting curve. The parameter <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.p7.m3" intent=":literal"><semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> acts analogously to memory strength <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.p7.m4" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>: larger values yield more persistent, durable memories, while smaller values indicate weaker memories that fade quickly.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p8">
<p class="ltx_p">This connection provides an intuitive justification for our design. By embedding a forgetting mechanism into attention, we enable the model to dynamically prioritize recent or intrinsically important tokens while gradually discarding less informative context, mirroring how humans manage limited memory capacity in practice. Note that <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhong2024memorybank</span></cite> also drew on Ebbinghausâ€™s forgetting curve to construct a long-term memory bank, but their focus was on retrieval-augmented generation, whereas our approach integrates forgetting directly into the attention mechanism.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training</h3>
<figure class="ltx_figure ltx_align_floatright" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="S4.F2.g1" src="figs/architecture.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Training architecture.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">Our goal is to train the retention gate <math alttext="g" class="ltx_Math" display="inline" id="S4.SS2.p1.m1" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> so that the LLM can preserve response quality under a memory constraint, thereby bridging the gap with the inference stage. Instead of training a separate gate for each layer and head, as formulated in ProblemÂ (<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.E2" title="In 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>), we optimize all retention gates jointly in an end-to-end fashion. This holistic approach mitigates error propagation, allowing the model to learn a coordinated, globally optimal caching policy rather than greedy layer-wise decisions. Starting from a pretrained LLM, we replace every standard attention block with our proposed retention-gated attention. Each block is equipped with a lightweight retention gate <math alttext="g" class="ltx_Math" display="inline" id="S4.SS2.p1.m2" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> that maps token representations to retention scores <math alttext="\beta_{t}\in[0,1]" class="ltx_Math" display="inline" id="S4.SS2.p1.m3" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mi>t</mi></msub><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\beta_{t}\in[0,1]</annotation></semantics></math>, which are then used to modulate attention weights according to EquationÂ (<a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.E3" title="In 4.1 Selective In-Context Memory via Retention-Gated Attention â€£ 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">3</span></a>). We call this proxy LLM a retention-gated LLM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Objectives.</span> To train these retention gates, we formulate the training objective that balances two goals: (i) preserving the predictive quality of the original pretrained LLM, and (ii) enforcing memory capacity constraints by controlling the sum of retention scores at each step.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p">For the first objective, we use a combination of the distillation and standard next-token prediction losses. The distillation loss encourages the proxy LLM to align its output distribution with that of the baseline LLM using standard attention. In parallel, the next-token prediction loss enables the model to uncover sparsity patterns directly from the data, extending beyond the constraints of the pretrained LLM. Let <math alttext="p(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.m1" intent=":literal"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\cdot)</annotation></semantics></math> and <math alttext="q_{\theta}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.m2" intent=":literal"><semantics><mrow><msub><mi>q</mi><mi>Î¸</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q_{\theta}(\cdot)</annotation></semantics></math> be the output distribution of the pretrained LLM and retention-gated LLM, respectively, where <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS2.p3.m3" intent=":literal"><semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> denotes the parameters of all retention gates. The quality loss is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{quality}}=\mathcal{L}_{\mathrm{KL}}+\mathcal{L}_{\mathrm{NTP}}=\mathcal{D}_{\mathrm{KL}}\!\big(p(\cdot|x)\,\|\,q_{\theta}(\cdot|x)\big)+\mathbb{E}_{(x,y)}\!\left[-\log q_{\theta}(y|x)\right]." class="ltx_math_unparsed" display="block" id="S4.E4.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>quality</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>KL</mi></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>NTP</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>KL</mi></msub><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mi>p</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0.337em">âˆ¥</mo><msub><mi>q</mi><mi>Î¸</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow><mo>+</mo><msub><mi>ğ”¼</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo>[</mo><mo lspace="0em">âˆ’</mo><mi>log</mi><msub><mi>q</mi><mi>Î¸</mi></msub><mrow><mo stretchy="false">(</mo><mi>y</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{quality}}=\mathcal{L}_{\mathrm{KL}}+\mathcal{L}_{\mathrm{NTP}}=\mathcal{D}_{\mathrm{KL}}\!\big(p(\cdot|x)\,\|\,q_{\theta}(\cdot|x)\big)+\mathbb{E}_{(x,y)}\!\left[-\log q_{\theta}(y|x)\right].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="\mathcal{D}_{\mathrm{KL}}" class="ltx_Math" display="inline" id="S4.SS2.p3.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>KL</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{\mathrm{KL}}</annotation></semantics></math> is the standard forward Kullback-Leibler divergenceÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kullback1951information</span>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p">For the second objective, we impose a hinge-like regularization penalty, which discourages the model from exceeding the available KV memory slots at each step. For a retention gate within a given layer and KV head, the memory capacity loss is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{cap}}=\frac{1}{T(T-M)}\sum_{t=1}^{T}\max~\{0,\sum_{i=1}^{t}\beta_{i}^{t-i}-M\}," class="ltx_Math" display="block" id="S4.E5.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>cap</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>T</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>âˆ’</mo><mi>M</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mi>max</mi><mo lspace="0.330em">â¡</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo rspace="0em">,</mo><mrow><mrow><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup></mrow><mo>âˆ’</mo><mi>M</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{cap}}=\frac{1}{T(T-M)}\sum_{t=1}^{T}\max~\{0,\sum_{i=1}^{t}\beta_{i}^{t-i}-M\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="T" class="ltx_Math" display="inline" id="S4.SS2.p4.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the sequence length and <math alttext="M" class="ltx_Math" display="inline" id="S4.SS2.p4.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is the predefined memory capacity. Here, <math alttext="M" class="ltx_Math" display="inline" id="S4.SS2.p4.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> acts as a <span class="ltx_text ltx_font_italic">soft</span> hyperparameter, primarily intended to prevent over-optimization during the early decoding stage when the sequence remains short. Training is performed with a fixed value of <math alttext="M" class="ltx_Math" display="inline" id="S4.SS2.p4.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>, while inference can flexibly accommodate different KV budgets. This regularization is applied uniformly across all layers and KV heads of the transformer. The combined training objective is then:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\mathcal{L}_{\mathrm{quality}}+\lambda_{\mathrm{cap}}\mathcal{L}_{\mathrm{cap}}," class="ltx_Math" display="block" id="S4.E6.m1" intent=":literal"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mi>Î¸</mi></munder><mo lspace="0.167em">â¡</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>quality</mi></msub></mrow><mo>+</mo><mrow><msub><mi>Î»</mi><mi>cap</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>cap</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\theta}\mathcal{L}_{\mathrm{quality}}+\lambda_{\mathrm{cap}}\mathcal{L}_{\mathrm{cap}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda_{\mathrm{cap}}" class="ltx_Math" display="inline" id="S4.SS2.p4.m5" intent=":literal"><semantics><msub><mi>Î»</mi><mi>cap</mi></msub><annotation encoding="application/x-tex">\lambda_{\mathrm{cap}}</annotation></semantics></math> is a hyperparameter balancing between quality and capacity loss. Note that during training, only the retention gate parameters are updated, while all other model weights remain frozen.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hardware-aware Computation.</span> Retention-gated attention is fully parallelizable and compatible with FlashAttention-style kernelsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">dao2023flashattention</span>)</cite>. We implement it with FlexAttentionÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">dong2024flex</span>)</cite> plus a custom Triton kernel for the capacity loss <math alttext="\mathcal{L}_{\mathrm{cap}}" class="ltx_Math" display="inline" id="S4.SS2.p5.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>cap</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{cap}}</annotation></semantics></math>, performing forward/backward without materializing the full attention or <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.p5.m2" intent=":literal"><semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> matrices. This enables long-context training (up to 128K tokens on four H100 GPUs) with minor overhead versus standard parameter-efficient fine-tuning.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Inference</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">At inference time, the base LLM is augmented with the retention gates learned during training (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S4.SS2" title="4.2 Training â€£ 4 Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">4.2</span></a>). These gates provide token-level intrinsic importance scores <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="S4.SS3.p1.m1" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math>, which quantify how strongly each past token should be retained for future computations. Unlike training, where the retention gates are used to modulate the attention weights, at inference, they act purely as decision-makers for eviction, operating alongside but independently of attention computation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p">The eviction process is designed to ensure that the KV cache respects a predefined memory budget. Let <math alttext="S_{t}\subseteq\{1,\ldots,t\}" class="ltx_Math" display="inline" id="S4.SS3.p2.m1" intent=":literal"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>âŠ†</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S_{t}\subseteq\{1,\ldots,t\}</annotation></semantics></math> denote the set of tokens currently stored in the KV cache at decoding step <math alttext="t" class="ltx_Math" display="inline" id="S4.SS3.p2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. When a new token <math alttext="t+1" class="ltx_Math" display="inline" id="S4.SS3.p2.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math> is generated, it is provisionally added to the cache. If this addition causes the cache size to exceed the memory capacity <math alttext="M" class="ltx_Math" display="inline" id="S4.SS3.p2.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>, an eviction is triggered. The eviction rule is simple yet principled: we remove the token with the lowest retention score, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="j_{\mathrm{evic}}=\operatorname*{arg\,min}_{j\in S_{t}}\{\beta_{j}^{t-j}|j\in S_{t}\}." class="ltx_Math" display="block" id="S4.Ex2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>j</mi><mi>evic</mi></msub><mo>=</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.170em" rspace="0em">â€‹</mo><mi>min</mi></mrow><mrow><mi>j</mi><mo>âˆˆ</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><msubsup><mi>Î²</mi><mi>j</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>j</mi></mrow></msubsup><mo fence="false">|</mo><mi>j</mi></mrow><mo>âˆˆ</mo><msub><mi>S</mi><mi>t</mi></msub></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">j_{\mathrm{evic}}=\operatorname*{arg\,min}_{j\in S_{t}}\{\beta_{j}^{t-j}|j\in S_{t}\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Intuitively, this favors retaining tokens deemed globally important by the learned retention gates while discarding those with little long-term value. In practice, this makes inference both memory-efficient and adaptive: as new context arrives, the model continually re-evaluates the importance of older tokens, enabling long-context generation while keeping memory usage bounded. AlgorithmÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#algorithm1" title="In A.1 Inference Algorithm. â€£ Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates a single decoding step, where attention computation is coupled with token eviction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Complexity.</span> Our inference is simpler and more efficient than existing works, including pure heuristic baselinesÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024snapkv</span>)</cite>. Throughput and runtime comparisons are in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A1.SS2" title="A.2 Complexity â€£ Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">In this section, we conduct extensive experiments to demonstrate the performance advantages of our method on both <span class="ltx_text ltx_font_italic">long-context</span> and <span class="ltx_text ltx_font_italic">long-generation</span> tasks.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Long Generation Evaluation</h3>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="S5.F3.g1" src="figs/math_main.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Patero frontiers of competing algorithms with different budgets on math benchmarks.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Benchmarks.</span> Following prior workÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>)</cite>, we evaluate on standard math-reasoning suitesâ€”AIME24Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">aime2024</span>)</cite>, GSM8KÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">cobbe2021training</span>)</cite>, and MATH-500Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">hendrycks2021measuring</span>)</cite>. To assess performance beyond math reasoning and under long-context settings, we also report results on LongProcÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ye2025longproc</span>)</cite>. FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>, we report average pass@1 accuracy over 64 samples for AIME24 and 8 samples for GSM8K, MATH-500. We use greedy decoding for LongProc as the default in the benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Base Models.</span> FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>, we mainly use Qwen3â€™s family modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2025qwen3</span>)</cite>, including Qwen3-1.7B, Qwen3-4B, Qwen3-8B, Qwen3-14B and DeepSeek R1 DistillÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">guo2025deepseek</span>)</cite> variants including, DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Lllam-8B. We report the results with Qwen3 models in the main paper, and the remaining is in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2" title="Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span> We compare our method against SeerAttn-RÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>, R-KVÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>)</cite>, SnapKVÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024snapkv</span>)</cite>, H2OÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023h2o</span>)</cite>, StreamingLLMÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2023efficient</span>)</cite>. R-KV, SnapKV, H2O, and StreamingLLM are heuristic, recency-driven KV <em class="ltx_emph ltx_font_italic">eviction</em> policies for long-form generation under a fixed memory budget. SeerAttn-R is a learnable KV <em class="ltx_emph ltx_font_italic">retrieval</em> approach for reasoning tasks: rather than evicting, it offloads the full KV cache to host memory and uses recent queries to fetch relevant blocks for attention. KV retrieval methods preserve all past information but require nontrivial CPUâ€“GPU orchestration and incur offloading overhead. We therefore treat SeerAttn-R as a strong learnable baseline, and R-KV/SnapKV as representative eviction baselines.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Implementation Details.</span> We train the retention gates using OpenR1-MATH-220kÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">openr1math2025</span>)</cite> dataset, similar toÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite>. Note that we only train the retention gatesâ€™ weights while keeping the original model parameters frozen. We set the objective hyperparameters <math alttext="\lambda_{\mathrm{cap}}=1.0" class="ltx_Math" display="inline" id="S5.SS1.p4.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mi>cap</mi></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">\lambda_{\mathrm{cap}}=1.0</annotation></semantics></math> and the memory capacity <math alttext="M=512" class="ltx_Math" display="inline" id="S5.SS1.p4.m2" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">M=512</annotation></semantics></math>. Each transformer block has a retention gate <math alttext="g" class="ltx_Math" display="inline" id="S5.SS1.p4.m3" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>, which is a single MLP layer with the hidden dimension of <math alttext="512" class="ltx_Math" display="inline" id="S5.SS1.p4.m4" intent=":literal"><semantics><mn>512</mn><annotation encoding="application/x-tex">512</annotation></semantics></math>, thus having dimensions of <math alttext="d\to 512\to h" class="ltx_Math" display="inline" id="S5.SS1.p4.m5" intent=":literal"><semantics><mrow><mi>d</mi><mo stretchy="false">â†’</mo><mn>512</mn><mo stretchy="false">â†’</mo><mi>h</mi></mrow><annotation encoding="application/x-tex">d\to 512\to h</annotation></semantics></math>, where <math alttext="h" class="ltx_Math" display="inline" id="S5.SS1.p4.m6" intent=":literal"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> is the number of KV heads. We use the activation function as the default activation function in MLP layers of the base model. We initialize the bias in the retention gates to a large value (e.g., <math alttext="b=8.0" class="ltx_Math" display="inline" id="S5.SS1.p4.m7" intent=":literal"><semantics><mrow><mi>b</mi><mo>=</mo><mn>8.0</mn></mrow><annotation encoding="application/x-tex">b=8.0</annotation></semantics></math>) to begin training with minimal forgetting or compression. All trainings are on 4 H100 80G GPUs.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Quantitative Result</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Math Reasoning Tasks.</span> FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.F3" title="Figure 3 â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">3</span></a> shows our method outperforming all baselines by a large margin, especially in low-budget regimes. Notably, TRIM-KV surpasses attention-guided methods (R-KV, SnapKV) even when they are given <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.SS1.SSS1.p1.m1" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics></math> KV budget. Under the same budget, <span class="ltx_text ltx_font_italic">i.e.</span>1024 for AIME24 and 128 for GSM8K/MATH-500, it yields a <span class="ltx_text ltx_font_bold">198%</span> relative improvement over these baselines. Against the SOTA learnable KV retrieval baseline, TRIM-KV outperforms SeerAttn-R across all settings, yielding a <span class="ltx_text ltx_font_bold">58.4%</span> pass@1 gain at the same budget. Crucially, TRIM-KV operates in a pure KV-eviction regime, a stricter setting than the KV retrieval methods such as SeerAttn-R, and thus avoids CPUâ€“GPU offloading overhead. In some settings, like for Qwen3-4B model and AIME24 dataset, our method can even surpass the standard full KV cache. These results suggest that a large fraction of KV-cache tokens in reasoning models is redundant and can be discarded without degrading performance.</p>
</div>
<div class="ltx_para ltx_minipage ltx_align_top" id="S5.SS1.SSS1.p2" style="width:234.2pt;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Long Procedural Generation Tasks.</span> We evaluate KV-eviction methods on tasks that require both long-context comprehension and extended generation. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.T1" title="Table 1 â€£ 5.1.1 Quantitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> reports results with Qwen3-4B model. Overall, TRIM-KV consistently outperforms all other eviction baselines and, in several settings, even surpasses the full-cache model. Moreover, this result highlights that TRIM-KV with retention gates trained on math-reasoning data generalizes well to non-math tasks. Full results and analysis are provided in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2" title="Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_logical-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:145.1pt;vertical-align:-68.8pt;"><span class="ltx_transformed_inner" style="transform:translate(70.9pt,-23.7pt) scale(1.48641380958882,1.48641380958882) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method<math alttext="{}_{\text{KV budget}}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.m1" intent=":literal"><semantics><msub><mi></mi><mtext>KV budget</mtext></msub><annotation encoding="application/x-tex">{}_{\text{KV budget}}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" style="padding-top:0.75pt;padding-bottom:0.75pt;">CountDown</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" style="padding-top:0.75pt;padding-bottom:0.75pt;">Pseudo to Code</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row" style="padding-top:0.75pt;padding-bottom:0.75pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.75pt;padding-bottom:0.75pt;">0.5k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.75pt;padding-bottom:0.75pt;">2k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.75pt;padding-bottom:0.75pt;">8k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.75pt;padding-bottom:0.75pt;">0.5k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.75pt;padding-bottom:0.75pt;">2k</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
FullKV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">96.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">69.0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">50.8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">25.0</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">7.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">20.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.75pt;padding-bottom:0.75pt;">H2O<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">12.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">7.5</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.5</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">33.7</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">0.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.75pt;padding-bottom:0.75pt;">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">57.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">13.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.7</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">4.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.75pt;padding-bottom:0.75pt;">R-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.5</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.0</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">48.2</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">97.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">93.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_framed ltx_framed_underline">66.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_framed ltx_framed_underline">49.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_framed ltx_framed_underline">19.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<figure class="ltx_table ltx_align_center" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Qwen3-4B on LongProc. Bold is for the best, underline is for the best KV eviction.</figcaption>
</figure>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S5.F4.g1" src="figs/beta_alpha.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of token retention score <math alttext="\beta_{i}^{t-i}" class="ltx_Math" display="inline" id="S5.F4.m3" intent=":literal"><semantics><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\beta_{i}^{t-i}</annotation></semantics></math> (top) and eviction decisions <math alttext="\alpha_{ti}" class="ltx_Math" display="inline" id="S5.F4.m4" intent=":literal"><semantics><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{ti}</annotation></semantics></math> (bottom).</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Qualitative Result</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p1">
<p class="ltx_p">To examine the eviction policy learned by our retention gates, we run TRIM-KV on Qwen3-4B for the first example in AIME24 (see FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F14" title="Figure 14 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">14</span></a> for a visualization of the example). FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.F5" title="Figure 5 â€£ 5.1.2 Qualitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold">a</span>â€“<span class="ltx_text ltx_font_bold">b</span> show the mean retention score, averaged over all layers and heads, for each token in the example sequence. Aligning with our intuition, retention gates assign high scores to task-relevant tokens (e.g., <span class="ltx_text ltx_font_typewriter">ometer</span>, <span class="ltx_text ltx_font_typewriter">shop</span>, <span class="ltx_text ltx_font_typewriter">walk</span>, <span class="ltx_text ltx_font_typewriter">minutes</span>) and to the initial token <span class="ltx_text ltx_font_typewriter">&lt;|im_start|&gt;</span>, which often serves as an attention sink. In contrast, whitespace and punctuation receive low retention scores and are discarded early, yielding short lifespans in the KV cache. Next, we examine retention scores and eviction decisions at layerâ€“head granularity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Emergent Eviction Heuristics.</span> FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.F4" title="Figure 4 â€£ 5.1.1 Quantitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> visualizes the retention scores <math alttext="\beta^{t-i}_{i}" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p2.m1" intent=":literal"><semantics><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\beta^{t-i}_{i}</annotation></semantics></math> and eviction decisions <math alttext="\alpha_{ti}" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p2.m2" intent=":literal"><semantics><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{ti}</annotation></semantics></math> for selected layers and heads. Many eviction heuristics, such as attention sinksÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2023efficient</span>)</cite>, sliding windowsÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu2021long</span>)</cite>, A-shapeÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">jiang2024minference</span>)</cite>, emerge naturally from our learned policy without being hard-coded, and they adapt to the functional roles of individual layers and heads. For instance, sliding-window behavior is more common in early layers, whereas attention sinks appear more frequently in later layers (seeÂ FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F12" title="Figure 12 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">12</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F13" title="Figure 13 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">13</span></a> for a comprehensive view). Moreover, TRIM-KV adapts the window size by layer and head: in <span class="ltx_text ltx_font_italic">Layer 1/Head 1</span>, tokens receive nearly uniform retention scores, so the KV cache behaves like a recency-biased window that keeps the most recent tokens; in <span class="ltx_text ltx_font_italic">Layer 0/Head 0</span>, multiple sliding windows of varying widths emerge from the learned policy; in <span class="ltx_text ltx_font_italic">Layer 15/Head 2</span>, no sliding window is observed because certain tokens receive substantially higher retention than others, suggesting a specialized functional role for this head. The A-shaped pattern typically appears in layers that emphasize instruction/problem-statement tokens (<span class="ltx_text ltx_font_italic">e.g.</span>, <span class="ltx_text ltx_font_italic">Layer 20/Head 5</span> and <span class="ltx_text ltx_font_italic">Layer 30/Head 4</span>) or chain-of-thought/reasoning prompts (<span class="ltx_text ltx_font_italic">e.g.</span>, <span class="ltx_text ltx_font_italic">Layer 35/Head 1</span>). These heads also exhibit context switching, where small, dense lower-triangular blocks emerge and then fade quickly when the context changes or a sentence completes. To the best of our knowledge, the absence of a sliding window, the presence of multiple coexisting windows, and context switching are newly observed eviction patterns that arise naturally from our learned policy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Token Retention Enables Interpretability.</span> Beyond guiding eviction policy, token-level retention scores provide a diagnostic tool for analyzing the functional roles of individual KV heads in the base LLM. Visualizing retention scores alongside the tokens preserved in the KV cache after generation reveals distinct specializations: some heads emphasize a recency window (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F17" title="Figure 17 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">17</span></a>), whereas others preferentially retain mathematical tokens-numbers and operators (FiguresÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F14" title="Figure 14 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">14</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F20" title="Figure 20 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">20</span></a>)-or variables (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F23" title="Figure 23 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">23</span></a>), as well as problem-description tokens (FiguresÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F19" title="Figure 19 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">19</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F25" title="Figure 25 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">25</span></a>) and chain-of-thought instructions (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F26" title="Figure 26 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">26</span></a>). Even function or filler words, such as pronouns, prepositions, conjunctions, <span class="ltx_text ltx_font_typewriter">wait</span> and <span class="ltx_text ltx_font_typewriter">let</span>, tend to be kept by specific heads (FiguresÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F18" title="Figure 18 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">18</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F24" title="Figure 24 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">24</span></a>). In particular, heads that exhibit context-switching patterns (e.g., <em class="ltx_emph ltx_font_italic">Layer 30/Head 4</em> and <em class="ltx_emph ltx_font_italic">Layer 16/Head 6</em>) tend to retain the period token while discarding others (FiguresÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F22" title="Figure 22 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">22</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A3.F21" title="Figure 21 â€£ Appendix C Additional Qualitative Results â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">21</span></a>). We hypothesize that, in these heads, periods act as implicit <em class="ltx_emph ltx_font_italic">gist</em> tokensÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mu2023learning</span>)</cite>, summarizing the information in the preceding sentences.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S5.F5.g1" src="figs/order_sparsity.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_italic">a)</span> Average retention scores across all layers and heads of Qwen3-4B on tokens of an AIME24 example. <span class="ltx_text ltx_font_italic">b)</span> Top 10 tokens with the highest (left table) and lowest (right table) average retention. <span class="ltx_text ltx_font_italic">c)</span> The layer- and head-wise sparsity level estimated by token retentions.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p4">
<p class="ltx_p">Our analyses indicate that KV heads in LLM develop different functional roles and therefore keep different types of tokens. These tokens are often dispersed across the context rather than forming contiguous chunks, as each already captures contextual information. This observation contrasts with existing approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yuan2025native</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>)</cite> that advocate chunk- or block-based KV-cache. Instead, we show that keeping a small number of high-context tokens is more budget-effective.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Budget Allocation.</span> FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.F5" title="Figure 5 â€£ 5.1.2 Qualitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold">c</span> reports head- and layer-wise sparsity estimated from the retention scores <span class="ltx_text ltx_font_italic">i.e.</span>, <math alttext="1-\frac{2}{T(T+1)}\sum_{i&lt;t}\beta_{i}^{\,t-i}" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p5.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mfrac><mn>2</mn><mrow><mi>T</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><msub><mo>âˆ‘</mo><mrow><mi>i</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">1-\frac{2}{T(T+1)}\sum_{i&lt;t}\beta_{i}^{\,t-i}</annotation></semantics></math>. We observe that later layers are typically sparser than earlier ones, consistent with prior findings inÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2024pyramidkv</span>)</cite>. Practically, the retention scores enable heterogeneous budgets across KV heads under a global constraint by evicting tokens with low global retention.
However, existing KV-cache and FlashAttention implementations assume uniform sequence lengths across heads within a layer; enabling efficient per-head variable-length caches is left to future work.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Long-Context Evaluation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p">To further stress-test our approach on long-context tasks, we evaluate it on LongMemEval<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">S</span></sub>Â <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wu2024longmemeval</span>)</cite>, a benchmark for assessing chat assistantsâ€™ long-term interactive memory (contexts up to 123K tokens with Qwen3) <span class="ltx_text" style="--ltx-fg-color:#000000;">and SCBenchÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024scbench</span>)</cite>, a benchmark adapted from RULERÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">hsieh2024ruler</span>)</cite> and InfiniteBenchÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2024bench</span>)</cite> to evaluate long-context understanding of KV cache compression methods under the same context but different queries</span>. We use Qwen3-4B-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">qwen3_4b_instruct</span>)</cite> as the base model and train the retention gates on SynthLongÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">cerebras2025synthlong</span>)</cite>, BookSumÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kryscinski2021booksum</span>)</cite>, and BuddhiÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">singhal2024buddhi</span>)</cite>, whose context lengths extend up to 128K tokens. All other experimental settings follow SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1" title="5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:44.6pt;vertical-align:-20.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-139.0pt,14.3pt) scale(0.609324685002965,0.609324685002965) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">En.MultiChoice</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Retr.KV</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ICL.ManyShot</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Math.Find</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">En.QA</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Code.RepoQA</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">En.Sum</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Mix.Sum+NIAH</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Retr.MultiHop</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
Full KV</th>
<td class="ltx_td ltx_align_right ltx_border_t">20.52</td>
<td class="ltx_td ltx_align_right ltx_border_t">66.00</td>
<td class="ltx_td ltx_align_right ltx_border_t">95.57</td>
<td class="ltx_td ltx_align_right ltx_border_t">32.60</td>
<td class="ltx_td ltx_align_right ltx_border_t">28.78</td>
<td class="ltx_td ltx_align_right ltx_border_t">53.86</td>
<td class="ltx_td ltx_align_right ltx_border_t">36.48</td>
<td class="ltx_td ltx_align_right ltx_border_t">38.33</td>
<td class="ltx_td ltx_align_right ltx_border_t">49.60</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</th>
<td class="ltx_td ltx_align_right">5.68</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold">2.20</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold">100.00</span></td>
<td class="ltx_td ltx_align_right">13.20</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline">6.84</span></td>
<td class="ltx_td ltx_align_right">2.96</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline">29.21</span></td>
<td class="ltx_td ltx_align_right">28.25</td>
<td class="ltx_td ltx_align_right">0.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">H2O<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</th>
<td class="ltx_td ltx_align_right">4.80</td>
<td class="ltx_td ltx_align_right">0.00</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold">100.00</span></td>
<td class="ltx_td ltx_align_right">8.00</td>
<td class="ltx_td ltx_align_right">3.70</td>
<td class="ltx_td ltx_align_right">0.46</td>
<td class="ltx_td ltx_align_right">8.97</td>
<td class="ltx_td ltx_align_right">6.51</td>
<td class="ltx_td ltx_align_right">0.27</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline">10.04</span></td>
<td class="ltx_td ltx_align_right">0.00</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold">100.00</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold">18.60</span></td>
<td class="ltx_td ltx_align_right">6.29</td>
<td class="ltx_td ltx_align_right">0.23</td>
<td class="ltx_td ltx_align_right">27.90</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline">29.28</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline">0.31</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">13.10</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">100.00</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline">13.80</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">19.09</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">4.32</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">33.66</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">34.06</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">43.11</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance on long-context tasks from the SCBench benchmark.</figcaption>
</figure>
<div class="ltx_para ltx_minipage ltx_align_top" id="S5.SS2.p2" style="width:342.6pt;">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.T3" title="Table 3 â€£ 5.2 Long-Context Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">3</span></a> and TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.T2" title="Table 2 â€£ 5.2 Long-Context Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> show the results on LongMemEval and SCBench benchmarks respectively. On LongMemEval, TRIM-KV outperforms baselines by a significant margin. Especially, TRIM-KV can maintain the performance of a full cache while using just <math alttext="25\%" class="ltx_Math" display="inline" id="S5.SS2.m1" intent=":literal"><semantics><mrow><mn mathcolor="#000000" style="--ltx-fg-color:#000000;">25</mn><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">%</mo></mrow><annotation encoding="application/x-tex">25\%</annotation></semantics></math> of the KV budget, while other baselines drop sharply. On SCBench, TRIM-KV also shows a competitive performance across most of the evaluated tasks. On challenging retrieval tasks such as Retr.KV and Code.RepoQA, all KV eviction methods failed to give a reasonable performance because the context is incompressible. This is already observed inÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024scbench</span>)</cite>. These results highlight our methodâ€™s advantage on both long-context and long-generation tasks, whereas most prior work targets either the prefill or the generation stage, but not both. More details and results for this experiment and LongBenchÂ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2025longbench</span>)</cite> are provided in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2" title="Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></p>
</div>
<div class="ltx_logical-block ltx_minipage ltx_align_top" style="width:84.6pt;">
<div class="ltx_para" id="S5.SS2.p3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:381.4pt;vertical-align:-183.6pt;"><span class="ltx_transformed_inner" style="transform:translate(140.4pt,-123.5pt) scale(2.83797932443083,2.83797932443083) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method<math alttext="{}_{\text{KV budget}}" class="ltx_Math" display="inline" id="S5.SS2.m2" intent=":literal"><semantics><msub><mi></mi><mtext>KV budget</mtext></msub><annotation encoding="application/x-tex">{}_{\text{KV budget}}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
Full KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">131072</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">27.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">27.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">48.2</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">19.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">18.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">42.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">10.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">13.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">30.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<figure class="ltx_table ltx_align_center" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on LongMemEval<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">S</span></sub>.</figcaption>
</figure>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h3>
<div class="ltx_para ltx_minipage ltx_align_top" id="S5.SS3.p1" style="width:303.5pt;">
<p class="ltx_p">We ablate the objective by training the Qwen3-4B retention gates with different loss combinations and report AIME24 pass@1 at a 4096-token budget in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.T4" title="Table 4 â€£ 5.3 Ablation Studies â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a>. Both forward KL and next-token prediction perform well on their own, and their combination further improves accuracy. The memory capacity loss is essential for compression, and removing it leads to a sharp drop. We provide comprehensive ablations with reversed KL, generalization with different training datasets, gateâ€™s architecture, and other hyperparameters such as <math alttext="M" class="ltx_Math" display="inline" id="S5.SS3.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.SS4" title="B.4 Additional Ablation Studies â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">B.4</span></a>.</p>
</div>
<div class="ltx_logical-block ltx_minipage ltx_align_top" style="width:125.7pt;">
<div class="ltx_para" id="S5.SS3.p2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:198.9pt;vertical-align:-92.7pt;"><span class="ltx_transformed_inner" style="transform:translate(137.0pt,-62.8pt) scale(2.71719590422043,2.71719590422043) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method<math alttext="{}_{\text{KV budget}}" class="ltx_Math" display="inline" id="S5.SS3.m2" intent=":literal"><semantics><msub><mi></mi><mtext>KV budget</mtext></msub><annotation encoding="application/x-tex">{}_{\text{KV budget}}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">pass@1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
Full KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_text ltx_font_bold">74.0</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">(TRIM-KV <math alttext="-\mathcal{L}_{\mathrm{KL}}" class="ltx_Math" display="inline" id="S5.SS3.m5" intent=":literal"><semantics><mrow><mo>âˆ’</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>KL</mi></msub></mrow><annotation encoding="application/x-tex">-\mathcal{L}_{\mathrm{KL}}</annotation></semantics></math>)<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">(TRIM-KV <math alttext="-\mathcal{L}_{\mathrm{NTP}}" class="ltx_Math" display="inline" id="S5.SS3.m7" intent=":literal"><semantics><mrow><mo>âˆ’</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>NTP</mi></msub></mrow><annotation encoding="application/x-tex">-\mathcal{L}_{\mathrm{NTP}}</annotation></semantics></math>)<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">(TRIM-KV <math alttext="-\mathcal{L}_{\mathrm{cap}}" class="ltx_Math" display="inline" id="S5.SS3.m9" intent=":literal"><semantics><mrow><mo>âˆ’</mo><msub><mi class="ltx_font_mathcaligraphic">â„’</mi><mi>cap</mi></msub></mrow><annotation encoding="application/x-tex">-\mathcal{L}_{\mathrm{cap}}</annotation></semantics></math>)<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.9</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<figure class="ltx_table ltx_align_center" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Objective ablation.</figcaption>
</figure>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">We introduced TRIM-KV, a learnable, retention-gated approach to KV cache management that prioritizes tokens by intrinsic importance rather than recent attention. By training lightweight gates with distillation and a capacity loss, our method enforces strict memory budgets with a simple and efficient eviction policy. Extensive experiments across math reasoning, long-procedural generation, and conversational long-memory benchmarks demonstrate that our method outperforms strong eviction and retrieval baselines, and sometimes even surpasses full-cache models. Analyses show that the learned retention scores align with human intuitions and reveal layer- and head-specific dynamics, offering a simple probe for interpretability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">Future Work.<span class="ltx_text ltx_font_medium"> Our results indicate that retention-gated attention is an effective learnable proxy for approximating standard attention with eviction during inference. In the current work, however, we keep the backbone parameters frozen during training and still rely on standard attention at inference time. A natural next step is to replace standard attention with retention-gated attention and train the retention mechanism jointly with the attention layers during pretraining or post-training. This could allow the retention scores to better cooperate with the learned query, key, and value states, shaping the modelâ€™s behaviors from the outset rather than optimizing retention on top of a fixed attention stack. Such a design would enable training objectives that explicitly trade off task performance and memory usage, potentially yielding models that are inherently memory-bounded without requiring any post hoc compression policy.</span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">Besides, building on these results, we plan to extend retention gating to multimodal inputs, tool-calling applications, and develop adaptive budgets that allocate memory across layers, heads, and tasks to further improve both performance and efficiency.</span></p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p">This work aims to improve the efficiency of large language models by reducing their memory and computational footprint. Our method can make long-context reasoning more accessible by lowering hardware costs, which may democratize access to advanced LLM capabilities. However, efficiency improvements may also accelerate the deployment of LLMs in high-stakes or resource-limited settings where risks around misinformation, bias, or misuse persist. We stress that our method does not mitigate these broader societal risks and should be paired with ongoing efforts in safety, fairness, and responsible deployment.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Reproducibility Statement</h2>
<div class="ltx_para ltx_noindent" id="Sx2.p1">
<p class="ltx_p">We ensure reproducibility by providing detailed descriptions of the model architecture, training objectives, and evaluation benchmarks in the main text and appendix. Hyperparameters, training schedules, and implementation details are included for all experiments. All datasets we use are publicly available, and we will release code, model checkpoints, and scripts for training and evaluation upon publication. Together, these materials allow independent researchers to fully reproduce and verify our results.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.p2">
<p class="ltx_p">The authors used large language models to help refine and polish the writing of this manuscript.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Methodology</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Inference Algorithm.</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p">AlgorithmÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#algorithm1" title="In A.1 Inference Algorithm. â€£ Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the attention computation with KV eviction using retention gates for a single decoding step. We mark the parts that are different from the standard attention computation in blue.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Input :</span>Â current hidden <math alttext="\mathbf{x}_{t}" class="ltx_Math" display="inline" id="algorithm1.m1" intent=":literal"><semantics><msub><mi>ğ±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_{t}</annotation></semantics></math>; KV cache <math alttext="\mathbf{K}_{t-1},\mathbf{V}_{t-1},\mathbf{B}_{t-1}" class="ltx_Math" display="inline" id="algorithm1.m2" intent=":literal"><semantics><mrow><msub><mi>ğŠ</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ğ•</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ğ</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{K}_{t-1},\mathbf{V}_{t-1},\mathbf{B}_{t-1}</annotation></semantics></math> indexed by <math alttext="S_{t-1}" class="ltx_Math" display="inline" id="algorithm1.m3" intent=":literal"><semantics><msub><mi>S</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">S_{t-1}</annotation></semantics></math>; retention gate <math alttext="g" class="ltx_Math" display="inline" id="algorithm1.m4" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Param :</span>Â capacity <math alttext="M" class="ltx_Math" display="inline" id="algorithm1.m5" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>;
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Output :</span>Â attention output <math alttext="\mathbf{o}_{t}" class="ltx_Math" display="inline" id="algorithm1.m6" intent=":literal"><semantics><msub><mi>ğ¨</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{o}_{t}</annotation></semantics></math>; updated <math alttext="(\mathbf{K}_{t},\mathbf{V}_{t},\mathbf{B}_{t})" class="ltx_Math" display="inline" id="algorithm1.m7" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>ğŠ</mi><mi>t</mi></msub><mo>,</mo><msub><mi>ğ•</mi><mi>t</mi></msub><mo>,</mo><msub><mi>ğ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{K}_{t},\mathbf{V}_{t},\mathbf{B}_{t})</annotation></semantics></math>; updated index set <math alttext="S_{t}" class="ltx_Math" display="inline" id="algorithm1.m8" intent=":literal"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>
</div>
<div class="ltx_listingline">1ex<span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">1) Project to Q/K/V for the current token </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
<math alttext="\mathbf{q}_{t}\leftarrow\mathbf{W}_{Q}\mathbf{x}_{t}" class="ltx_Math" display="inline" id="algorithm1.m9" intent=":literal"><semantics><mrow><msub><mi>ğª</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><msub><mi>ğ–</mi><mi>Q</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}_{t}\leftarrow\mathbf{W}_{Q}\mathbf{x}_{t}</annotation></semantics></math>;â€ƒ<math alttext="\mathbf{k}_{t}\leftarrow\mathbf{W}_{K}\mathbf{x}_{t}" class="ltx_Math" display="inline" id="algorithm1.m10" intent=":literal"><semantics><mrow><msub><mi>ğ¤</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><msub><mi>ğ–</mi><mi>K</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{k}_{t}\leftarrow\mathbf{W}_{K}\mathbf{x}_{t}</annotation></semantics></math>;â€ƒ<math alttext="\mathbf{v}_{t}\leftarrow\mathbf{W}_{V}\mathbf{x}_{t}" class="ltx_Math" display="inline" id="algorithm1.m11" intent=":literal"><semantics><mrow><msub><mi>ğ¯</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><msub><mi>ğ–</mi><mi>V</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ±</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v}_{t}\leftarrow\mathbf{W}_{V}\mathbf{x}_{t}</annotation></semantics></math>;â€ƒ<math alttext="\beta_{t}=g(\mathbf{x}_{t})" class="ltx_Math" display="inline" id="algorithm1.m12" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">=</mo><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">(</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ±</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\beta_{t}=g(\mathbf{x}_{t})</annotation></semantics></math>;
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">2) Append current token to the KV cache </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>
<math alttext="\mathbf{K}_{t}\leftarrow\mathbf{K}_{t-1}\cup\mathbf{k}_{t}" class="ltx_Math" display="inline" id="algorithm1.m13" intent=":literal"><semantics><mrow><msub><mi>ğŠ</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><msub><mi>ğŠ</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>âˆª</mo><msub><mi>ğ¤</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{K}_{t}\leftarrow\mathbf{K}_{t-1}\cup\mathbf{k}_{t}</annotation></semantics></math>;â€ƒ<math alttext="\mathbf{V}_{t}\leftarrow\mathbf{V}_{t-1}\cup\mathbf{v}_{t}" class="ltx_Math" display="inline" id="algorithm1.m14" intent=":literal"><semantics><mrow><msub><mi>ğ•</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><msub><mi>ğ•</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>âˆª</mo><msub><mi>ğ¯</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{V}_{t}\leftarrow\mathbf{V}_{t-1}\cup\mathbf{v}_{t}</annotation></semantics></math>;â€ƒ<math alttext="\mathbf{B}_{t}\leftarrow\mathbf{B}_{t-1}\cup\beta_{t}" class="ltx_Math" display="inline" id="algorithm1.m15" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">â†</mo><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ</mi><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ’</mo><mn mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">1</mn></mrow></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆª</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbf{B}_{t}\leftarrow\mathbf{B}_{t-1}\cup\beta_{t}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">;â€ƒ<math alttext="S_{t}\leftarrow S_{t-1}\cup\{t\}" class="ltx_Math" display="inline" id="algorithm1.m16" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">â†</mo><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ’</mo><mn mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">1</mn></mrow></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆª</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">{</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">S_{t}\leftarrow S_{t-1}\cup\{t\}</annotation></semantics></math></span>;
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">3) Compute attention over currently cached keys/values (restricted to <math alttext="S_{t}" class="ltx_Math" display="inline" id="algorithm1.m17" intent=":literal"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation></semantics></math>) </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>
<math alttext="\mathbf{o}_{t}\leftarrow\textsc{FlashAttn}(\mathbf{q}_{t},\mathbf{K}_{t},\mathbf{V}_{t})" class="ltx_Math" display="inline" id="algorithm1.m18" intent=":literal"><semantics><mrow><msub><mi>ğ¨</mi><mi>t</mi></msub><mo stretchy="false">â†</mo><mrow><mtext class="ltx_font_smallcaps">FlashAttn</mtext><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğª</mi><mi>t</mi></msub><mo>,</mo><msub><mi>ğŠ</mi><mi>t</mi></msub><mo>,</mo><msub><mi>ğ•</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{o}_{t}\leftarrow\textsc{FlashAttn}(\mathbf{q}_{t},\mathbf{K}_{t},\mathbf{V}_{t})</annotation></semantics></math>;
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">4) If capacity exceeded, evict the least important token </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">5</span></span>
<span class="ltx_text" style="--ltx-fg-color:#0000FF;">
<span class="ltx_text ltx_font_bold">while</span> <em class="ltx_emph ltx_font_italic"><math alttext="|S_{t}|&gt;M" class="ltx_Math" display="inline" id="algorithm1.m19" intent=":literal"><semantics><mrow><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">|</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">|</mo></mrow><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">&gt;</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">M</mi></mrow><annotation encoding="application/x-tex">|S_{t}|&gt;M</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold">do</span> </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">6</span></span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€‚</span><span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;">Â </span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€ƒ</span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">
</span><math alttext="j_{\mathrm{evic}}\leftarrow\arg\min\ \{\beta_{j}^{t-j}|j\in S_{t}\}" class="ltx_Math" display="inline" id="algorithm1.m20" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">evic</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">â†</mo><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">arg</mi><mo lspace="0.167em">â¡</mo><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">min</mi><mo lspace="0.500em">â¡</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">{</mo><mrow><mrow><msubsup><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">Î²</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ’</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi></mrow></msubsup><mo fence="false" mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">|</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi></mrow><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆˆ</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub></mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">j_{\mathrm{evic}}\leftarrow\arg\min\ \{\beta_{j}^{t-j}|j\in S_{t}\}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">7</span></span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€‚</span><span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;">Â </span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€ƒ</span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">
Remove </span><math alttext="\mathbf{K}_{t}[j_{\mathrm{evic}}]" class="ltx_Math" display="inline" id="algorithm1.m21" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğŠ</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">[</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">evic</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{K}_{t}[j_{\mathrm{evic}}]</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">, </span><math alttext="\mathbf{V}_{t}[j_{\mathrm{evic}}]" class="ltx_Math" display="inline" id="algorithm1.m22" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ•</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">[</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">evic</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{V}_{t}[j_{\mathrm{evic}}]</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">, </span><math alttext="\mathbf{B}_{t}[{j_{\mathrm{evic}}}]" class="ltx_Math" display="inline" id="algorithm1.m23" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">ğ</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">[</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">evic</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{B}_{t}[{j_{\mathrm{evic}}}]</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">8</span></span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€‚</span><span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;">Â </span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€ƒ</span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">
</span><math alttext="S_{t}\leftarrow S_{t}\setminus\{j_{\mathrm{evic}}\}" class="ltx_Math" display="inline" id="algorithm1.m24" intent=":literal"><semantics><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">â†</mo><mrow><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">S</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">âˆ–</mo><mrow><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">{</mo><msub><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">j</mi><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">evic</mi></msub><mo mathcolor="#0000FF" stretchy="false" style="--ltx-fg-color:#0000FF;">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">S_{t}\leftarrow S_{t}\setminus\{j_{\mathrm{evic}}\}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0000FF;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">9</span></span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€‚</span><span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;">Â </span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">â€ƒ</span><span class="ltx_text" style="--ltx-fg-color:#0000FF;">
</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="--ltx-fg-color:#0000FF;">10</span></span><span class="ltx_text" style="--ltx-fg-color:#0000FF;"> end while</span>
</div>
<div class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">AlgorithmÂ 1</span> </span>Attention computation with KV eviction (single decoding step)</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">Positional Encoding in KV Cache Eviction.<span class="ltx_text ltx_font_medium"> Our retention mechanism is designed to be positional-encoding agnostic and does not add any extra recency bias beyond what is already present in the base model. The exponential decay in the retention-gated attention is a smooth approximation of the decay process from 1 to 0 of the standard attention with eviction, not to encode the positional information. Therefore, regardless of whether the base model uses absolute positions, RoPE, or no positional encoding, that information is already folded into <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="A1.SS1.p2.m1" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğ±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, <math alttext="\mathbf{q}" class="ltx_Math" display="inline" id="A1.SS1.p2.m2" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğª</mi><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math>, and <math alttext="\mathbf{k}" class="ltx_Math" display="inline" id="A1.SS1.p2.m3" intent=":literal"><semantics><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ğ¤</mi><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math> in the standard attention (see EqÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S3.E1" title="In 3.2 Revisiting KV Cache Eviction â€£ 3 Preliminaries â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>).</span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p3">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">Implementation-wise, when using RoPE, we follow prior work (R-KV, SnapKV) and cache post-rotated keys, so the eviction is orthogonal to the positional embeddings used.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Complexity</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Memory efficiency.</span> Like other KV-eviction schemes, <span class="ltx_text ltx_font_smallcaps">TRIM-KV</span> uses a fixed-size cache with <math alttext="\mathcal{O}(M)" class="ltx_Math" display="inline" id="A1.SS2.p1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ğ’ª</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{O}(M)</annotation></semantics></math> slots, independent of sequence length <math alttext="T" class="ltx_Math" display="inline" id="A1.SS2.p1.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>. For each token (per head), it stores a single scalar retention score <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="A1.SS2.p1.m3" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math>, adding only <math alttext="\approx 1/d_{h}" class="ltx_Math" display="inline" id="A1.SS2.p1.m4" intent=":literal"><semantics><mrow><mi></mi><mo>â‰ˆ</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>d</mi><mi>h</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\approx 1/d_{h}</annotation></semantics></math> overhead, where <math alttext="d_{h}" class="ltx_Math" display="inline" id="A1.SS2.p1.m5" intent=":literal"><semantics><msub><mi>d</mi><mi>h</mi></msub><annotation encoding="application/x-tex">d_{h}</annotation></semantics></math> is the dimension of the key and vector states, relative to the KV states, which is negligible in practice. Unlike R-KVÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cai2025r</span>]</cite>, <span class="ltx_text ltx_font_smallcaps">TRIM-KV</span> does not store queries.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:130.3pt;vertical-align:-63.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.7pt,13.8pt) scale(0.825042853916507,0.825042853916507) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Context Length</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Gen Length</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Batch Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Throughput (tok/sec)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Decode Time (s)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">FullKV</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">32786</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">4</td>
<td class="ltx_td ltx_align_center ltx_border_t">68.44</td>
<td class="ltx_td ltx_align_center ltx_border_t">59.84</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SeerAttn-R</td>
<td class="ltx_td ltx_align_center">68.93</td>
<td class="ltx_td ltx_align_center">59.41</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV</td>
<td class="ltx_td ltx_align_center">124.67</td>
<td class="ltx_td ltx_align_center">33.00</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV</td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">130.48</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">31.39</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">FullKV</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">16378</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">4</td>
<td class="ltx_td ltx_align_center ltx_border_t">114.39</td>
<td class="ltx_td ltx_align_center ltx_border_t">35.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SeerAttn-R</td>
<td class="ltx_td ltx_align_center">100.45</td>
<td class="ltx_td ltx_align_center">40.77</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">153.21</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">26.73</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV</td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">151.04</td>
<td class="ltx_td ltx_align_center">27.11</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">FullKV</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">16378</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3">8</td>
<td class="ltx_td ltx_align_center ltx_border_t">138.97</td>
<td class="ltx_td ltx_align_center ltx_border_t">58.94</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SeerAttn-R</td>
<td class="ltx_td ltx_align_center">139.34</td>
<td class="ltx_td ltx_align_center">58.78</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV</td>
<td class="ltx_td ltx_align_center">244.60</td>
<td class="ltx_td ltx_align_center">33.49</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV</td>
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">279.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">29.26</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Throughput and decoding time comparisons of different KV cache methods on a single H200 GPU. The memory budget <math alttext="M" class="ltx_Math" display="inline" id="A1.T5.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is 1024.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Computational efficiency.</span> For each generated token, <span class="ltx_text ltx_font_smallcaps">TRIM-KV</span> computes a scalar retention score <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="A1.SS2.p2.m1" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math> via a lightweight MLP that can be fused with QKV projections; scores are cached and not recomputed each step. During compression, it applies a temporal discount (elementwise power) and evicts the argmin; both costs only <math alttext="\mathcal{O}(M)" class="ltx_Math" display="inline" id="A1.SS2.p2.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ğ’ª</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{O}(M)</annotation></semantics></math>. This is cheaper than heuristics like R-KV, which require keyâ€“key similarity scoring over the cache. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A1.T5" title="Table 5 â€£ A.2 Complexity â€£ Appendix A Methodology â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5</span></a> reports throughput and latency: at 32K context, <span class="ltx_text ltx_font_smallcaps">TRIM-KV</span> achieves <math alttext="\sim\!2\times" class="ltx_math_unparsed" display="inline" id="A1.SS2.p2.m3" intent=":literal"><semantics><mrow><mo rspace="0.108em">âˆ¼</mo><mn>2</mn><mo lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex">\sim\!2\times</annotation></semantics></math> higher decoding throughput than full-cache decoding and even faster than SnapKV, a purely heuristic method. SeerAttn-R does not provide any computation advantage over full cache model.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Experiments</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Long Generation Evaluation</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p">We provide more comprehensive experiment details in this section.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Experiment settings.</span> For the training, we set the maximum training length to be 16384. We train the retention gates with a learning rate of <math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A2.SS1.p2.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">Ã—</mo><msup><mn>10</mn><mrow><mo>âˆ’</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math> and a weight decay of 0.01. <span class="ltx_text" style="--ltx-fg-color:#000000;">For math reasoning tasks, we follow SeerAttn-RÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>]</cite> that uses OpenR1-Math-220KÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">openr1math2025</span>]</cite> dataset, which has 564M tokens for training. During training, we use a batch size of 1 for each GPU, and gradient accumulation is set to 4. Other hyperparameters are set to the default in Huggingface Trainer. All training is conducted on 4 H100 GPUs.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Benchmarks.</span> AIME24Â <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">aime2024</span>]</cite>, GSM8KÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cobbe2021training</span>]</cite>, and MATH-500Â <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hendrycks2021measuring</span>]</cite> are standard math reasoning benchmarks. LongProcÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ye2025longproc</span>]</cite> is a long-context benchmark of six procedural-generation tasks that require integrating dispersed information and producing structured long-form outputs (up to <math alttext="\sim 8" class="ltx_Math" display="inline" id="A2.SS1.p3.m1" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">\sim 8</annotation></semantics></math>K tokens)â€”from extracting tables from HTML to executing multi-step search to build feasible travel itineraries. The suite spans varied access patterns (sequential vs. targeted retrieval), deductive reasoning demands, and search execution, enabling stress tests of long-range coherence and procedure following. Each task includes deterministic solution procedures and structured outputs, allowing rule-based evaluation (row-level F1 for HTMLâ†’TSV, unit tests for pseudocodeâ†’code, exact-match traces for Path/ToM, and validators for Countdown/Travel). To probe generation length, we use three difficulty tiers targeting Â 0.5K, 2K, and 8K output tokens.</p>
</div>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="A2.F6.g1" src="figs/math_additional.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Patero frontiers of competing algorithms with different budgets on AIME24.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Math reasoning results.</span> FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F6" title="Figure 6 â€£ B.1 Long Generation Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">6</span></a> reports AIME24 performance for Qwen3-1.7B and DeepSeek-R1-Distill variants. Across both families, TRIM-KV consistently outperforms eviction baselines. The gains over heuristic baselines are smaller on DeepSeek-R1-Distillâ€“Llama-8B, which we hypothesize reflects lower attention sparsity in this model compared to the Qwen3 family.</p>
</div>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="A2.F7.g1" src="figs/keydiff_amie24.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparison to KeyDiff, a query-agnostic baseline.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">Comparison to a query-agnostic baseline.<span class="ltx_text ltx_font_medium"> We provide a comparison to KeyDiffÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">park2025keydiff</span>]</cite>, a query-agnostic baseline that only considers a key diversity for eviction. The result in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F7" title="Figure 7 â€£ B.1 Long Generation Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the performance of KeyDiff is significantly worse than that of other baselines. Note that R-KV can be considered as a generalization of KeyDiff since it considers both key diversity and attention scores for eviction heuristics.</span></span></p>
</div>
<figure class="ltx_table" id="A2.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:281.9pt;height:106.3pt;vertical-align:-51.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.6pt,19.8pt) scale(0.72817369467411,0.72817369467411) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Method<math alttext="{}_{\text{KV budget}}" class="ltx_Math" display="inline" id="A2.T6.m1" intent=":literal"><semantics><msub><mi></mi><mtext>KV budget</mtext></msub><annotation encoding="application/x-tex">{}_{\text{KV budget}}</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">HTML to TSV</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">Thought of Mind</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Travel Planning</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center">0.5k</td>
<td class="ltx_td ltx_align_center">2k</td>
<td class="ltx_td ltx_align_center">8k</td>
<td class="ltx_td ltx_align_center">0.5k</td>
<td class="ltx_td ltx_align_center">2k</td>
<td class="ltx_td ltx_align_center">8k</td>
<td class="ltx_td ltx_align_center">2k</td>
<td class="ltx_td ltx_align_center">8k</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
FullKV</th>
<td class="ltx_td ltx_align_center ltx_border_t">49.0</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">41.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">13.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">33.0</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">10.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t">37.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">9.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">26.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">7.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">H2O<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</th>
<td class="ltx_td ltx_align_center">28.3</td>
<td class="ltx_td ltx_align_center">6.4</td>
<td class="ltx_td ltx_align_center">0.4</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">38.0</span></td>
<td class="ltx_td ltx_align_center">7.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">R-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</th>
<td class="ltx_td ltx_align_center">38.0</td>
<td class="ltx_td ltx_align_center">7.1</td>
<td class="ltx_td ltx_align_center">0.5</td>
<td class="ltx_td ltx_align_center">26.0</td>
<td class="ltx_td ltx_align_center">7.5</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">58.2</span></td>
<td class="ltx_td ltx_align_center">36.0</td>
<td class="ltx_td ltx_align_center">12.5</td>
<td class="ltx_td ltx_align_center">32.5</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">10.5</span></td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t">1.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center">1.5</td>
<td class="ltx_td ltx_align_center">0.2</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">15.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">H2O<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center">0.4</td>
<td class="ltx_td ltx_align_center">0.8</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">7.6</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">R-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center">1.6</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">3.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">2048</span></sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb">34.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb">7.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb">17.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of Qwen3-4B across LongProc tasks: F1-score for HTML to TSV task and accuracies (%) for the remaining tasks. Best per task column in bold.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS1.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results on LongProc.</span> TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.T6" title="Table 6 â€£ B.1 Long Generation Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">6</span></a> reports KV-eviction results on long procedureâ€“generation tasks. Across tasks and budgets, <span class="ltx_text ltx_font_smallcaps">TRIM-KV</span> achieves the best performance, and it even surpasses the full-cache baseline on <span class="ltx_text ltx_font_smallcaps">Countdown</span> (0.5K/2K) and <span class="ltx_text ltx_font_smallcaps">HTML to TSV</span> (0.5K). Under tighter memory budgets, its margin over heuristic baselines widens.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Long-Context Evaluation</h3>
<figure class="ltx_table" id="A2.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:157.4pt;vertical-align:-76.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.9pt,24.9pt) scale(0.759466217722431,0.759466217722431) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method<math alttext="{}_{\text{KV budget}}" class="ltx_Math" display="inline" id="A2.T7.m1" intent=":literal"><semantics><msub><mi></mi><mtext>KV budget</mtext></msub><annotation encoding="application/x-tex">{}_{\text{KV budget}}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Multi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Knowledge</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SS-User</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SS-Pref</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SS-Assist</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Temporal</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span>gray!20
Full KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">131072</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">49.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">25.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">68.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">62.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">93.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">85.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">30.1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">27.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">15.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">50.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">32.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">56.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">33.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">15.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center">27.6</td>
<td class="ltx_td ltx_align_center">15.8</td>
<td class="ltx_td ltx_align_center">42.3</td>
<td class="ltx_td ltx_align_center">24.3</td>
<td class="ltx_td ltx_align_center">73.3</td>
<td class="ltx_td ltx_align_center">28.6</td>
<td class="ltx_td ltx_align_center">21.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">32768</span></sub>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">48.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">23.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">68.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">58.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">80.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">85.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">32.3</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">19.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">12.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">35.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">24.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">26.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">17.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">11.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center">18.2</td>
<td class="ltx_td ltx_align_center">9.0</td>
<td class="ltx_td ltx_align_center">25.6</td>
<td class="ltx_td ltx_align_center">17.1</td>
<td class="ltx_td ltx_align_center">70.0</td>
<td class="ltx_td ltx_align_center">12.5</td>
<td class="ltx_td ltx_align_center">14.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">16384</span></sub>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">42.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">21.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">62.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">42.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">80.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">69.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">31.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">12288</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">17.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">32.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">33.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">14.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">8.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">12288</span></sub>
</td>
<td class="ltx_td ltx_align_center">17.0</td>
<td class="ltx_td ltx_align_center">9.8</td>
<td class="ltx_td ltx_align_center">20.5</td>
<td class="ltx_td ltx_align_center">14.3</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">73.3</span></td>
<td class="ltx_td ltx_align_center">12.5</td>
<td class="ltx_td ltx_align_center">12.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">12288</span></sub>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">36.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">19.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">59.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">35.7</span></td>
<td class="ltx_td ltx_align_center">66.7</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">50.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">29.3</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">13.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">9.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">15.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">16.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">10.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">8.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</td>
<td class="ltx_td ltx_align_center">15.8</td>
<td class="ltx_td ltx_align_center">9.8</td>
<td class="ltx_td ltx_align_center">19.2</td>
<td class="ltx_td ltx_align_center">12.9</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">70.0</span></td>
<td class="ltx_td ltx_align_center">7.1</td>
<td class="ltx_td ltx_align_center">12.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">8192</span></sub>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">33.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">15.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">55.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">34.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">70.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">46.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">24.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">StreamingLLM<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">10.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">9.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">14.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">14.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">16.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">7.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">6.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SnapKV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center">13.8</td>
<td class="ltx_td ltx_align_center">11.3</td>
<td class="ltx_td ltx_align_center">14.1</td>
<td class="ltx_td ltx_align_center">14.3</td>
<td class="ltx_td ltx_align_center">56.7</td>
<td class="ltx_td ltx_align_center">7.1</td>
<td class="ltx_td ltx_align_center">9.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>blue!20
TRIM-KV<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">4096</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">30.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">12.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">35.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">24.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">80.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">46.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">29.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results on LongMemEval<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">S</span></sub>: overall and partial accuracies (%). </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Experimental settings.</span> We adopt Qwen3-4B-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">qwen3_4b_instruct</span>]</cite> as the base model, which supports a context window of up to 256K tokens. Retention gates are trained on a mixture of SynthLong-32KÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cerebras2025synthlong</span>]</cite>, BookSumÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kryscinski2021booksum</span>]</cite>, and BuddhiÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">singhal2024buddhi</span>]</cite>, covering sequence lengths from 32K to 128K tokens. We shuffle the combined corpus and train for 10,000 steps (i.e., 10,000 randomly sampled examples), with a maximum training sequence length of 128K and memory capacity <math alttext="M=4096" class="ltx_Math" display="inline" id="A2.SS2.p1.m1" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">M=4096</annotation></semantics></math>. All other settings follow SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1" title="5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Benchmark Dataset.</span> We evaluate chat-assistant capabilities under strict memory budgets using LongMemEval<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">S</span></sub>Â <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wu2024longmemeval</span>]</cite>. This subset provides contexts up to 123k tokens (measured with the Qwen3 tokenizer) and includes six question types that probe long-term memory: <span class="ltx_text ltx_font_italic">single-session-user</span> (SS-User) and <span class="ltx_text ltx_font_italic">single-session-assistant</span> (SS-Assist), which test recall of facts stated by the user or assistant within a session; <span class="ltx_text ltx_font_italic">single-session-preference</span> (SS-Pref), which requires personalized responses from stored personal information; <span class="ltx_text ltx_font_italic">multi-session</span> (Multi), which aggregates or compares information across sessions; <span class="ltx_text ltx_font_italic">knowledge-update</span> (Knowledge), which tracks and answers with the most recent, changed user information; <span class="ltx_text ltx_font_italic">temporal-reasoning</span> (Temporal), which reasons over timestamps and time references.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p3">
<p class="ltx_p">To evaluate KV-cache eviction methods on this benchmark, we follow the multi-turn, multi-session protocol ofÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024scbench</span>]</cite>. Specifically, before each query, the eviction-based model must compress the accumulated dialogue into a fixed-size, reusable KV cacheâ€”mirroring real-world assistants that maintain state across turns and sessions under strict memory budgets. We use Qwen3-4B-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">qwen3_4b_instruct</span>]</cite> to assess whether model outputs match the ground-truth responses.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results.</span> The results in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.T7" title="Table 7 â€£ B.2 Long-Context Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">7</span></a> show that our method outperforms baseline eviction strategies by a significant margin. Especially, TRIM-KV can match the performance of a full cache while using just <math alttext="25\%" class="ltx_Math" display="inline" id="A2.SS2.p4.m1" intent=":literal"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">25\%</annotation></semantics></math> of the KV budget.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection" style="--ltx-fg-color:#000000;">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Chunked-Prefill Evaluation</h3>
<div class="ltx_para ltx_noindent" id="A2.SS3.p1">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">In this section, we evaluate our method in the chunked-prefill settingÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2024locret</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;">, which enables long-context inference without exceeding memory limits. In this framework, long prompts are split into multiple chunks; the model computes the KV cache for each chunk sequentially and compresses the cache whenever it surpasses the memory budget. We compare our method against LocRetÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2024locret</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;">, a learnable KV eviction baseline that also assigns token-importance scores for eviction. Following the experimental setup of LocRet, we evaluate TRIM-KV on the LongBenchÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2024longbench</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;"> and LongBench-V2Â </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2025longbench</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;"> benchmarks. For LongBench-V2, we restrict evaluation to examples with context length below 128K tokens, matching the maximum context length advertised for Phi3-mini-128K. To train TRIM-KV, we use only LongAlpacaÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2023longlora</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;">, as in LocRet, to ensure that improvements are not attributable to differences in training data. In this setting, we set </span><math alttext="M=2048" class="ltx_Math" display="inline" id="A2.SS3.p1.m1" intent=":literal"><semantics><mrow><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">M</mi><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">=</mo><mn mathcolor="#000000" style="--ltx-fg-color:#000000;">2048</mn></mrow><annotation encoding="application/x-tex">M=2048</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#000000;">, and keep all other hyperparameters identical to those in SectionÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS2" style="--ltx-fg-color:#000000;" title="5.2 Long-Context Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.2</span></a><span class="ltx_text" style="--ltx-fg-color:#000000;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p2">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">For a fair comparison, we adopt the hyperparameters used by LocRet: the chunk size is set to 3072 and the budget size to 6000. We evaluate performance on Phi-3-mini-128KÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">abdin2024phi</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;">, reproducing Table 6 from the LocRet paper. Since the original LongBench experiments have not been released, we use the default chat template of Phi-3-mini-128K for all runs. TableÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.T8" style="--ltx-fg-color:#000000;" title="Table 8 â€£ B.3 Chunked-Prefill Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" style="--ltx-fg-color:#000000;"> andÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.T9" style="--ltx-fg-color:#000000;" title="Table 9 â€£ B.3 Chunked-Prefill Evaluation â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" style="--ltx-fg-color:#000000;"> report the results for LongBenchÂ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2024longbench</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;"> and LongBench-V2Â </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="--ltx-fg-color:#000000;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2025longbench</span><span class="ltx_text" style="--ltx-fg-color:#000000;">]</span></cite><span class="ltx_text" style="--ltx-fg-color:#000000;"> benchmarks, respectively. We observe discrepancies in the full-KV performance, likely due to differences in chat templates.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p3">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">Overall, TRIM-KV remains highly effective in the chunked-prefill setting. On LongBench, TRIM-KV nearly matches full-KV performance, whereas LocRet exhibits a 4.82% drop relative to full-KV cache inference. Notably, on a more challenging benchmark, LongBench-V2, we even surpass the performance of the full KV cache by 6.5%.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p4">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">Conceptually, LocRet is designed for chunk prefilling in long-context inference, while our method is general. The training paradigms also differ: LocRet predicts attention logits independently for each KV head, whereas we train retention gates end-to-end, allowing importance scores across heads to jointly optimize the eviction strategy. At inference time, LocRet further relies on a hand-crafted sliding window to preserve the most recent tokens from the latest chunk, and they show that removing this heuristic substantially degrades performance. In contrast, our method requires no such manually designed mechanism: sliding-window-like behavior emerges automatically from the learned policy when beneficial, and some heads (e.g., layer 15, head 2 in FigureÂ </span><a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.F4" style="--ltx-fg-color:#000000;" title="Figure 4 â€£ 5.1.1 Quantitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" style="--ltx-fg-color:#000000;">) do not exhibit sliding-window patterns at all.</span></p>
</div>
<figure class="ltx_table" id="A2.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:34.1pt;vertical-align:-15.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-250.3pt,19.7pt) scale(0.4641639384775,0.4641639384775) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Method</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">2wikimqa</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">gov_report</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">hotpotqa</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">lcc</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">multi_news</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">mfieldqa</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">musique</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">narrativeqa</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">pssg_count</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">pssg_retrv</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">qasper</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">qmsum</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">repobench-p</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">samsum</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">triviaqa</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt">
<math alttext="\Delta" class="ltx_Math" display="inline" id="A2.T8.m1" intent=":literal"><semantics><mi mathcolor="#000000" mathvariant="normal" style="--ltx-fg-color:#000000;">Î”</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#000000;"> (%)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">gray!10
Full KV*</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">33.37</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">33.35</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">43.06</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">51.86</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">26.57</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">49.82</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">19.82</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">18.21</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">2.97</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">93.50</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">41.07</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">19.51</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">58.02</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">23.15</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">86.38</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">â€“</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">gray!10
LocRet*</span>
</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">35.93</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">33.46</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">48.70</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">52.61</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">26.41</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">52.77</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">25.12</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">24.56</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">3.00</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">83.00</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">40.17</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">23.35</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">57.16</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">26.37</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">82.39</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">â€“</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">gray!20
Full KV</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">37.01</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">33.35</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">53.35</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">33.35</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">26.02</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">54.45</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">25.90</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">26.17</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">5.00</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">96.50</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">40.18</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">24.08</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">34.08</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">38.77</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">85.50</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">0.00</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-fg-color:#000000;">LocRet</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">37.24</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">32.80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">48.67</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">28.60</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">26.77</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">54.12</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">26.63</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">22.96</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">3.50</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">87.50</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">39.39</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">22.98</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">37.28</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">37.99</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">83.29</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">-4.82</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">blue!20
TRIM-KV</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="--ltx-fg-color:#000000;">36.65</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">33.37</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">54.78</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">33.08</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">26.02</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="--ltx-fg-color:#000000;">53.25</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="--ltx-fg-color:#000000;">25.39</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">25.00</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">5.00</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">94.50</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">40.17</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">23.59</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">37.46</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="--ltx-fg-color:#000000;">36.82</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">83.38</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">-0.64</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="--ltx-fg-color:#000000;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance on long-context tasks with Phi3-mini-128K on the LongBench benchmark, including average relative change (Avg. <math alttext="\Delta" class="ltx_Math" display="inline" id="A2.T8.m3" intent=":literal"><semantics><mi mathcolor="#000000" mathvariant="normal" style="--ltx-fg-color:#000000;">Î”</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>) compared to Full KV. * indicates that the numbers are reported fromÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2024locret</span>, Table 6]</cite>.</figcaption>
</figure>
<figure class="ltx_table" id="A2.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Method</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Acc. Short</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Acc. Medium</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Acc. Easy</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Acc. Hard</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="--ltx-fg-color:#000000;">Avg. Acc</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<math alttext="\Delta" class="ltx_Math" display="inline" id="A2.T9.m1" intent=":literal"><semantics><mi mathcolor="#000000" mathvariant="normal" style="--ltx-fg-color:#000000;">Î”</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#000000;"> (%)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">gray!20
Full KV</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">33.71</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="--ltx-fg-color:#000000;">18.60</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">34.44</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">25.86</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">28.79</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">0.00</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-fg-color:#000000;">LocRet</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">32.02</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">19.78</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed ltx_framed_underline" style="--ltx-fg-color:#000000;">26.67</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">28.74</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">28.03</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="--ltx-fg-color:#000000;">-2.64</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span><span class="ltx_text" style="--ltx-fg-color:#000000;">blue!20
TRIM-KV</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">35.39</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">20.93</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">34.44</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">28.74</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">30.68</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">+6.56</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="--ltx-fg-color:#000000;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Performance on long-context tasks of KV eviction methods with Phi3-mini-128K on the LongBench-V2 benchmark, including average relative change (Avg. <math alttext="\Delta" class="ltx_Math" display="inline" id="A2.T9.m3" intent=":literal"><semantics><mi mathcolor="#000000" mathvariant="normal" style="--ltx-fg-color:#000000;">Î”</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>) compared to Full KV.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Additional Ablation Studies</h3>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="A2.F8.g1" src="figs/ablation_training_dataset.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Generation Ablation.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Ablation on training datasets.</span> In SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1" title="5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we train the retention gates on a reasoning datasetâ€”OpenR1-MathÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">openr1math2025</span>]</cite>â€”and evaluate on AIME24, MATH-500, and GSM8K. This follows standard practice and matches the setting ofÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>]</cite>, ensuring a fair comparison. To assess cross-domain generalization, we instead train the gates on general long-context datasets (SynthLong, BookSum, Buddhi), similar to SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS2" title="5.2 Long-Context Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.2</span></a>, and then evaluate on math reasoning benchmarks to test whether retention scores learned from general data transfer to long chain-of-thought tasks. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F8" title="Figure 8 â€£ B.4 Additional Ablation Studies â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">8</span></a>, gates trained on general datasets generalize well and even surpass math-specific training at a 2048 KV budget. However, their performance degrades more quickly under tighter KV budgets. Overall, these results are promising and suggest that scaling the training of the retention gates by combining all datasets could yield further improvements.</p>
</div>
<figure class="ltx_figure" id="A2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="A2.F9.g1" src="figs/ablation_obj.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Ablating different objective components.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS4.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Ablation for the objective function.</span> We ablate the objective by training the Qwen3-4B retention gates with different loss combinations and report AIME24 pass@1 at a 4096-token KV budget in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F9" title="Figure 9 â€£ B.4 Additional Ablation Studies â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">9</span></a>. Here, we consider both forward KL divergence and reversed KL divergence for distillation loss. Generally, all distillation losses perform well on their own. However, reversed KL underperforms when compared to forward KL. Both show benefits in combination with next token prediction. The memory capacity loss is essential for compression, and removing it leads to a sharp drop.</p>
</div>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="A2.F10.g1" src="figs/ablation_gate_arch.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Ablating the retention gateâ€™s architecture.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS4.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Ablation for the retention gateâ€™s architecture.</span> We evaluate several retention-gate architectures and report the performance of Qwen3 1.7B on AIME24 in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F10" title="Figure 10 â€£ B.4 Additional Ablation Studies â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">10</span></a>. Due to computational constraints, this ablation uses greedy decoding. For the MLP gate, we use a single-hidden-layer MLP with width 512. We find that the MLP gate outperforms a simple linear projection, and that a large positive initial bias is crucial for stable training by keeping the gateâ€™s output nearly 1 at initialization to ensure minimal early forgetting.</p>
</div>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="A2.F11.g1" src="figs/ablation_m.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Ablating the training memory capacity <math alttext="M" class="ltx_Math" display="inline" id="A2.F11.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS4.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Ablation on training memory capacity <math alttext="M" class="ltx_Math" display="inline" id="A2.SS4.p4.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.</span> We evaluate multiple settings of <math alttext="M" class="ltx_Math" display="inline" id="A2.SS4.p4.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#A2.F11" title="Figure 11 â€£ B.4 Additional Ablation Studies â€£ Appendix B Additional Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">11</span></a>. With <math alttext="M=\infty" class="ltx_Math" display="inline" id="A2.SS4.p4.m3" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mi mathvariant="normal">âˆ</mi></mrow><annotation encoding="application/x-tex">M=\infty</annotation></semantics></math>, there is no capacity penalty, which hurts performance due to the absence of sparsity pressure. Setting <math alttext="M=128" class="ltx_Math" display="inline" id="A2.SS4.p4.m4" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">M=128</annotation></semantics></math> outperforms attention-guided heuristics but shows signs of over-optimizing for sparsity. In practice, we recommend choosing <math alttext="M" class="ltx_Math" display="inline" id="A2.SS4.p4.m5" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> to match the expected deployment-time memory budget.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Qualitative Results</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p"><span class="ltx_text" style="--ltx-fg-color:#000000;">In this section, we provide more qualitative results to illustrate the eviction decisions made by TRIM-KV. All visualizations are from the first example in the AIME24 dataset. Please refer to SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2512.03324v1#S5.SS1.SSS2" title="5.1.2 Qualitative Result â€£ 5.1 Long Generation Evaluation â€£ 5 Experiments â€£ Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"><span class="ltx_text ltx_ref_tag">5.1.2</span></a> for discussions.</span></p>
</div>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="698" id="A3.F12.g1" src="figs/beta_matrices.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A visualization of token retention matrices of Qwen3-4B when answering a math question in the AIME24 dataset. Each subplot is a token retention matrix <math alttext="\{\beta_{i}^{t-i}\}_{i}^{t}" class="ltx_Math" display="inline" id="A3.F12.m2" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î²</mi><mi>i</mi><mrow><mi>t</mi><mo>âˆ’</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">}</mo></mrow><mi>i</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">\{\beta_{i}^{t-i}\}_{i}^{t}</annotation></semantics></math> in a specific layer and head. <span class="ltx_text ltx_font_bold">Observations:</span> earlier layers often exhibit sliding-window-like patterns, while later layers develop clearer functional specializations.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="698" id="A3.F13.g1" src="figs/eviction_decisions.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A visualization of eviction decisions of Qwen3-4B when answering a math question in the AIME24 dataset. Each subplot is a matrix of eviction decisions <math alttext="\alpha_{ti}" class="ltx_Math" display="inline" id="A3.F13.m2" intent=":literal"><semantics><msub><mi>Î±</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{ti}</annotation></semantics></math> in a specific layer and head.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F14.g1" src="figs/example_l15_h2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Visualization of token retention scores <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="A3.F14.m2" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math> from the retention gate in <span class="ltx_text ltx_font_italic">layer 15, head 2</span> of the Qwen3-4B model. Brighter colors denote higher retention; gray indicates near-zero retention. <span class="ltx_text ltx_font_bold">Observation:</span> This head consistently assigns a high retention to the problem description and tokens expressing mathematical operations, suggesting a specialization for task-critical content. In contrast, filler phrases (e.g., â€˜waitâ€™, â€˜let me thinkâ€™) receive very low retention. Moreover, for a multi-digit number, retaining only the hidden state of the final digit suffices to maintain performance, suggesting that the last digitâ€™s representation already captures the semantic information of the whole number.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F15.g1" src="figs/example_l9_h7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Visualization of token retention scores <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="A3.F15.m3" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math> from the retention gate in <span class="ltx_text ltx_font_italic">layer 9, head 7</span> of the Qwen3-4B model. Brighter colors denote higher retention; gray indicates near-zero retention. <span class="ltx_text ltx_font_bold">Observations:</span> This head exhibits a retention pattern similar to <span class="ltx_text ltx_font_italic">layer 15, head 2</span>, but places greater emphasis on arithmetic operators (e.g., <math alttext="+,-,/" class="ltx_Math" display="inline" id="A3.F15.m4" intent=":literal"><semantics><mrow><mo rspace="0em">+</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">âˆ’</mo><mo rspace="0em">,</mo><mo lspace="0em">/</mo></mrow><annotation encoding="application/x-tex">+,-,/</annotation></semantics></math>).</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F16.g1" src="figs/example_l35_h2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Visualization of token retention scores <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="A3.F16.m2" intent=":literal"><semantics><msub><mi>Î²</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_{i}</annotation></semantics></math> from the retention gate in <span class="ltx_text ltx_font_italic">layer 35, head 2</span> of the Qwen3-4B model. Brighter colors denote higher retention; gray indicates near-zero retention. <span class="ltx_text ltx_font_bold">Observation:</span> Unlike <span class="ltx_text ltx_font_italic">layer 15, head 2</span> and <span class="ltx_text ltx_font_italic">layer 9, head 7</span>, this head assigns elevated retention to general-purpose tokens that support coherence and instruction following; for example, <span class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span> commands and the directive <span class="ltx_text ltx_font_italic">boxed</span>{) receive high scores, while tokens associated with mathematical operations receive low retention.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F17.g1" src="figs/topk_l0_h3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 0 head 3</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F18.g1" src="figs/topk_l0_h6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 0 head 6</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F19.g1" src="figs/topk_l9_h7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 9 head 7</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F20.g1" src="figs/topk_l15_h2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 15 head 2</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F21.g1" src="figs/topk_l16_h6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 16 head 6</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="381" id="A3.F22.g1" src="figs/topk_l30_h4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 30 head 4</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens. <span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#000000;">Observations:<span class="ltx_text ltx_font_medium"> This head mostly keeps period tokens. This suggests that this head may implicitly perform gist tokens that summarize information from the previous sentence. This contrasts with recent trends that advocate for saving a chunk of tokensÂ <cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yuan2025native</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">gao2025seerattention</span>]</cite>. Our results suggest that it can be more budget-efficient to save individual tokens because they already capture contextual information.</span></span>
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F23.g1" src="figs/topk_l34_h1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 34 head 1</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F24"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F24.g1" src="figs/topk_l35_h3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 35 head 3</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F25"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F25.g1" src="figs/topk_l35_h5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 35 head 5</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F26"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F26.g1" src="figs/topk_l35_h7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Visualization of tokens retained in the <span class="ltx_text ltx_font_italic">layer 35 head 7</span> of the KV cache after generation, where the KV budget is 256. The model is Qwen3-4B. Bold blue indicates tokens retained in the KV cache, where gray indicates evicted tokens.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Dec  3 00:27:24 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
