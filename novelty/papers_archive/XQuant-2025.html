<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</title>
<!--Generated on Thu Aug 14 06:53:02 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2508.10395v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S1" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2.SS1" title="In 2 Related Work ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Memory Wall for LLM Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2.SS2" title="In 2 Related Work ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Activation Rematerialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2.SS3" title="In 2 Related Work ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>KV Cache Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2.SS4" title="In 2 Related Work ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Low-Rank Decomposition for KV Cache and KV Rematerialization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS1" title="In 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_smallcaps">XQuant</span>: Quantizing <span class="ltx_text ltx_font_italic">X</span> Instead of KV</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS2" title="In 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>: Leveraging Cross-Layer Similarity in <span class="ltx_text ltx_font_italic">X</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3" title="In 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Support for Grouped-Query Attention Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3.SSS1" title="In 3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Extending <span class="ltx_text ltx_font_smallcaps">XQuant</span> to support GQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3.SSS2" title="In 3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Extending <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> to support GQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS4" title="In 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>System-Level Analysis of Rematerialization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Empirical Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="In 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS2" title="In 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Downstream Task Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="In 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results with Cross-Layer Compression Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S5" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A1" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prefill for <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2" title="In XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Observed Outlier Property When Applying <span class="ltx_text ltx_font_smallcaps">XQuant</span> for GQA Models</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.SS0.SSS0.Px1" title="In Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title">Recap</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.SS0.SSS0.Px2" title="In Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title">Observed Outlier Property</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.SS0.SSS0.Px3" title="In Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_title">Connection to KV cache quantization methods</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined">\useunder</span>
<p class="ltx_p"><span class="ltx_text ltx_ulem_uline"></span><span class="ltx_text ltx_framed ltx_framed_underline"></span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps">XQuant</span>: Breaking the Memory Wall for 
<br class="ltx_break"/>LLM Inference with KV Cache Rematerialization
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Aditya Tomar <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>  Coleman Hooper<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>  Minjae Lee<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>  Haocheng Xi<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"/>Rishabh Tiwari<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>   Wonjun Kang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>   Luca Manolache<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"/>Michael W. Mahoney<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,3,4</span></sup>  Kurt Keutzer<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>  Amir Gholami<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,3</span></sup>
<br class="ltx_break"/><sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup> UC Berkeley  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup> FuriosaAI  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">3</span></sup> ICSI  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">4</span></sup> LBNL
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Equal Contribution</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements.
In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference.
As such, new algorithms are emerging that trade increased computation for reduced memory operations.
To that end, we present <span class="ltx_text ltx_font_smallcaps">XQuant</span>, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations <span class="ltx_text ltx_font_italic">X</span>, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference.
This results in an immediate 2<math alttext="\times" class="ltx_Math" display="inline" id="m15"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings compared to KV caching.
By applying <span class="ltx_text ltx_font_smallcaps">XQuant</span>, we achieve up to <math alttext="\sim 7.7\times" class="ltx_math_unparsed" display="inline" id="m16"><semantics><mrow><mo>∼</mo><mn>7.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\sim 7.7\times</annotation></semantics></math> memory savings with <math alttext="&lt;0.1" class="ltx_Math" display="inline" id="m17"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">&lt;0.1</annotation></semantics></math> perplexity degradation compared to the FP16 baseline.
Furthermore, our approach leverages the fact that <span class="ltx_text ltx_font_italic">X</span> values are similar across layers.
Building on this observation, we introduce <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, which exploits the cross-layer similarity in the <span class="ltx_text ltx_font_italic">X</span> embeddings for extreme compression.
Across different models, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> attains up to 10<math alttext="\times" class="ltx_Math" display="inline" id="m18"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5<math alttext="\times" class="ltx_Math" display="inline" id="m19"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings with only <math alttext="0.1" class="ltx_Math" display="inline" id="m20"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> perplexity degradation.
Notably, despite using standard uniform quantization, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> is able to surpass intricate KV cache quantization methods that employ non-uniform quantization with outlier-aware strategies.
Given the aforementioned trends in compute versus memory scaling for future generations of hardware platforms, <span class="ltx_text ltx_font_smallcaps">XQuant</span> adopts a forward-looking perspective to accelerate LLM inference: <span class="ltx_text ltx_font_smallcaps">XQuant</span> seeks to exploit the rapidly increasing compute capabilities to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="488" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">Perplexity degradation (lower is better) versus memory compression factor (higher is better) evaluated using Llama-2-7B on WikiText-2 for state-of-the-art KV cache quantization methods and for our <span class="ltx_text ltx_font_smallcaps">XQuant</span>, across {4,3,2}-bit widths. The top right edge of the plot represents the optimal configuration that attains the most memory compression and the least perplexity degradation. Memory compression factor and perplexity degradation are with respect to the FP16 baseline. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T4" title="Table 4 ‣ 4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4</span></a>, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> achieves only 0.01 perplexity degradation while getting <math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S1.F1.m3"><semantics><mrow><mn>10</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math> memory savings with 3-bit quantization, and 0.1 perplexity degradation while getting <math alttext="12.5\times" class="ltx_math_unparsed" display="inline" id="S1.F1.m4"><semantics><mrow><mn>12.5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">12.5\times</annotation></semantics></math> memory compression with 2-bit quantization.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">
A visualization of how <span class="ltx_text ltx_font_smallcaps">XQuant</span> reduces the memory footprint by caching the input embedding (<span class="ltx_text ltx_font_italic">X</span>) instead of the KV cache.
We use the cached input to rematerialize the Keys and Values in order to compute attention.
This increases the amount of computation required when computing attention. However, since LLM inference is typically memory bandwidth-bound, we can accelerate inference by reducing memory operations, even at the expense of additional compute operations.
</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Large Language Models (LLMs) have seen widespread adoption as a standard paradigm across a range of Natural Language Processing (NLP) applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib34" title="">34</a>]</cite>.
While LLMs achieve remarkable performance on these tasks, they have substantial inference costs due to their large parameter count as well as the number of memory operations required when running generation.
Prior work has demonstrated how LLM inference tends to be <span class="ltx_text ltx_font_italic">Memory Bandwidth-Bound</span>, rather than Compute-Bound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib27" title="">27</a>]</cite>, and therefore reducing the memory footprint of LLMs is critical to enable downstream applications.
For short context lengths and small batch sizes, the model weights are typically the memory bottleneck.
However, for long context lengths and large batch sizes, the main memory bottleneck for LLM inference is the Key-Value (KV) cache, which is the embedded representation of the entire sequence used in the self-attention mechanism and which grows linearly with respect to the sequence length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib27" title="">27</a>]</cite>.
During inference, generating each new token requires repeatedly loading and storing the entire KV cache, which becomes prohibitively expensive and leads to substantial slowdown.
This motivates efforts to reduce KV cache memory operations to speed up the inference process.
One promising solution to compress the KV cache is through KV cache quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite>.
Quantizing the KV cache reduces the memory footprint and number of memory operations required during decoding by using fewer bits to represent the Keys and Values.
However, while existing methods retain accuracy even when quantizing the KV cache to low precision (e.g., 4-bit quantization), further reducing the bit-width of KV activations often degrades model performance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">In this work, we present <span class="ltx_text ltx_font_smallcaps">XQuant</span>, a method which quantizes the input activations <span class="ltx_text ltx_font_italic">X</span> of each layer, rather than the KV cache, to reduce the required memory consumption.
Our method is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">2</span></a>.
Quantizing <span class="ltx_text ltx_font_italic">X</span> provides a 2<math alttext="\times" class="ltx_Math" display="inline" id="S1.p2.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings compared with quantizing the KV cache, since we only need to store one tensor per layer instead of separate Keys and Values. Interestingly, we also find that <span class="ltx_text ltx_font_italic">X</span> is more amenable to extremely low-bit quantization than the KV cache. Moreover, although rematerializing KV cache from <math alttext="X" class="ltx_Math" display="inline" id="S1.p2.m2"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> requires additional computation during decoding, we can afford this cost because LLM inference is progressively becoming more memory-bandwidth bound.
This phenomenon will be increasingly prevalent with future hardware platforms, as the rate of improvement in compute capabilities continue to outpace increases in memory bandwidth and capacity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib11" title="">11</a>]</cite>.
In this work, we make the following contributions (which are summarized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
</div>
<div class="ltx_para" id="S1.p3">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">To reduce the memory consumption for LLM inference, <span class="ltx_text ltx_font_smallcaps">XQuant</span> quantizes the input <span class="ltx_text ltx_font_italic">X</span> activations, providing a 2<math alttext="\times" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings relative to quantizing the KV cache directly, and then rematerializes the KV activations on-the-fly during inference (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS1" title="3.1 XQuant: Quantizing X Instead of KV ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
We show that for the same memory footprint (<math alttext="\sim 7.7\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i1.p1.m2"><semantics><mrow><mo>∼</mo><mn>7.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\sim 7.7\times</annotation></semantics></math> savings compared to FP16), <span class="ltx_text ltx_font_smallcaps">XQuant</span> attains up to <math alttext="\sim 0.9" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m3"><semantics><mrow><mi></mi><mo>∼</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\sim 0.9</annotation></semantics></math> less perplexity degradation compared to KV cache quantization and <math alttext="&lt;0.1" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m4"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">&lt;0.1</annotation></semantics></math> perplexity degradation compared to the FP16 baseline (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">For ultra-low precision quantization with accuracy comparable to the FP16 baseline, we present <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, a method that exploits the cross-layer similarity in the <span class="ltx_text ltx_font_italic">X</span> embeddings. Our approach compresses the differences in <span class="ltx_text ltx_font_italic">X</span> between successive layers, which have much smaller range as a result of the residual stream in the Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib36" title="">36</a>]</cite> architecture (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS2" title="3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Using standard asymmetric uniform quantization, we observe only <math alttext="0.01" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math> perplexity degradation with 3-bit quantization while attaining <math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i2.p1.m2"><semantics><mrow><mn>10</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math> memory savings compared to the FP16 baseline, and only <math alttext="0.1" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m3"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> perplexity degradation with 2-bit quantization and <math alttext="12.5\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i2.p1.m4"><semantics><mrow><mn>12.5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">12.5\times</annotation></semantics></math> memory savings relative to FP16 (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
Remarkably, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> outperforms state-of-the-art KV cache quantization methods like KVQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> that use complex techniques such as non-uniform quantization and outlier-aware dense-and-sparse strategies.
On the Wikitext-2 and C4 datasets, we reduce perplexity degradation by <math alttext="\sim 0.4" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m5"><semantics><mrow><mi></mi><mo>∼</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">\sim 0.4</annotation></semantics></math> compared to KVQuant while using <math alttext="1.9\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i2.p1.m6"><semantics><mrow><mn>1.9</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.9\times</annotation></semantics></math> less memory (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">We extend <span class="ltx_text ltx_font_smallcaps">XQuant</span> and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> to support new models that use Grouped Query Attention (GQA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib1" title="">1</a>]</cite>.
For GQA models, we decompose the Key and Value projection weight matrices offline using the singular value decomposition (SVD), and we then down-project the input activations into a smaller latent space, reducing memory consumption. Interestingly, we observe that the <span class="ltx_text ltx_font_italic">X</span> distribution in this latent space is suitable for extremely low precision quantization (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3" title="3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
For 2-bit quantization, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves less than 2.2 perplexity degradation relative to KV cache quantization (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
<span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> further improves performance by resulting in only <math alttext="\sim 0.1" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m1"><semantics><mrow><mi></mi><mo>∼</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\sim 0.1</annotation></semantics></math> perplexity degradation compared to the FP16 baseline while saving <math alttext="6.7\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i3.p1.m2"><semantics><mrow><mn>6.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">6.7\times</annotation></semantics></math> memory (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p">Finally, we provide a system-level analysis of computational overheads and memory savings from rematerialization. We show that <span class="ltx_text ltx_font_smallcaps">XQuant</span> is able to significantly reduce memory consumption while preserving accuracy, which will ultimately result in speedup even at the cost of additional computation, as compute continues to dominate memory capacity and bandwidth (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS4" title="3.4 System-Level Analysis of Rematerialization ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Memory Wall for LLM Inference</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">When assessing the performance of kernels on a target hardware platform, their runtime is determined by either 1) the amount of compute operations that need to be performed or 2) the amount of memory operations that need to be performed.
Which of these two factors is the bottleneck depends on the characteristics of the target hardware as well as those of the kernel.
<span class="ltx_text ltx_font_italic">Arithmetic Intensity</span>, which is defined as the ratio of the number of compute operations that need to be performed per byte transferred to or from memory, is a typical metric to evaluate a kernel and is usually expressed in units of floating point operations (FLOPs) per byte:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Arithmetic\ Intensity}=\frac{\mathrm{Compute\ Operations\ Performed}}{\mathrm{Bytes\ Transfered}}," class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mrow><mi>Arithmetic</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Intensity</mi></mrow><mo>=</mo><mfrac><mrow><mi>Compute</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Operations</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Performed</mi></mrow><mrow><mi>Bytes</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Transfered</mi></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{Arithmetic\ Intensity}=\frac{\mathrm{Compute\ Operations\ Performed}}{\mathrm{Bytes\ Transfered}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We can characterize the target hardware platform in terms of the ratio of the peak compute performance of the device versus the amount of memory bandwidth that it provides.
The ratio of peak compute to memory bandwidth, typically also expressed as FLOPs per byte similar to arithmetic intensity, is referred to here as the <span class="ltx_text ltx_font_italic">ridge point</span> for that hardware platform (we use this term as it aligns with the point on a Roofline plot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib37" title="">37</a>]</cite> which delineates compute-bound versus memory-bandwidth bound kernels for a target hardware platform):</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Ridge\ Point}=\frac{\mathrm{Peak\ Compute\ Throughput}}{\mathrm{Peak\ Memory\ Bandwidth}}." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mrow><mi>Ridge</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Point</mi></mrow><mo>=</mo><mfrac><mrow><mi>Peak</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Compute</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Throughput</mi></mrow><mrow><mi>Peak</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Memory</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>Bandwidth</mi></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{Ridge\ Point}=\frac{\mathrm{Peak\ Compute\ Throughput}}{\mathrm{Peak\ Memory\ Bandwidth}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">When assessing whether a provided kernel is memory bandwidth-bound or compute-bound, we need only compare the arithmetic intensity of the kernel with the ridge point of our target hardware platform.
If the arithmetic intensity is larger, then the kernel is compute-bound when run on the target hardware; if the arithmetic intensity is smaller, then the kernel will be memory-bandwidth bound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">When considering LLM inference, a key challenge is that it has low arithmetic intensity for most batch sizes and context length regimes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib33" title="">33</a>]</cite>.
This is because generating each token only requires performing low intensity matrix-vector multiplications, whereas loading these matrices requires far more memory operations.
Thus, LLM inference performs very few floating point operations per byte loaded from memory.
Additionally, when we assess scaling trends with LLMs and hardware compute and memory, there is a growing discrepancy between the memory capabilities of existing devices and an LLM’s demands.
This phenomenon has been termed the Memory Wall problem
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib11" title="">11</a>]</cite>.
There are two key components to this problem:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p">The scaling trends in terms of memory requirements for modern LLMs have dramatically outpaced both the increases in memory capacity and bandwidth on hardware.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">The scaling in peak computational performance is orders of magnitude greater than the corresponding increases in memory capacity and bandwidth on hardware.</p>
</div>
</li>
</ol>
<p class="ltx_p">Taking these factors together, it is critical to reduce the number of memory operations required for LLM inference.
If we can additionally reduce the memory requirements by increasing the amount of computation, this will be beneficial, due to the growing gap
between peak computational performance and memory capabilities of hardware platforms.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="108.97" id="S2.SS1.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 108.97" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,108.97) matrix(1 0 0 -1 0 0)"><g fill="#000099" fill-opacity="1.0"><path d="M 0 0 L 0 108.97 L 600 108.97 L 600 0 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 87.7 L 598.03 87.7 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="9.46" overflow="visible" style="--fo_width :40.23em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">Motivation</span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="62.11" overflow="visible" style="--fo_width :40.23em;--fo_height:4.29em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 59.42)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">While increasing computation in exchange for reduced memory usage may introduce latency on today’s hardware, this tradeoff is expected to become increasingly favorable. Our work leverages this trend, aiming to reduce memory bottlenecks by utilizing additional compute, ultimately enabling faster LLM inference on future hardware generations.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Activation Rematerialization</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">Prior work has explored rematerializing activations, where activations are recomputed on-the-fly from a smaller checkpointed state, as a partial solution to the memory wall problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib16" title="">16</a>]</cite>.
Checkmate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib16" title="">16</a>]</cite> allowed training large DNNs in memory-constrained GPUs by retaining a subset of intermediate activations as checkpoints, with other activations being discarded and then rematerialized from these checkpointed states.
Checkmate solves for the optimal recomputation setup for the target hardware platform with provided memory constraints.
Recent work has also explored rematerializing the KV cache from the input embedding <span class="ltx_text ltx_font_italic">X</span> in the context of serving systems which offload some of the KV entries to the CPU.
HCache <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib10" title="">10</a>]</cite> offloads <span class="ltx_text ltx_font_italic">X</span> to CPU memory, and performs restoration by moving <span class="ltx_text ltx_font_italic">X</span> from CPU to GPU and then recomputing to recover the Keys and Values. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib21" title="">21</a>]</cite> also performs rematerialization to recover the Keys and Values from <span class="ltx_text ltx_font_italic">X</span>, and determines the optimal amount of rematerialization to perform, given a combination of compute and memory constraints.
While these works focus on system-level optimizations to determine the amount of rematerialization that can be performed, our work focuses on compressing <span class="ltx_text ltx_font_italic">X</span> rather than KV cache activations to attain improved savings relative to existing KV cache compression methods, and then exploits cross-layer similarity in <span class="ltx_text ltx_font_italic">X</span> to attain greater memory savings.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">There has also been prior work on trying to avoid storing Keys and Values separately for the KV cache.
El-Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib42" title="">42</a>]</cite> merged the Key and Value projection matrices into other matrices in the model, thereby allowing for computing attention directly using the input embedding.
However, this method is not compatible with the rotary positional embedding (RoPE) encodings or with models that use Grouped Query Attention.
Slim-Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib12" title="">12</a>]</cite> similarly aimed to only cache Keys and to multiply them by the inverse Key projection matrix to recover the values (and to merge this inverse matrix into other matrices in the model offline).
This approach also requires applying RoPE on-the-fly during inference and is not compatible with models that use grouped-query attention.
Moreover, such inverses are not guaranteed to be numerically stable.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>KV Cache Quantization</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">KV cache quantization has emerged as a promising method for reducing the memory requirements for the KV cache by using fewer bits to represent each floating point element in each KV cache entry.
Previous work has quantized the Key distributions per-channel and the Value distributions per-token in order to adapt to the outlier channels in Keys <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite>.
A crucial aspect of Key cache quantization is handling RoPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib30" title="">30</a>]</cite>.
Prior methods have either applied pre-RoPE Key quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> or quantized the Key cache using polar-form representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib13" title="">13</a>]</cite>, in order to retain accuracy.
Mixed-precision KV cache quantization has been used in order to preserve model accuracy by retaining particularly sensitive tokens in higher precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib14" title="">14</a>]</cite>.
Prior work has also explored retaining initial pivot tokens intact <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib22" title="">22</a>]</cite>; this builds on prior work that identified initial tokens as “attention sink,” tokens which are disproportionately important for preserving model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib39" title="">39</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Low-Rank Decomposition for KV Cache and KV Rematerialization</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p">There has also been prior work which has applied a low-rank decomposition to the KV cache or the corresponding projection matrices, in order to cache KV entries with a reduced latent dimension before rematerializing the original KV entries.
xKV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib4" title="">4</a>]</cite> exploits the fact that the singular vectors for the KV cache entries for successive layers are well aligned to group the KV cache entries across layers and apply SVD to the concatenated KV caches. Loki <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib29" title="">29</a>]</cite> finds that the keys are low-rank and performs attention in a lower dimension to identify the most important keys, and then only loads those keys for full-dimensional sparse attention. While Loki only reduces memory operations, Eigen Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib28" title="">28</a>]</cite> actually compresses the KV cache and only does attention in low-rank space.
There have also been multiple works which have applied low-rank decomposition to the weights in order to project the KV cache to a lower dimension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib41" title="">41</a>]</cite>.
LoRC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib47" title="">47</a>]</cite> applies SVD to the weight matrices and keeps more singular values at earlier layers to minimize error amplification.
Palu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib5" title="">5</a>]</cite> performs a low-rank decomposition on the weights offline ahead of inference, and then caches the intermediate KV cache entries.
ReCalKV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib41" title="">41</a>]</cite> reorders heads before applying SVD to groups of heads in order to retain accuracy and reduce rematerialization overhead.
In contrast with these works that aim to compress the KV cache using a low-rank decomposition, <span class="ltx_text ltx_font_smallcaps">XQuant</span> aims to quantize the <span class="ltx_text ltx_font_italic">X</span> embeddings in order to get a 2<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS4.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory savings without applying a low-rank decomposition.
<span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> also exploits the cross-layer similarity in the <span class="ltx_text ltx_font_italic">X</span> distributions by compressing the differences across layers, which provides substantial memory reduction for the same accuracy relative to compressing the KV cache directly.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">In this section, we introduce our algorithms <span class="ltx_text ltx_font_smallcaps">XQuant</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS1" title="3.1 XQuant: Quantizing X Instead of KV ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> (Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS2" title="3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and we discuss them in the context of Multi-Head Attention (MHA) models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib36" title="">36</a>]</cite>. Then in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3" title="3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we discuss how we extend our algorithm to support Grouped Query Attention (GQA) models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib1" title="">1</a>]</cite>. Note that we specifically address GQA because of its widespread popularity and adoption as an optimization technique for KV cache reduction among many model families such as Llama, Mistral, Gemma, Qwen, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib43" title="">43</a>]</cite>. Lastly, in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS4" title="3.4 System-Level Analysis of Rematerialization ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we discuss the tradeoffs between compute and memory operations that our methods make.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_smallcaps">XQuant</span>: Quantizing <span class="ltx_text ltx_font_italic">X</span> Instead of KV</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">The core idea of <span class="ltx_text ltx_font_smallcaps">XQuant</span> is to reduce the memory requirements of KV caching by checkpointing the input activations and regenerating the Keys and Values from these smaller checkpoints when they need to be used to compute attention.
This is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">2</span></a>, where we quantize and cache the input embedding <span class="ltx_text ltx_font_italic">X</span>, which requires 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> less memory than using standard KV caching.
The drawback with caching <span class="ltx_text ltx_font_italic">X</span> is that it requires rematerializing the KV cache on-the-fly; this requires multiplying the input embedding by projection matrices <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> and <math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.SS1.p1.m3"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math>.
However, as outlined in Sections <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S2.SS1" title="2.1 Memory Wall for LLM Inference ‣ 2 Related Work ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">2.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS4" title="3.4 System-Level Analysis of Rematerialization ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.4</span></a>, since the arithmetic intensity is typically low for attention in LLM decoding, we can afford to perform additional computation for rematerialization in order to reduce the number of memory operations required.
Note that whenever we refer to <span class="ltx_text ltx_font_italic">X</span>, we mean the input activations after layer normalization has been applied to them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>: Leveraging Cross-Layer Similarity in <span class="ltx_text ltx_font_italic">X</span>
</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" style="font-size:90%;">
Comparison of the post-norm input embeddings <span class="ltx_text ltx_font_italic">X</span>, pre-RoPE Keys, and Values for successive layers in the Llama-3.1-8B model.
The distributions were collected using a test sample with 2K sequence length from Wikitext-2.
Although the Keys and Values exhibit distinct differences across successive layers, the <span class="ltx_text ltx_font_italic">X</span> embeddings bear remarkable similarity. We exploit this similarity using cross-layer compression in <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.F3" title="Figure 3 ‣ 3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3</span></a>, the <span class="ltx_text ltx_font_italic">X</span> embeddings across successive layers are remarkably similar (when compared with the similarities of KV cache embeddings across layers).
This observed property can be attributed to the residual stream which flows from each layer’s input to the outputs of the attention and multi-layer perceptron blocks within that layer, where these outputs
are added to the residual stream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib36" title="">36</a>]</cite>.
In this way, each layer’s function can be understood as simply refining its input, and since this refinement occurs gradually, it is intuitive
that the inputs of successive layers are not substantially dissimilar <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib17" title="">17</a>]</cite>.
This represents a promising opportunity for further compression if we can exploit cross-layer similarity.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">To exploit the cross-layer similarity in <span class="ltx_text ltx_font_italic">X</span>, we propose <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, which compresses the <span class="ltx_text ltx_font_italic">differences</span> between <span class="ltx_text ltx_font_italic">X</span> for successive layers. Our algorithm during prefill is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A1.F1" title="Figure A.1 ‣ Appendix A Prefill for XQuant-CL ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">A.1</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A1" title="Appendix A Prefill for XQuant-CL ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">A</span></a>, and our algorithm for decoding is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.F4" title="Figure 4 ‣ 3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4</span></a>.
Instead of directly quantizing <span class="ltx_text ltx_font_italic">X</span> for layer <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.m1"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, we instead quantize <math alttext="\Delta X_{i}=X_{i}-X_{i-1}" class="ltx_Math" display="inline" id="S3.SS2.p2.m2"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>−</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\Delta X_{i}=X_{i}-X_{i-1}</annotation></semantics></math>, which has a much smaller range and hence is much easier to quantize.
For this method, we leave the first layer <math alttext="X_{0}" class="ltx_Math" display="inline" id="S3.SS2.p2.m3"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math> in higher precision, and then compute the differences relative to <math alttext="X_{0}" class="ltx_Math" display="inline" id="S3.SS2.p2.m4"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math>.
However, if we directly compute <math alttext="X_{i}-X_{0}" class="ltx_Math" display="inline" id="S3.SS2.p2.m5"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>−</mo><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_{i}-X_{0}</annotation></semantics></math> for a layer <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.m6"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> which is far from layer <math alttext="0" class="ltx_Math" display="inline" id="S3.SS2.p2.m7"><mn>0</mn></math>, <math alttext="X_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m8"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_{i}</annotation></semantics></math> would have accumulated enough changes such that it has diverged substantially from <math alttext="X_{0}" class="ltx_Math" display="inline" id="S3.SS2.p2.m9"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math>, meaning that this delta is no longer easy to quantize.
To address this, we quantize and cache <math alttext="\Delta\hat{X}_{i}=Q(X_{i}-\hat{X}_{i-1})" class="ltx_Math" display="inline" id="S3.SS2.p2.m10"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mo>=</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta\hat{X}_{i}=Q(X_{i}-\hat{X}_{i-1})</annotation></semantics></math>, where <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p2.m11"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is the quantization function and <math alttext="\hat{X}_{i-1}" class="ltx_Math" display="inline" id="S3.SS2.p2.m12"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{i-1}</annotation></semantics></math> is the cross-layer approximation for the previous layer’s <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.p2.m13"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.
We then approximate <math alttext="X_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m14"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_{i}</annotation></semantics></math> as <math alttext="\hat{X}_{i}=X_{0}+\sum_{j=1}^{i}\Delta\hat{X}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p2.m15"><semantics><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>X</mi><mn>0</mn></msub><mo rspace="0.055em">+</mo><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>j</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{X}_{i}=X_{0}+\sum_{j=1}^{i}\Delta\hat{X}_{j}</annotation></semantics></math>.
This sequential delta summation allows for the explicit accumulation of the refinements that each layer applies to its input, allowing us to exploit the easy quantization properties of these individual deltas for extreme compression. However with the above formulation, computing <math alttext="\hat{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.m16"><semantics><mover accent="true"><mi>X</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{X}</annotation></semantics></math> for layer <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.m17"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> requires loading all <math alttext="i-1" class="ltx_Math" display="inline" id="S3.SS2.p2.m18"><semantics><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i-1</annotation></semantics></math> previous deltas, which is very expensive. Thus, we maintain an accumulator which sums the deltas of all previous layers (as highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.F4" title="Figure 4 ‣ 3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4</span></a>). This way, computing <math alttext="\hat{X}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m19"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{X}_{i}</annotation></semantics></math> only requires loading the accumulator (<math alttext="\hat{X}_{i-1}=X_{0}+\sum_{j=1}^{i-1}\Delta\hat{X}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p2.m20"><semantics><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>X</mi><mn>0</mn></msub><mo rspace="0.055em">+</mo><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>j</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{X}_{i-1}=X_{0}+\sum_{j=1}^{i-1}\Delta\hat{X}_{j}</annotation></semantics></math>) and a single delta <math alttext="\Delta\hat{X}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m21"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta\hat{X}_{i}</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" style="font-size:90%;">
Illustration of <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> algorithm during decoding. Besides Layer 0, the input to all other layers is a cross layer approximation, computed using the deltas of all previous layers and the input of Layer 0. The input of Layer 0 is summed with each layer’s delta so it can be treated as an accumulator, allowing us to avoid loading all <math alttext="N-1" class="ltx_Math" display="inline" id="S3.F4.m6"><semantics><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math> deltas to compute Layer <math alttext="N" class="ltx_Math" display="inline" id="S3.F4.m7"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>’s <math alttext="X" class="ltx_Math" display="inline" id="S3.F4.m8"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. After a layer is done processing, the input embedding to the layer for the last token is subtracted from the output activations of the layer (the same shape as a single token), and this delta is quantized and appended to the <math alttext="\Delta\hat{X}" class="ltx_Math" display="inline" id="S3.F4.m9"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>X</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\Delta\hat{X}</annotation></semantics></math> cache. <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> during prefill is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A1.F1" title="Figure A.1 ‣ Appendix A Prefill for XQuant-CL ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">A.1</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A1" title="Appendix A Prefill for XQuant-CL ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">A</span></a>, which shows how the full <math alttext="\Delta\hat{X}" class="ltx_Math" display="inline" id="S3.F4.m10"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>X</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\Delta\hat{X}</annotation></semantics></math> is computed and cached for each layer.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Support for Grouped-Query Attention Models</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" style="font-size:90%;">
A diagram outlining how we apply <span class="ltx_text ltx_font_smallcaps">XQuant</span> for GQA-based models.
GQA down-projects the input embedding (<span class="ltx_text ltx_font_italic">X</span>) to a smaller <math alttext="d/g" class="ltx_Math" display="inline" id="S3.F5.m17"><semantics><mrow><mi>d</mi><mo>/</mo><mi>g</mi></mrow><annotation encoding="application/x-tex">d/g</annotation></semantics></math> dimension when computing the Keys and Values. Hence, if we naively quantize the input <span class="ltx_text ltx_font_italic">X</span> rather than the KV cache, this will potentially have greater memory consumption.
To address this, we first apply SVD to the <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.F5.m18"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> and <math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.F5.m19"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math> matrices offline.
Online during prefill, we down-project <math alttext="X" class="ltx_Math" display="inline" id="S3.F5.m20"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> by <math alttext="U_{k}" class="ltx_Math" display="inline" id="S3.F5.m21"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> to get <math alttext="X_{k}" class="ltx_Math" display="inline" id="S3.F5.m22"><semantics><msub><mi>X</mi><mi>k</mi></msub><annotation encoding="application/x-tex">X_{k}</annotation></semantics></math> and by <math alttext="U_{v}" class="ltx_Math" display="inline" id="S3.F5.m23"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> to get <math alttext="X_{v}" class="ltx_Math" display="inline" id="S3.F5.m24"><semantics><msub><mi>X</mi><mi>v</mi></msub><annotation encoding="application/x-tex">X_{v}</annotation></semantics></math> before applying <span class="ltx_text ltx_font_smallcaps">XQuant</span>, thereby reducing memory consumption. For generation, each new token is also down-projected into this latent space, appended to the <math alttext="X_{k}" class="ltx_Math" display="inline" id="S3.F5.m25"><semantics><msub><mi>X</mi><mi>k</mi></msub><annotation encoding="application/x-tex">X_{k}</annotation></semantics></math> and <math alttext="X_{v}" class="ltx_Math" display="inline" id="S3.F5.m26"><semantics><msub><mi>X</mi><mi>v</mi></msub><annotation encoding="application/x-tex">X_{v}</annotation></semantics></math> cache, and quantized. The concatenated <math alttext="[X_{k}|\vec{x}_{k}]" class="ltx_Math" display="inline" id="S3.F5.m27"><semantics><mrow><mo stretchy="false">[</mo><mrow><msub><mi>X</mi><mi>k</mi></msub><mo fence="false">|</mo><msub><mover accent="true"><mi>x</mi><mo stretchy="false">→</mo></mover><mi>k</mi></msub></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[X_{k}|\vec{x}_{k}]</annotation></semantics></math> is multiplied by (<math alttext="\Sigma_{k}B^{T}_{k}" class="ltx_Math" display="inline" id="S3.F5.m28"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{k}B^{T}_{k}</annotation></semantics></math>) to recompute the Keys, and the concatenated <math alttext="[X_{k}|\vec{x}_{v}]" class="ltx_Math" display="inline" id="S3.F5.m29"><semantics><mrow><mo stretchy="false">[</mo><mrow><msub><mi>X</mi><mi>k</mi></msub><mo fence="false">|</mo><msub><mover accent="true"><mi>x</mi><mo stretchy="false">→</mo></mover><mi>v</mi></msub></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[X_{k}|\vec{x}_{v}]</annotation></semantics></math> is multiplied by (<math alttext="\Sigma_{v}B^{T}_{v}" class="ltx_Math" display="inline" id="S3.F5.m30"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{v}B^{T}_{v}</annotation></semantics></math>) to recompute the values. Note that the group size <math alttext="g" class="ltx_Math" display="inline" id="S3.F5.m31"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> is typically greater than or equal to 4, meaning that naively caching <span class="ltx_text ltx_font_italic">X</span> uses greater than or equal to <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S3.F5.m32"><semantics><mrow><mn>2</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math> as much memory as simply doing KV caching.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">One challenge with extending <span class="ltx_text ltx_font_smallcaps">XQuant</span> to many newer LLMs is that these models typically use Grouped Query Attention (GQA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib1" title="">1</a>]</cite>, as opposed to standard Multi-Headed Attention (MHA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib36" title="">36</a>]</cite> used by the models we’ve addressed thus far.
As mentioned previously, we explicitly address GQA models since GQA has seen broad adoption for LLMs as a way of reducing the size of the KV cache <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib43" title="">43</a>]</cite>.
GQA reduces the memory consumption of the KV cache by sharing Keys and Values amongst a subset of the attention heads.
The challenge with extending our methodology to GQA models is that they down-project the input activation when computing the Keys and Values.
This means that while the <span class="ltx_text ltx_font_italic">X</span> embeddings have shape <math alttext="l\times d" class="ltx_Math" display="inline" id="S3.SS3.p1.m1"><semantics><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">l\times d</annotation></semantics></math> (where <math alttext="l" class="ltx_Math" display="inline" id="S3.SS3.p1.m2"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> is the sequence length and <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p1.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the hidden dimension), K and V are <math alttext="l\times\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS3.p1.m4"><semantics><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">l\times\frac{d}{g}</annotation></semantics></math> each, where <math alttext="g" class="ltx_Math" display="inline" id="S3.SS3.p1.m5"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> is the number of query heads that share Keys and Values.
For example, the Llama-3.1-8B model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib25" title="">25</a>]</cite> has a hidden dimension of 4,096 and uses <math alttext="g=4" class="ltx_Math" display="inline" id="S3.SS3.p1.m6"><semantics><mrow><mi>g</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">g=4</annotation></semantics></math>. Therefore, <span class="ltx_text ltx_font_italic">X</span> has dimension <math alttext="l\times 4K" class="ltx_Math" display="inline" id="S3.SS3.p1.m7"><semantics><mrow><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>4</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">l\times 4K</annotation></semantics></math> and the Key and Value activations have dimension <math alttext="l\times 1K" class="ltx_Math" display="inline" id="S3.SS3.p1.m8"><semantics><mrow><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">l\times 1K</annotation></semantics></math> each, so the KV cache is of shape <math alttext="l\times 2K" class="ltx_Math" display="inline" id="S3.SS3.p1.m9"><semantics><mrow><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">l\times 2K</annotation></semantics></math>.
Caching KV would therefore require storing 2 vectors of size 1K each for each token, whereas caching <span class="ltx_text ltx_font_italic">X</span> would require storing a single vector of size 4K for each token.
This means that if we naively apply <span class="ltx_text ltx_font_smallcaps">XQuant</span> to GQA models, we will have 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.p1.m10"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> memory overhead for the same precision, which negates the benefits of our approach.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Extending <span class="ltx_text ltx_font_smallcaps">XQuant</span> to support GQA</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p">To apply <span class="ltx_text ltx_font_smallcaps">XQuant</span> for models which use GQA, we apply a Singular Value Decomposition (SVD) offline to the weight matrices in order to allow the input activations to be stored in a lower-dimensional latent space.
Our algorithm is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.F5" title="Figure 5 ‣ 3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">5</span></a>.
We apply SVD to the <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m1"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> and <math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m2"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math> matrices to obtain <math alttext="U_{k}\Sigma_{k}B_{k}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m3"><semantics><mrow><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_{k}\Sigma_{k}B_{k}^{T}</annotation></semantics></math> and <math alttext="U_{v}\Sigma_{v}B_{v}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m4"><semantics><mrow><msub><mi>U</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_{v}\Sigma_{v}B_{v}^{T}</annotation></semantics></math>, respectively.
During prefill, we down-project the input embeddings as <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m5"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m6"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> (where <math alttext="U_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m7"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m8"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> are each <math alttext="d\times\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m9"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">d\times\frac{d}{g}</annotation></semantics></math>), which lowers the dimensionality of <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m10"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> by <math alttext="\frac{g}{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m11"><semantics><mfrac><mi>g</mi><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{g}{2}</annotation></semantics></math> and results in the same memory footprint as the GQA KV cache.
During generation, each new token’s input activation <math alttext="\vec{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m12"><semantics><mover accent="true"><mi>x</mi><mo stretchy="false">→</mo></mover><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math> is similarly down-projected by <math alttext="U_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m13"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m14"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> to get <math alttext="\vec{x}_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m15"><semantics><msub><mover accent="true"><mi>x</mi><mo stretchy="false">→</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\vec{x}_{k}</annotation></semantics></math> and <math alttext="\vec{x}_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m16"><semantics><msub><mover accent="true"><mi>x</mi><mo stretchy="false">→</mo></mover><mi>v</mi></msub><annotation encoding="application/x-tex">\vec{x}_{v}</annotation></semantics></math>, respectively.
These are appended to the cached <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m17"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m18"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math>, which are then multiplied by <math alttext="\Sigma_{k}B_{k}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m19"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{k}B_{k}^{T}</annotation></semantics></math> and <math alttext="\Sigma_{v}B_{v}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m20"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{v}B_{v}^{T}</annotation></semantics></math>, respectively, to get the Keys and Values.
Moreover, <math alttext="\Sigma_{k}B_{k}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m21"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{k}B_{k}^{T}</annotation></semantics></math> and <math alttext="\Sigma_{v}B_{v}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m22"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{v}B_{v}^{T}</annotation></semantics></math> are each fused (<math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m23"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> and <math alttext="B^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m24"><semantics><msup><mi>B</mi><mi>T</mi></msup><annotation encoding="application/x-tex">B^{T}</annotation></semantics></math> are multiplied to merge them into a single matrix)
to serve as the new weight matrices that the latent <math alttext="U_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m25"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m26"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math>, respectively, are multiplied with to rematerialize the KV cache.
This also reduces the recomputation cost during inference, as the fused <math alttext="\Sigma_{k}B_{k}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m27"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{k}B_{k}^{T}</annotation></semantics></math> and <math alttext="\Sigma_{v}B_{v}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m28"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{v}B_{v}^{T}</annotation></semantics></math> are smaller square matrices of shape <math alttext="\frac{d}{g}\times\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.m29"><semantics><mrow><mfrac><mi>d</mi><mi>g</mi></mfrac><mo lspace="0.222em" rspace="0.222em">×</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{d}{g}\times\frac{d}{g}</annotation></semantics></math>. We elaborate on the associated cost in more detail in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS4" title="3.4 System-Level Analysis of Rematerialization ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
Importantly, the SVD and weight fusing are done <span class="ltx_text ltx_font_italic">offline</span> and therefore don’t add any latency overhead.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p">Importantly, note that while this approach has the same memory consumption as KV quantization for the same bit width, the down-projected <span class="ltx_text ltx_font_italic">X</span> distributions are easier to quantize, giving us higher accuracy for the same bit width (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>). In fact, the latent <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m1"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> distributions reveal a very interesting structure: we find that the latent <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m2"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> groups all outliers on the first channel, and we observe this for all layers of the model across several different models on different datasets (see Figures <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F2" title="Figure B.2 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F3" title="Figure B.3 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.3</span></a>). Note that we quantize and cache <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m3"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> without applying the <math alttext="\Sigma_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m4"><semantics><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\Sigma_{k}</annotation></semantics></math> matrix of singular values. <math alttext="U_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m5"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> is a matrix with orthonormal columns, so observing this structure in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m6"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> where all the outliers lie on the first channel is very interesting. We do not, however, observe any similar interesting structure in the <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m7"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> distribution. This is in keeping with observations made by other works which find that Keys have outlier channels whereas Values do not have a clearly structured axis where outliers lie <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>. We believe that the outliers present in the first channel of <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m8"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> are simply distributed to other channels once the latent distribution is transformed by <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m9"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>, which gives rise to the outlier channels found in the Keys. We discuss this further in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2" title="Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B</span></a>. We utilize per-channel quantization for <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m10"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> and per-token quantization for <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.m11"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math>, as we find this to be the best configuration resulting in the least accuracy degradation. This is similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> which also quantize the Keys per-channel and Values per-token.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Extending <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> to support GQA</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p">We also extend our cross-layer method to support GQA models. For GQA models, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> faces the same issue addressed in Section <a class="ltx_ref ltx_markedasmath ltx_font_italic" href="https://arxiv.org/html/2508.10395v1#S3.SS3.SSS1" title="3.3.1 Extending XQuant to support GQA ‣ 3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>, specifically that GQA models down-project <span class="ltx_text ltx_font_italic">X</span> of shape <math alttext="l\times d" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m2"><semantics><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">l\times d</annotation></semantics></math> to the KV cache of shape <math alttext="l\times 2\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m3"><semantics><mrow><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">l\times 2\frac{d}{g}</annotation></semantics></math>. Since <math alttext="\Delta X" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m4"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">\Delta X</annotation></semantics></math> has the same shape as <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m5"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, naively caching this delta for GQA models results in more memory overhead than storing the KV cache. To address this, we column-wise concatenate the <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m6"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> and <math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m7"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math> projection matrices, resulting in <math alttext="W_{kv}=[W_{k}|W_{v}]\in\mathbb{R}^{d\times 2\frac{d}{g}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m8"><semantics><mrow><msub><mi>W</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>W</mi><mi>k</mi></msub><mo fence="false">|</mo><msub><mi>W</mi><mi>v</mi></msub></mrow><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">W_{kv}=[W_{k}|W_{v}]\in\mathbb{R}^{d\times 2\frac{d}{g}}</annotation></semantics></math>, on which we perform an SVD to get <math alttext="U_{kv}\Sigma_{kv}B^{T}_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m9"><semantics><mrow><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_{kv}\Sigma_{kv}B^{T}_{kv}</annotation></semantics></math>. Here, <math alttext="U_{kv}\in\mathbb{R}^{d\times 2\frac{d}{g}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m10"><semantics><mrow><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">U_{kv}\in\mathbb{R}^{d\times 2\frac{d}{g}}</annotation></semantics></math> is a shared subspace for the individual projection matrices, and <math alttext="\Sigma_{kv}B^{T}_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m11"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{kv}B^{T}_{kv}</annotation></semantics></math> is discarded.
We down-project <math alttext="\Delta X" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m12"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">\Delta X</annotation></semantics></math> by <math alttext="U_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m13"><semantics><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><annotation encoding="application/x-tex">U_{kv}</annotation></semantics></math>, resulting in the same memory footprint as the KV cache, and we quantize and cache this latent distribution.
Although this approach has the same memory consumption as KV quantization, the deltas are easier to quantize despite the latent projection, giving us higher accuracy for the same bit width (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Crucially, the SVD is performed offline, so there is no additional latency overhead during inference. When computing <math alttext="\hat{X}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m14"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{X}_{i}</annotation></semantics></math>, which is the approximation of <span class="ltx_text ltx_font_italic">X</span> for layer <span class="ltx_text ltx_font_italic">i</span>, we need to merge our accumulator <math alttext="\hat{X}_{i-1}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m15"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{i-1}</annotation></semantics></math> of shape <math alttext="l\times d" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m16"><semantics><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">l\times d</annotation></semantics></math> with the latent <math alttext="\Delta X_{i}U_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m17"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta X_{i}U_{kv}</annotation></semantics></math>. To do this, we up-project <math alttext="\Delta X_{i}U_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m18"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta X_{i}U_{kv}</annotation></semantics></math> using <math alttext="U_{kv}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m19"><semantics><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">U_{kv}^{T}</annotation></semantics></math>, add this result to the accumulator <math alttext="\hat{X}_{i-1}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m20"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{i-1}</annotation></semantics></math>, and multiply the updated accumulator <math alttext="\hat{X}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m21"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{X}_{i}</annotation></semantics></math> by <math alttext="W_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m22"><semantics><msub><mi>W</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><annotation encoding="application/x-tex">W_{kv}</annotation></semantics></math> to complete the KV cache rematerialization. One concern is whether up-projecting the latent delta by <math alttext="U_{kv}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m23"><semantics><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">U_{kv}^{T}</annotation></semantics></math> is able to retrieve the original delta: for the non-square matrix <math alttext="W_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m24"><semantics><msub><mi>W</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><annotation encoding="application/x-tex">W_{kv}</annotation></semantics></math>, the SVD produces non-square <math alttext="U_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m25"><semantics><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><annotation encoding="application/x-tex">U_{kv}</annotation></semantics></math> with orthonormal columns such that <math alttext="U_{kv}^{T}U_{kv}=I_{2\frac{d}{g}}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m26"><semantics><mrow><mrow><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo>=</mo><msub><mi>I</mi><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msub></mrow><annotation encoding="application/x-tex">U_{kv}^{T}U_{kv}=I_{2\frac{d}{g}}</annotation></semantics></math> but <math alttext="U_{kv}U_{kv}^{T}\neq I_{d}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m27"><semantics><mrow><mrow><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow><mo>≠</mo><msub><mi>I</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">U_{kv}U_{kv}^{T}\neq I_{d}</annotation></semantics></math>.
However, when the quantization function <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m28"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is the identity function, up-projecting the latent <math alttext="\Delta X_{i}U_{kv}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m29"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta X_{i}U_{kv}</annotation></semantics></math> by <math alttext="U_{kv}^{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m30"><semantics><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">U_{kv}^{T}</annotation></semantics></math> is a lossless reconstruction of the original <math alttext="\Delta X_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m31"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta X_{i}</annotation></semantics></math> when computing the KV cache for layer <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m32"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle[K|V]_{i}" class="ltx_Math" display="inline" id="S3.Ex1.m1"><semantics><msub><mrow><mo stretchy="false">[</mo><mrow><mi>K</mi><mo fence="false">|</mo><mi>V</mi></mrow><mo stretchy="false">]</mo></mrow><mi>i</mi></msub><annotation encoding="application/x-tex">\displaystyle[K|V]_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(\hat{X}_{i-1}+Q(\Delta X_{i}U_{kv})\cdot U_{kv}^{T})\cdot U_{kv}\Sigma_{kv}B_{kv}^{T}" class="ltx_Math" display="inline" id="S3.Ex1.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(\hat{X}_{i-1}+Q(\Delta X_{i}U_{kv})\cdot U_{kv}^{T})\cdot U_{kv}\Sigma_{kv}B_{kv}^{T}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad\text{where}\quad U_{kv}^{T}U_{kv}=I_{2\frac{d}{g}},\quad U_{kv}U_{kv}^{T}\neq I_{d},\quad Q(x)=x" class="ltx_Math" display="inline" id="S3.Ex2.m1"><semantics><mrow><mrow><mrow><mtext>where</mtext><mspace width="1em"></mspace><mrow><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow></mrow><mo>=</mo><msub><mi>I</mi><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msub></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow><mo>≠</mo><msub><mi>I</mi><mi>d</mi></msub></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>x</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\qquad\text{where}\quad U_{kv}^{T}U_{kv}=I_{2\frac{d}{g}},\quad U_{kv}U_{kv}^{T}\neq I_{d},\quad Q(x)=x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(\hat{X}_{i-1}U_{kv}+\Delta X_{i}U_{kv})\cdot\Sigma_{kv}B_{kv}^{T}" class="ltx_Math" display="inline" id="S3.Ex3.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(\hat{X}_{i-1}U_{kv}+\Delta X_{i}U_{kv})\cdot\Sigma_{kv}B_{kv}^{T}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(\hat{X}_{i-1}+\Delta X_{i})\cdot U_{kv}\Sigma_{kv}B_{kv}^{T}" class="ltx_Math" display="inline" id="S3.Ex4.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(\hat{X}_{i-1}+\Delta X_{i})\cdot U_{kv}\Sigma_{kv}B_{kv}^{T}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(\hat{X}_{i-1}+\Delta X_{i})\cdot W_{kv}" class="ltx_Math" display="inline" id="S3.Ex5.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mi>W</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(\hat{X}_{i-1}+\Delta X_{i})\cdot W_{kv}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\hat{X}_{i}\cdot[W_{k}|W_{v}]." class="ltx_Math" display="inline" id="S3.Ex6.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>W</mi><mi>k</mi></msub><mo fence="false">|</mo><msub><mi>W</mi><mi>v</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\hat{X}_{i}\cdot[W_{k}|W_{v}].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>System-Level Analysis of Rematerialization</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">Here, we present system-level modeling that outlines the computational and memory overheads from rematerialization with <span class="ltx_text ltx_font_smallcaps">XQuant</span> and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>.
For this analysis, we count a multiply-accumulate operations as two FLOPs.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p">Assume we are using a model with hidden dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS4.p2.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> and assume sequence length <math alttext="l" class="ltx_Math" display="inline" id="S3.SS4.p2.m2"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.
To apply <span class="ltx_text ltx_font_smallcaps">XQuant</span> on MHA models, the amount of computation required for rematerialization for a single layer is <math alttext="2\cdot 2\cdot l\cdot d^{2}" class="ltx_Math" display="inline" id="S3.SS4.p2.m3"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">2\cdot 2\cdot l\cdot d^{2}</annotation></semantics></math>.
Suppose that we apply <math alttext="e" class="ltx_Math" display="inline" id="S3.SS4.p2.m4"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>-bit quantization for <math alttext="X" class="ltx_Math" display="inline" id="S3.SS4.p2.m5"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.
Then the total number of memory operations in bytes is equal to <math alttext="\frac{e}{8}\cdot l\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p2.m6"><semantics><mrow><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\frac{e}{8}\cdot l\cdot d</annotation></semantics></math>; whereas for the KV cache quantization, it would have been <math alttext="2\cdot\frac{e}{8}\cdot l\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p2.m7"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">2\cdot\frac{e}{8}\cdot l\cdot d</annotation></semantics></math>.
To apply <span class="ltx_text ltx_font_smallcaps">XQuant</span> on GQA models, the amount of computation required for rematerialization for a single layer is <math alttext="2\cdot 2\cdot l\cdot(\frac{d}{g})^{2}" class="ltx_Math" display="inline" id="S3.SS4.p2.m8"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mrow><mo stretchy="false">(</mo><mfrac><mi>d</mi><mi>g</mi></mfrac><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">2\cdot 2\cdot l\cdot(\frac{d}{g})^{2}</annotation></semantics></math>, where <math alttext="g" class="ltx_Math" display="inline" id="S3.SS4.p2.m9"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> is the number of query heads that share Keys and Values.
This is a factor of <math alttext="g^{2}" class="ltx_Math" display="inline" id="S3.SS4.p2.m10"><semantics><msup><mi>g</mi><mn>2</mn></msup><annotation encoding="application/x-tex">g^{2}</annotation></semantics></math> less floating point operations compared to MHA models.
Suppose that we apply <math alttext="e" class="ltx_Math" display="inline" id="S3.SS4.p2.m11"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>-bit quantization for <math alttext="X" class="ltx_Math" display="inline" id="S3.SS4.p2.m12"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.
Then the total number of memory operations in bytes is equal to <math alttext="2\cdot\frac{e}{8}\cdot l\cdot\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS4.p2.m13"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">2\cdot\frac{e}{8}\cdot l\cdot\frac{d}{g}</annotation></semantics></math>, which is the same as KV cache quantization.
However, for the same <math alttext="e" class="ltx_Math" display="inline" id="S3.SS4.p2.m14"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>-bit quantization, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves much higher accuracy than KV cache quantization (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
Equivalently, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves similar accuracy with a smaller <math alttext="e" class="ltx_Math" display="inline" id="S3.SS4.p2.m15"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math> to KV cache quantization with a larger effective <math alttext="e" class="ltx_Math" display="inline" id="S3.SS4.p2.m16"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>, meaning that for the same accuracy, <span class="ltx_text ltx_font_smallcaps">XQuant</span> performs fewer memory operations.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p">Here, we also provide an example to demonstrate the amount of the rematerialization that can be performed without having the additional compute operations become the latency bottleneck.
Here, we assume the NVIDIA H100 GPU as our hardware target, which has a ridge point of <math alttext="P=\frac{\mathrm{Peak}\ \mathrm{FLOPs}}{\mathrm{Memory}\ \mathrm{BW}}=\frac{756\mathrm{TFLOPs}}{2\ \mathrm{TB/s}}=378" class="ltx_Math" display="inline" id="S3.SS4.p3.m1"><semantics><mrow><mi>P</mi><mo>=</mo><mfrac><mrow><mi>Peak</mi><mo lspace="0.410em" rspace="0em">​</mo><mi>FLOPs</mi></mrow><mrow><mi>Memory</mi><mo lspace="0.410em" rspace="0em">​</mo><mi>BW</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>756</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">L</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">P</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">s</mi></mrow><mrow><mrow><mn>2</mn><mo lspace="0.410em" rspace="0em">​</mo><mi>TB</mi></mrow><mo>/</mo><mi mathvariant="normal">s</mi></mrow></mfrac><mo>=</mo><mn>378</mn></mrow><annotation encoding="application/x-tex">P=\frac{\mathrm{Peak}\ \mathrm{FLOPs}}{\mathrm{Memory}\ \mathrm{BW}}=\frac{756\mathrm{TFLOPs}}{2\ \mathrm{TB/s}}=378</annotation></semantics></math>, and we assume that we can overlap the KV cache recomputation with loading the model weights (which corresponds to <math alttext="2\cdot 12\cdot d^{2}" class="ltx_Math" display="inline" id="S3.SS4.p3.m2"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>12</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">2\cdot 12\cdot d^{2}</annotation></semantics></math> additional memory operations for a single layer for the Llama-2-7B model).
We can solve for the maximum amount that we can reconstruct without compute becoming the bottleneck:</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P=\frac{2\cdot 2\cdot l\cdot d^{2}}{\frac{e}{8}\cdot l\cdot d+2\cdot 12\cdot d^{2}}" class="ltx_Math" display="block" id="S3.E3.m1"><semantics><mrow><mi>P</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><mrow><mrow><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>12</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">P=\frac{2\cdot 2\cdot l\cdot d^{2}}{\frac{e}{8}\cdot l\cdot d+2\cdot 12\cdot d^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p">Solving this equation for <math alttext="P=378" class="ltx_Math" display="inline" id="S3.SS4.p5.m1"><semantics><mrow><mi>P</mi><mo>=</mo><mn>378</mn></mrow><annotation encoding="application/x-tex">P=378</annotation></semantics></math>, <math alttext="d=4K" class="ltx_Math" display="inline" id="S3.SS4.p5.m2"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">d=4K</annotation></semantics></math>, and <math alttext="e=2" class="ltx_Math" display="inline" id="S3.SS4.p5.m3"><semantics><mrow><mi>e</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">e=2</annotation></semantics></math> gives a maximum sequence length of <math alttext="2.3K" class="ltx_Math" display="inline" id="S3.SS4.p5.m4"><semantics><mrow><mn>2.3</mn><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">2.3K</annotation></semantics></math> that can be rematerialized without having compute operations become the bottleneck.</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p">We can perform similar analysis for the Llama-3.1-8B model (which requires <math alttext="2\cdot 13\cdot d^{2}+2\cdot 2\cdot(\frac{d}{g})^{2}" class="ltx_Math" display="inline" id="S3.SS4.p6.m1"><semantics><mrow><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>13</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mrow><mo stretchy="false">(</mo><mfrac><mi>d</mi><mi>g</mi></mfrac><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow><annotation encoding="application/x-tex">2\cdot 13\cdot d^{2}+2\cdot 2\cdot(\frac{d}{g})^{2}</annotation></semantics></math> memory operations for the weights for a single layer, including the overhead of loading the <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS4.p6.m2"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math>/<math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.SS4.p6.m3"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math> matrices in SVD-decomposed form, and which has <math alttext="g=4" class="ltx_Math" display="inline" id="S3.SS4.p6.m4"><semantics><mrow><mi>g</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">g=4</annotation></semantics></math>):</p>
</div>
<div class="ltx_para" id="S3.SS4.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P=\frac{2\cdot 2\cdot l\cdot(\frac{d}{g})^{2}}{\frac{e}{8}\cdot l\cdot\frac{d}{g}+2\cdot 13\cdot d^{2}+2\cdot 2\cdot(\frac{d}{g})^{2}}" class="ltx_Math" display="block" id="S3.E4.m1"><semantics><mrow><mi>P</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mrow><mo stretchy="false">(</mo><mfrac><mi>d</mi><mi>g</mi></mfrac><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mrow><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>13</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mrow><mo stretchy="false">(</mo><mfrac><mi>d</mi><mi>g</mi></mfrac><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">P=\frac{2\cdot 2\cdot l\cdot(\frac{d}{g})^{2}}{\frac{e}{8}\cdot l\cdot\frac{d}{g}+2\cdot 13\cdot d^{2}+2\cdot 2\cdot(\frac{d}{g})^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p8">
<p class="ltx_p">Solving this equation for <math alttext="P=378" class="ltx_Math" display="inline" id="S3.SS4.p8.m1"><semantics><mrow><mi>P</mi><mo>=</mo><mn>378</mn></mrow><annotation encoding="application/x-tex">P=378</annotation></semantics></math>, <math alttext="d=4K" class="ltx_Math" display="inline" id="S3.SS4.p8.m2"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">d=4K</annotation></semantics></math>, <math alttext="g=4" class="ltx_Math" display="inline" id="S3.SS4.p8.m3"><semantics><mrow><mi>g</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">g=4</annotation></semantics></math>, and <math alttext="e=2" class="ltx_Math" display="inline" id="S3.SS4.p8.m4"><semantics><mrow><mi>e</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">e=2</annotation></semantics></math> gives a maximum sequence length of <math alttext="40.6K" class="ltx_Math" display="inline" id="S3.SS4.p8.m5"><semantics><mrow><mn>40.6</mn><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">40.6K</annotation></semantics></math> that can be rematerialized without having compute operations become the bottleneck.</p>
</div>
<div class="ltx_para" id="S3.SS4.p9">
<p class="ltx_p">With <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, we perform an additional <math alttext="2\cdot l\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p9.m1"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">2\cdot l\cdot d</annotation></semantics></math> compute operations for each layer, since we need to add the the cached delta <math alttext="\Delta\hat{X}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p9.m2"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta\hat{X}_{i}</annotation></semantics></math> to the accumulator <math alttext="\hat{X}_{i-1}" class="ltx_Math" display="inline" id="S3.SS4.p9.m3"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{i-1}</annotation></semantics></math>. However, this cost is negligible: if we combine it with the aforementioned rematerialization cost, we get <math alttext="2\cdot 2\cdot l\cdot d^{2}+2\cdot l\cdot d=2\cdot 2\cdot l\cdot d\cdot(d+\frac{1}{2})" class="ltx_Math" display="inline" id="S3.SS4.p9.m4"><semantics><mrow><mrow><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow></mrow><mo>=</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">2\cdot 2\cdot l\cdot d^{2}+2\cdot l\cdot d=2\cdot 2\cdot l\cdot d\cdot(d+\frac{1}{2})</annotation></semantics></math>. <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> also requires additional memory operations, as we need to load and store both the accumulator and the cached delta at each layer. Since we keep the accumulator in higher precision, we perform <math alttext="\frac{e_{b}}{8}\cdot l\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p9.m5"><semantics><mrow><mfrac><msub><mi>e</mi><mi>b</mi></msub><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\frac{e_{b}}{8}\cdot l\cdot d</annotation></semantics></math> memory operations in bytes at each layer, where <math alttext="e_{b}" class="ltx_Math" display="inline" id="S3.SS4.p9.m6"><semantics><msub><mi>e</mi><mi>b</mi></msub><annotation encoding="application/x-tex">e_{b}</annotation></semantics></math> is the accumulator’s precision (typically <math alttext="e_{b}=4" class="ltx_Math" display="inline" id="S3.SS4.p9.m7"><semantics><mrow><msub><mi>e</mi><mi>b</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">e_{b}=4</annotation></semantics></math> bits; see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS3" title="4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
Additionally, we perform <math alttext="\frac{e}{8}\cdot l\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p9.m8"><semantics><mrow><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\frac{e}{8}\cdot l\cdot d</annotation></semantics></math> memory operations required to load the cached delta, which is the same as <span class="ltx_text ltx_font_smallcaps">XQuant</span>.
For GQA models, since we are caching the quantized down-projected <math alttext="\Delta X_{i}U_{kv}" class="ltx_Math" display="inline" id="S3.SS4.p9.m9"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta X_{i}U_{kv}</annotation></semantics></math> at each layer, the number of memory operations for the delta is <math alttext="2\cdot\frac{e}{8}\cdot l\cdot\frac{d}{g}" class="ltx_Math" display="inline" id="S3.SS4.p9.m10"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>e</mi><mn>8</mn></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow><annotation encoding="application/x-tex">2\cdot\frac{e}{8}\cdot l\cdot\frac{d}{g}</annotation></semantics></math>, which is the same for <span class="ltx_text ltx_font_smallcaps">XQuant</span> applied on GQA models.
After loading this delta, we up-project it by <math alttext="U_{kv}^{T}" class="ltx_Math" display="inline" id="S3.SS4.p9.m11"><semantics><msubsup><mi>U</mi><mrow><mi>k</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">U_{kv}^{T}</annotation></semantics></math> to merge with the accumulator <math alttext="\hat{X}_{i-1}" class="ltx_Math" display="inline" id="S3.SS4.p9.m12"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{i-1}</annotation></semantics></math>, and then apply the Key and Value weight projections onto the updated accumulator.
In total, this requires <math alttext="2\cdot 4\cdot l\cdot\frac{d}{g}\cdot d" class="ltx_Math" display="inline" id="S3.SS4.p9.m13"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>4</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mi>d</mi><mi>g</mi></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">2\cdot 4\cdot l\cdot\frac{d}{g}\cdot d</annotation></semantics></math> compute operations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Empirical Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">To evaluate our method, we use the Llama-2-7B/13B, Llama-3.1-8B, and Mistral-7B-v0.3 models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib18" title="">18</a>]</cite>.
We measure perplexity on the WikiText-2 and C4 datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib26" title="">26</a>]</cite>, and we perform downstream task evaluation on the LongBench and GSM8K datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib7" title="">7</a>]</cite>.
Note that the Llama-2-7B/13B models use MHA, whereas Llama-3.1-8B and Mistral-7B use GQA.
We compare our method against KIVI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite> and KVQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>, two representative state-of-the-art KV cache quantization methods.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">For KIVI, we use a stronger baseline (referred to here as KIVI*). The original KIVI method quantizes the Keys after rotary positional embeddings (RoPE) are applied to them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite>. However, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> finds that applying RoPE to the Keys results in a less structured distribution, whereas the Keys have more structured outlier channels pre-RoPE. We therefore follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> and quantize the Keys before applying RoPE.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">For KVQuant, we use the best configuration from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>, which performs non-uniform, per-vector dense-and-sparse quantization. KVQuant’s non-uniform quantization requires deriving per-layer sensitivity-weighted non-uniform datatypes from a calibration dataset offline to
better represent the distributions. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> and use 16 calibration samples of sequence length 2K from WikiText-2.
The dense-and-sparse quantization isolates outliers separately for each vector and preserves them in higher precision.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p">Note that <span class="ltx_text ltx_font_smallcaps">XQuant</span> and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> use simple uniform quantization, with no special outlier handling or calibration required.
For the KV cache quantization baselines, we quantize the Keys per-channel and the Values per-token <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>.
We use group size of 128 for all quantization experiments.
For generative tasks, in order to be able to leverage per-channel quantization during decoding, we leave the final residual tokens unquantized (up to group size tokens), similar to the residual method in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite>.
We adopt this residual method across all generation experiments for fair comparison.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_smallcaps">XQuant</span> evaluation using perplexity on WikiText-2 and C4.
Left: (MHA) Llama-2-7B/13B; Right: (GQA) Llama-3.1-8B, and Mistral-7B.
We provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline) and group rows by similar memory consumption. For MHA models, each group shows <span class="ltx_text ltx_font_smallcaps">XQuant</span> using slightly less memory compared to KIVI*, as <span class="ltx_text ltx_font_smallcaps">XQuant</span> only needs to store scale factors and zero points for a single <span class="ltx_text ltx_font_italic">X</span> tensor, whereas KIVI* needs to store the same for <span class="ltx_text ltx_font_italic">K</span> and <span class="ltx_text ltx_font_italic">V</span>.
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_top" style="width:206.7pt;">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Llama-2-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Llama-2-13B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">(MHA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">C4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">C4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">7.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">4.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.73</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">7.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">4.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.75</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-8bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.26</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">5.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">7.26</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">4.88</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">6.73</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">7.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">4.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.83</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">8.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">7.67</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.13</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">5.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">7.36</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">4.94</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">6.79</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">6.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">8.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">5.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">7.07</span></td>
</tr>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_top" style="width:253.7pt;">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Llama-3.1-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Mistral-7B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;">(GQA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">C4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">C4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">9.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">8.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">9.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">8.51</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">6.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">9.60</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">5.33</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">8.49</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">10.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">5.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">8.62</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">6.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">9.89</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">5.39</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">8.57</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">9.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">15.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">6.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;">9.88</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:70%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-2bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text" style="font-size:70%;background-color:#E6F2F2;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">7.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">12.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">5.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span class="ltx_text ltx_font_bold" style="font-size:70%;background-color:#E6F2F2;">9.13</span></td>
</tr>
</table>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">In Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T1" title="Table 1 ‣ 4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">1</span></a>, we report perplexity results for Llama-2-7B/13B, Llama-3.1-8B, and Mistral-7B models on WikiText-2 and C4.
Perplexity has been measured using teacher forcing with the output logits of all input tokens.
For <span class="ltx_text ltx_font_smallcaps">XQuant</span> applied to MHA models, we apply per-token quantization to <span class="ltx_text ltx_font_italic">X</span>, whereas for KIVI*, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> and we apply per-channel quantization to the pre-RoPE Keys and per-token quantization to the Values.
For <span class="ltx_text ltx_font_smallcaps">XQuant</span> applied to GQA models, we cache <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S4.SS1.p1.m1"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S4.SS1.p1.m2"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math>, and we find that applying per-channel quantization to the latent <math alttext="XU_{k}" class="ltx_Math" display="inline" id="S4.SS1.p1.m3"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> and per-token quantization to the latent <math alttext="XU_{v}" class="ltx_Math" display="inline" id="S4.SS1.p1.m4"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> is the best configuration.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">Overall, we find that <span class="ltx_text ltx_font_smallcaps">XQuant</span> not only substantially outperforms KIVI* for the same memory footprint, but it also retains accuracy very close to the FP16 baseline for great memory savings.
For the same memory footprint, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves 0.88 and 0.67 less perplexity degradation compared to KIVI* for the Llama-2-7B and Llama-2-13B models, respectively.
Relative to the FP16 baseline, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves under 0.1 perplexity degradation, while using <math alttext="7.7\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.m1"><semantics><mrow><mn>7.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">7.7\times</annotation></semantics></math> less memory.
Similarly, for GQA models, <span class="ltx_text ltx_font_smallcaps">XQuant</span> pushes the boundaries of 2-bit quantization by achieving up to less than 2.2 perplexity degradation compared to KIVI* on Llama-3.1-8B.
For Mistral-7B, relative to the FP16 baseline, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves <math alttext="&lt;0.1" class="ltx_Math" display="inline" id="S4.SS1.p2.m2"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">&lt;0.1</annotation></semantics></math> perplexity degradation in 3-bit with <math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.m3"><semantics><mrow><mn>5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">5\times</annotation></semantics></math> memory savings, and only <math alttext="0.01" class="ltx_Math" display="inline" id="S4.SS1.p2.m4"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math> perplexity degradation in 4-bit with <math alttext="3.7\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.m5"><semantics><mrow><mn>3.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">3.7\times</annotation></semantics></math> memory savings. In 2-bit precision, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves 0.57 less perplexity degradation compared to KIVI*, while getting <math alttext="7.1\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.m6"><semantics><mrow><mn>7.1</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">7.1\times</annotation></semantics></math> more memory savings relative to the FP16 baseline.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Downstream Task Evaluation</h3>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_smallcaps">XQuant</span> evaluation on LongBench. We report results for the (MHA) Llama-2-7B-chat and (GQA) Llama-3.1-8B-Instruct models.
We include baseline comparisons with KIVI* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite> (details for this configuration are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
We report accuracy on each task as well as average cross-task accuracy.
We also provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline).
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;position:relative; bottom:-13.7pt;">Config</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;position:relative; bottom:-13.7pt;">KV Budget</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Single-Doc. QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Multi-Doc. QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Summarization</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Few-shot Learning</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" rowspan="2" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;position:relative; bottom:-13.7pt;">Avg.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:20.8pt;height:20.8pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:22.4pt;transform:translate(-0.79pt,-5pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">NQA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:19.6pt;height:19.6pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:20.7pt;transform:translate(-0.54pt,-4.42pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Qspr</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:26.1pt;height:26.1pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:29.9pt;transform:translate(-1.89pt,-7.66pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">MFQA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:25.5pt;height:25.5pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:29.1pt;transform:translate(-1.77pt,-7.37pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">HPQA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:22.0pt;height:22pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:25.6pt;transform:translate(-1.79pt,-8.24pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">2Wiki</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:20.6pt;height:20.6pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:22.1pt;transform:translate(-0.74pt,-4.9pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">MSQ</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:22.6pt;height:22.6pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:25.0pt;transform:translate(-1.16pt,-5.91pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">GRep</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:29.5pt;height:29.5pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:34.6pt;transform:translate(-2.58pt,-9.33pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">QMSM</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:26.8pt;height:26.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-2.8pt,-10.63pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">MNews</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:23.4pt;height:23.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.6pt;transform:translate(-2.11pt,-8.97pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">TREC</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:20.2pt;height:20.2pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="width:21.6pt;transform:translate(-0.67pt,-4.71pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">TQA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:21.2pt;height:21.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.5pt;transform:translate(-1.65pt,-7.86pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">SSum</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:14.0pt;height:14pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-0.15pt,-4.24pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">RB</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;">
<div class="ltx_inline-block ltx_transformed_outer" style="width:18.0pt;height:18pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.0pt;transform:translate(-0.99pt,-6.28pt) rotate(-45deg) ;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:80%;">LCC</span></p>
</span></div>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-2-7B-Chat</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">All KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">19.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">22.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">36.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">27.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">8.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">20.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">83.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">41.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">52.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">58.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">37.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">18.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">37.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">8.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">27.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">25.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">83.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">40.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">52.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">57.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">37.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">18.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">37.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">30.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">8.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">61.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">84.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">40.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">52.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">57.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">36.5</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">13.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">19.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">23.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">4.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">24.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">20.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">25.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">55.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">39.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">48.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">32.1</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.13</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">19.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">21.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">36.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">27.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">32.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">8.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">26.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">21.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">26.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">66.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">82.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">41.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">54.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">60.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">37.5</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">11.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">16.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">35.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">21.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">19.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">7.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">27.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">20.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">25.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">57.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">77.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">38.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">49.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">52.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">32.8</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-3.1-8B-Instruct</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">All KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">45.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">53.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">55.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">47.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">34.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">25.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">72.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">91.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">43.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">56.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">63.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">48.5</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">30.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">46.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">54.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">55.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">45.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">34.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">25.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">27.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">74.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">90.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">44.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">55.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">48.4</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">30.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">46.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">55.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">54.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">47.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">30.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">34.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">25.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">27.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">73.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">91.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">43.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">55.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">62.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">48.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">29.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">45.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">55.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">54.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">43.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">34.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">25.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">27.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">70.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">90.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">43.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">49.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">61.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">47.2</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">30.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">46.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">52.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">54.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">43.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">30.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">34.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">25.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">27.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">73.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">91.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">43.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">53.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">62.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">47.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">23.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">36.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">41.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">44.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">30.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">21.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">30.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">26.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">68.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">87.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">44.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">38.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">41.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;">39.9</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-2bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">25.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">38.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">49.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">48.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">35.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">28.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">34.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">24.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">27.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">88.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">44.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">52.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">57.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.2pt;padding-right:3.2pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">44.6</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_smallcaps">XQuant</span> evaluation on GSM8K. We report results for the Llama-2-7B-chat model.
We include baseline comparisons with KIVI* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> (details for this configuration are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
We also provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline).
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Config</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">KV Cache Size (Normalized)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">All KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.132</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.149</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.130</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.20</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.092</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;">0.14</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">0.129</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.13</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.086</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.10</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">We also provide cross-task evaluation to demonstrate the applicability of our strategy for a range of downstream tasks.
Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T2" title="Table 2 ‣ 4.2 Downstream Task Evaluation ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">2</span></a> provides evaluation on LongBench, a benchmark suite containing a range of long-context length tasks including in-context learning, document Q/A, summarization, and coding tasks.
We report <span class="ltx_text ltx_font_smallcaps">XQuant</span> results for the Llama-2-7B-Chat (MHA) and Llama-3.1-8B (GQA) models, and we also provide baseline comparisons against KIVI* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>.
For Llama-2-7B-Chat, <span class="ltx_text ltx_font_smallcaps">XQuant</span>-4bit attains the same accuracy as KIVI*-4bit and the same accuracy as the baseline, while providing 2<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> additional memory compression.
For Llama-3.1-8B-Instruct, <span class="ltx_text ltx_font_smallcaps">XQuant</span> provides improved accuracy on average for the same bit width across all precision settings.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">We also include results on long-generation reasoning tasks to demonstrate the applicability of our method for complex reasoning.
Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T3" title="Table 3 ‣ 4.2 Downstream Task Evaluation ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3</span></a> provides evaluation for the GSM8K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib7" title="">7</a>]</cite> (using lm-eval-harness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib9" title="">9</a>]</cite>), which evaluates arithmetic reasoning capabilities.
We use the chain-of-thought (CoT) configuration, and we report strict match accuracy.
We report results for the Llama-2-7B-Chat model, and we also provide baseline comparisons against KIVI* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>.
We find that <span class="ltx_text ltx_font_smallcaps">XQuant</span>-4bit attains similar accuracy to KIVI*-3bit, while providing 1.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p2.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> additional memory savings, and outperforms KIVI*-2bit while using <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.p2.m2"><semantics><mrow><mn>2</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math> less memory.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results with Cross-Layer Compression Method</h3>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_smallcaps" style="font-size:90%;">XQuant<span class="ltx_text ltx_font_upright"> evaluation using perplexity (PPL) on WikiText-2 and C4 for (MHA) Llama-2-7B/13B and (GQA) Llama-3.1-8B and Mistral-7B models.
We provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline). Note that the first 3 layers are quantized in 4-bit for KIVI*, </span>XQuant<span class="ltx_text ltx_font_upright">, and </span>XQuant-CL<span class="ltx_text ltx_font_upright">. This is done so that these methods have a comparable memory footprint to KVQuant, which stores additional memory for outliers.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="3" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-2-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-2-13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-3.1-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Mistral-7B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_rr" colspan="5" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">(MHA)</span></td>
<td class="ltx_td ltx_align_center" colspan="5" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">(GQA)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">4.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.48</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">4.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.50</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KVQuant-4bit-1%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.28</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">4.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.63</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.34</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.50</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.13</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">5.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">7.36</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">4.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">6.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">6.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">9.65</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">5.33</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">8.50</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#D1E8E8;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#D1E8E8;">XQuant-CL<span class="ltx_text ltx_font_upright">-4bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.13</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">7.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">4.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">6.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">6.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">9.61</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.32</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">8.49</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">4.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">10.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.61</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KVQuant-3bit-1%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.21</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.36</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">4.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.21</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.90</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.41</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.58</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.10</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">6.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">8.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">5.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">6.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.21</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">6.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">9.87</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">5.37</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">8.57</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#D1E8E8;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#D1E8E8;">XQuant-CL<span class="ltx_text ltx_font_upright">-3bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.10</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">7.28</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">4.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">6.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.21</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">6.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">9.67</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.34</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">8.51</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">13.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.59</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KVQuant-2bit-1%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.15</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.83</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.15</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">11.49</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.87</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.10</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-2bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.08</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">11.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">15.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">7.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">10.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.15</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">7.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">11.54</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">5.69</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">9.01</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#D1E8E8;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#D1E8E8;">XQuant-CL<span class="ltx_text ltx_font_upright">-2bit</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">7.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">7.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">0.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">6.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">10.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">5.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#D1E8E8;">8.67</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">In Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T4" title="Table 4 ‣ 4.3 Results with Cross-Layer Compression Method ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4</span></a>, we provide an evaluation for our <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> method, which retains near FP16 accuracy while achieving great memory savings with standard asymmetric uniform quantization. We report results for Llama-2-7B/13, Llama-3.1-8B, and Mistral-7B-v0.3. We also provide baseline comparisons against KIVI* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>]</cite>, which also uses asymmetric uniform quantization, and KVQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>, which uses non-uniform, per-vector dense-and-sparse quantization (see Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.SS1" title="4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4.1</span></a> for all configuration details). In our results, we list KVQuant as KVQuant-<math alttext="\langle e\rangle" class="ltx_Math" display="inline" id="S4.SS3.p1.m1"><semantics><mrow><mo stretchy="false">⟨</mo><mi>e</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle e\rangle</annotation></semantics></math>bit-<math alttext="\langle o\rangle\%" class="ltx_Math" display="inline" id="S4.SS3.p1.m2"><semantics><mrow><mrow><mo stretchy="false">⟨</mo><mi>o</mi><mo stretchy="false">⟩</mo></mrow><mo>%</mo></mrow><annotation encoding="application/x-tex">\langle o\rangle\%</annotation></semantics></math>, where <math alttext="\langle e\rangle" class="ltx_Math" display="inline" id="S4.SS3.p1.m3"><semantics><mrow><mo stretchy="false">⟨</mo><mi>e</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle e\rangle</annotation></semantics></math> is the quantization bit-width and <math alttext="\langle o\rangle\%" class="ltx_Math" display="inline" id="S4.SS3.p1.m4"><semantics><mrow><mrow><mo stretchy="false">⟨</mo><mi>o</mi><mo stretchy="false">⟩</mo></mrow><mo>%</mo></mrow><annotation encoding="application/x-tex">\langle o\rangle\%</annotation></semantics></math> is the percentage of outliers per layer stored in a sparse, high precision format. We use an outlier threshold of <math alttext="1\%" class="ltx_Math" display="inline" id="S4.SS3.p1.m5"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">1\%</annotation></semantics></math>, which is the best configuration shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>. Note that for KIVI*, <span class="ltx_text ltx_font_smallcaps">XQuant</span>, and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, we keep the first 3 layers in higher precision (4-bit). We find that keeping these early layers in higher precision noticeably mitigates perplexity degradation and also results in a comparable memory footprint with KVQuant, which uses additional memory to keep outliers in higher precision. This is in keeping with the findings of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib17" title="">17</a>]</cite> which showed that the first few layers of a network with residual connections do substantial representation learning (large transformations of the input) whereas later layers apply small iterative refinements to the input. Thus, the deltas for the first few layers are harder to quantize, whereas the deltas for the remaining layers of the network are much easier to quantize in low-bit precision. For <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, we use the third layer as the higher-precision base layer which becomes the accumulator.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p">For Llama-2-7B, relative to the FP16 baseline, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> results in only <math alttext="0.1" class="ltx_Math" display="inline" id="S4.SS3.p2.m1"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> perplexity degradation in 2-bit with <math alttext="12.5\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m2"><semantics><mrow><mn>12.5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">12.5\times</annotation></semantics></math> memory savings, and only <math alttext="0.01" class="ltx_Math" display="inline" id="S4.SS3.p2.m3"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math> perplexity degradation in 3-bit with <math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m4"><semantics><mrow><mn>10</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math> memory savings. Compared to KVQuant-2bit-1%, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>-2bit results in <math alttext="0.42" class="ltx_Math" display="inline" id="S4.SS3.p2.m5"><semantics><mn>0.42</mn><annotation encoding="application/x-tex">0.42</annotation></semantics></math> less perplexity degradation while using <math alttext="1.9\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m6"><semantics><mrow><mn>1.9</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.9\times</annotation></semantics></math> less memory. Similarly for Llama-2-13B, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>-2bit achieves 0.23 less perplexity degradation compared to KVQuant-2bit-1% on WikiText-2 while using <math alttext="1.9\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m7"><semantics><mrow><mn>1.9</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.9\times</annotation></semantics></math> less memory. On Llama-3.1-8B, in 2-bit precision, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> retains a perplexity of 10.15 on C4, which is more than 3 points less than KIVI*-2bit (13.54) and more than 1 point less than KVQuant-2bit-1% (11.49). On Mistral-7B, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>-2bit saves <math alttext="6.7\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m8"><semantics><mrow><mn>6.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">6.7\times</annotation></semantics></math> memory while only facing a perplexity degradation of 0.14 compared to the FP16 baseline on WikiText-2. Impressively, for only a perplexity degradation of 0.12 relative to KVQuant-4bit-1% on WikiText-2, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>-2bit uses <math alttext="1.8\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m9"><semantics><mrow><mn>1.8</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.8\times</annotation></semantics></math> less memory than KVQuant-4bit-1%. Overall, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> outperforms state-of-the-art KV cache quantization methods for ultra low precision bit widths, achieving near FP16 accuracy with <math alttext="1.5-2\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m10"><semantics><mrow><mn>1.5</mn><mo>−</mo><mn>2</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.5-2\times</annotation></semantics></math> memory savings over KV cache quantization and <math alttext="6-12\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.m11"><semantics><mrow><mn>6</mn><mo>−</mo><mn>12</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">6-12\times</annotation></semantics></math> memory savings over the FP16 baseline.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">As compute capabilities continue to outpace memory bandwidth and capacity on modern GPU hardware, LLM inference is increasingly becoming more memory-bandwidth bound.
In light of this hardware scaling trend, a natural strategy is to perform additional compute operations in order to reduce memory requirements.
In this work, we adopt a forward-looking vision to speed up inference of LLMs by exploiting the compute scaling trends on newer generation hardware.
Specifically, LLM inference is typically memory-bandwidth bound due to loading the large KV cache when generate each token.
To address this, we aim to reduce the memory requirements for LLM inference through reducing the size of the KV cache activations in exchange for higher computation cost, thus reducing the number of memory operations needed to generate each token and speeding up inference.
We propose <span class="ltx_text ltx_font_smallcaps">XQuant</span>, which quantizes the layer input activations in order to reduce memory consumption by 2<math alttext="\times" class="ltx_Math" display="inline" id="S5.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> relative to KV caching, and rematerializes the Keys and Values on-the-fly during inference.
We then extend our basic method and propose <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>, which exploits the cross-layer compressibility of the <span class="ltx_text ltx_font_italic">X</span> embeddings between successive layers.
We find that using simple uniform quantization, <span class="ltx_text ltx_font_smallcaps">XQuant</span> and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> surpass state-of-the-art KV cache quantization methods like KVQuant that use non-uniform quantization and dense-and-sparse quantization, while also retaining accuracy close to the FP16 baseline.
Relative to the FP16 baseline, <span class="ltx_text ltx_font_smallcaps">XQuant</span> achieves under 0.1 perplexity degradation while also using <math alttext="7.7\times" class="ltx_math_unparsed" display="inline" id="S5.p1.m2"><semantics><mrow><mn>7.7</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">7.7\times</annotation></semantics></math> less memory for the Llama-2-7B and Llama-2-13B models. <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> achieves <math alttext="12.5\times" class="ltx_math_unparsed" display="inline" id="S5.p1.m3"><semantics><mrow><mn>12.5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">12.5\times</annotation></semantics></math> memory savings with only 0.1 perplexity degradation in 2-bit precision, and <math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S5.p1.m4"><semantics><mrow><mn>10</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math> memory savings with only 0.01 perplexity degradation in 3-bit precision compared to the FP16 baseline. Both <span class="ltx_text ltx_font_smallcaps">XQuant</span> and <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> reduce perplexity degradation by several points compared to KIVI* and KVQuant low-bit precision quantization.
With the growing discrepancy between compute and memory capabilities on hardware platforms, rematerialization methods like <span class="ltx_text ltx_font_smallcaps">XQuant</span> can help exploit the available computation in order to accelerate memory bandwidth-bound LLM inference, while also retaining near FP16 accuracy even in low-bit precision.</p>
</div>
<section class="ltx_subsection" id="S5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Limitations</h3>
<div class="ltx_para" id="S5.SSx1.p1">
<p class="ltx_p">Our work focuses on reducing the memory requirements for LLM inference by quantizing the input <span class="ltx_text ltx_font_italic">X</span> embeddings and rematerializing KV cache activations.
While this approach allows for aggressive memory compression with minimal accuracy loss, it requires additional compute operations to perform rematerialization, which may increase latency on particular hardware platforms.
Additionally, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> reduces the memory capacity requirements, but it requires additional compute and memory operations in order to rematerialize KV cache activations due to having to load the accumulator. However, in memory-constrained scenarios where the goal is to attain near FP16 accuracy, <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> can be an optimal choice.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Acknowledgements</h3>
<div class="ltx_para" id="S5.SSx2.p1">
<p class="ltx_p">We acknowledge gracious support from the FuriosaAI team including Jihoon Yoon, Kevin Galim, Heeju Kim, and Hyung Il Koo, as well as from Intel, Apple, NVIDIA, and Mozilla.
We also appreciate the support from Microsoft through their Accelerating Foundation Model Research.
Furthermore, we appreciate support from
Google Cloud, the Google TRC team and Prof. David Patterson.
Prof. Keutzer’s lab is sponsored by the Intel corporation, UC Berkeley oneAPI Center of Excellence, Intel VLAB team, as well as funding through BDD and BAIR.
MWM would also like to acknowledge DARPA, DOE, NSF, and ONR.
This work was supported in part by the Director, Office of Science, Office of Advanced Scientific Computing Research, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.
Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">Gqa: Training generalized multi-query transformer models from multi-head checkpoints.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 4895–4901, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 3119–3137, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, and Mohamed S Abdelfattah.

</span>
<span class="ltx_bibblock">xkv: Cross-layer svd for kv-cache compression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2503.18893</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, and Kai-Chiang Wu.

</span>
<span class="ltx_bibblock">Palu: Compressing kv-cache with low-rank projection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning Representations (ICLR)</span>, April 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 24(240):1–113, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2407.21783</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">The language model evaluation harness, 07 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Shiwei Gao, Youmin Chen, and Jiwu Shu.

</span>
<span class="ltx_bibblock">Fast state restoration in llm serving with hcache.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Twentieth European Conference on Computer Systems</span>, pages 128–143, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Ai and memory wall.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">IEEE Micro</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Nils Graef and Andrew Wasielewski.

</span>
<span class="ltx_bibblock">Slim attention: cut your context memory in half without loss–k-cache is all you need for mha.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2503.05840</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Insu Han, Praneeth Kacham, Amin Karbasi, Vahab Mirrokni, and Amir Zandieh.

</span>
<span class="ltx_bibblock">Polarquant: Quantizing kv caches with polar transformation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2502.02617</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, and Bohan Zhuang.

</span>
<span class="ltx_bibblock">Zipcache: Accurate and efficient kv cache quantization with salient token identification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 37:68287–68307, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun S Shao, Kurt Keutzer, and Amir Gholami.

</span>
<span class="ltx_bibblock">Kvquant: Towards 10 million context length llm inference with kv cache quantization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 37:1270–1303, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica.

</span>
<span class="ltx_bibblock">Checkmate: Breaking the memory wall with optimal tensor rematerialization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 2:497–511, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Residual connections encourage iterative inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.06825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir Gholami.

</span>
<span class="ltx_bibblock">Full stack optimization of transformer inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Architecture and System Support for Transformer Models (ASSYST @ISCA 2023)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Sehoon Kim, Coleman Richard Charles Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Squeezellm: Dense-and-sparse quantization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 23901–23923. PMLR, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, and Jaehyuk Huh.

</span>
<span class="ltx_bibblock">Efficient llm inference with activation checkpointing and hybrid caching.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2501.01792</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan.

</span>
<span class="ltx_bibblock">Intactkv: Improving large language model quantization by keeping pivot tokens intact.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics ACL 2024</span>, pages 7716–7741, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu.

</span>
<span class="ltx_bibblock">Kivi: A tuning-free asymmetric 2bit quantization for kv cache.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 32332–32344. PMLR, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Llama 3.1: <a class="ltx_ref ltx_url ltx_font_typewriter" href="%7Bhttps://ai.meta.com/blog/meta-llama-3-1%7D" title="">{https://ai.meta.com/blog/meta-llama-3-1}</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv e-prints</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen.

</span>
<span class="ltx_bibblock">Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">The Thirteenth International Conference on Learning Representations</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy.

</span>
<span class="ltx_bibblock">Eigen attention: Attention in low-rank space for kv cache compression.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2024</span>, pages 15332–15344, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele.

</span>
<span class="ltx_bibblock">Loki: Low-rank keys for efficient sparse attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 37:16692–16723, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Neurocomputing</span>, 568:127063, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish
Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar
Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis,
Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot.

</span>
<span class="ltx_bibblock">Gemma 3 technical report, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael W Mahoney, Kurt Keutzer, and Amir Gholami.

</span>
<span class="ltx_bibblock">Quantspec: Self-speculative decoding with hierarchical quantized kv cache.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">LLaMA: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages 5998–6008, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Samuel Williams, Andrew Waterman, and David Patterson.

</span>
<span class="ltx_bibblock">Roofline: an insightful visual performance model for multicore architectures.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Communications of the ACM</span>, 52(4):65–76, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Songhao Wu, Ang Lv, Xiao Feng, Yufei Zhang, Xun Zhang, Guojun Yin, Wei Lin, and Rui Yan.

</span>
<span class="ltx_bibblock">Polarquant: Leveraging polar transformation for efficient key cache quantization and decoding acceleration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2502.00527</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.

</span>
<span class="ltx_bibblock">Efficient streaming language models with attention sinks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">The Twelfth International Conference on Learning Representations</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.

</span>
<span class="ltx_bibblock">On layer normalization in the transformer architecture.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages 10524–10533. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Recalkv: Low-rank kv cache compression via head reordering and offline calibration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2505.24357</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, and Ruofei Zhang.

</span>
<span class="ltx_bibblock">El-attention: Memory efficient lossless attention for generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 11648–11658. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu.

</span>
<span class="ltx_bibblock">Qwen3 technical report, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, and Dongsoo Lee.

</span>
<span class="ltx_bibblock">No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.18096</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al.

</span>
<span class="ltx_bibblock">Llm inference unveiled: Survey and roofline model insights.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.16363</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 33rd International Conference on Neural Information Processing Systems</span>, pages 12381–12392, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen.

</span>
<span class="ltx_bibblock">Lorc: Low-rank compression for llms kv cache with a progressive compression strategy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2410.03111</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prefill for <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>
</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p">Here we include a visualization of the prefill phase of inference for <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span>. In particular, we show how the deltas between the <math alttext="X" class="ltx_Math" display="inline" id="A1.p1.m1"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> embeddings for successive layers are calculated, quantized, and cached. Note that besides Layer 0’s input <math alttext="X_{0}" class="ltx_Math" display="inline" id="A1.p1.m2"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math>, the input for all other layers is an approximation calculated using the sum of <math alttext="X_{0}" class="ltx_Math" display="inline" id="A1.p1.m3"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math> and the previous quantized deltas.</p>
</div>
<figure class="ltx_figure" id="A1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="A1.F1.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure A.1</span>: </span><span class="ltx_text" style="font-size:90%;">
Illustration of <span class="ltx_text ltx_font_smallcaps">XQuant-CL</span> algorithm during prefill. Besides Layer 0, the input to all other layers is a cross layer approximation, computed using the deltas of all previous layers and the input of Layer 0. The input of Layer 0 is summed with each layer’s delta so it can be treated as an accumulator, allowing us to avoid loading all <math alttext="N-1" class="ltx_Math" display="inline" id="A1.F1.m5"><semantics><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math> deltas to compute Layer <math alttext="N" class="ltx_Math" display="inline" id="A1.F1.m6"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>’s <math alttext="X" class="ltx_Math" display="inline" id="A1.F1.m7"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. After a layer is done processing, the input embeddings to the layer for all tokens in the sequence are subtracted from the output activations of the layer (the same shape as the input embeddings), and this delta is quantized and cached as <math alttext="\Delta\hat{X}" class="ltx_Math" display="inline" id="A1.F1.m8"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>X</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\Delta\hat{X}</annotation></semantics></math>.
</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Observed Outlier Property When Applying <span class="ltx_text ltx_font_smallcaps">XQuant</span> for GQA Models</h2>
<figure class="ltx_figure" id="A2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="264" id="A2.F1.g1" src="x7.png" width="830"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.1</span>: </span><span class="ltx_text" style="font-size:90%;">
<math alttext="X\in\mathbb{R}^{L\times d}" class="ltx_Math" display="inline" id="A2.F1.m6"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>L</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X\in\mathbb{R}^{L\times d}</annotation></semantics></math>, where <math alttext="L" class="ltx_Math" display="inline" id="A2.F1.m7"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is the sequence length, and <math alttext="d" class="ltx_Math" display="inline" id="A2.F1.m8"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the hidden dimension. <math alttext="U_{k}\in\mathbb{R}^{d\times\frac{d}{g}}" class="ltx_Math" display="inline" id="A2.F1.m9"><semantics><mrow><msub><mi>U</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">U_{k}\in\mathbb{R}^{d\times\frac{d}{g}}</annotation></semantics></math> and <math alttext="\Sigma_{k}\in\mathbb{R}^{\frac{d}{g},\frac{d}{g}}" class="ltx_Math" display="inline" id="A2.F1.m10"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mfrac><mi>d</mi><mi>g</mi></mfrac><mo>,</mo><mfrac><mi>d</mi><mi>g</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">\Sigma_{k}\in\mathbb{R}^{\frac{d}{g},\frac{d}{g}}</annotation></semantics></math></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">where <math alttext="g" class="ltx_Math" display="inline" id="A2.F1.m11"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> is the group size used by GQA models. Note that <math alttext="\sigma_{0}\geq\sigma_{1}\geq\cdots\geq\sigma_{\frac{d}{g}}&gt;0" class="ltx_Math" display="inline" id="A2.F1.m12"><semantics><mrow><msub><mi>σ</mi><mn>0</mn></msub><mo>≥</mo><msub><mi>σ</mi><mn>1</mn></msub><mo>≥</mo><mi mathvariant="normal">⋯</mi><mo>≥</mo><msub><mi>σ</mi><mfrac><mi>d</mi><mi>g</mi></mfrac></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma_{0}\geq\sigma_{1}\geq\cdots\geq\sigma_{\frac{d}{g}}&gt;0</annotation></semantics></math>. We find that <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.F1.m13"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> has massive outliers in the first channel across all layers for different models on different datasets (see Figures <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F2" title="Figure B.2 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F3" title="Figure B.3 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.3</span></a>). Multiplying <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.F1.m14"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> by <math alttext="\Sigma_{k}" class="ltx_Math" display="inline" id="A2.F1.m15"><semantics><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\Sigma_{k}</annotation></semantics></math> preserves the ordering of outlier channels (first channel still has the largest magnitude outliers). When multiplying <math alttext="XU_{k}\Sigma_{k}" class="ltx_Math" display="inline" id="A2.F1.m16"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}\Sigma_{k}</annotation></semantics></math> by <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.F1.m17"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>, the first row of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.F1.m18"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> interacts with the first outlier channel of <math alttext="XU_{k}\Sigma_{k}" class="ltx_Math" display="inline" id="A2.F1.m19"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}\Sigma_{k}</annotation></semantics></math>, as highlighted in red above. The top-k largest magnitude values from this first row can be used to identify which channels in the Keys are outlier channels. This can be achieved offline without any calibration data, and simply by inspecting the weights of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.F1.m20"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>.</p>
</div>
</div>
</figure>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Recap</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p">As a reminder, we decompose the Key and Value projection matrices <math alttext="W_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m1"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> and <math alttext="W_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m2"><semantics><msub><mi>W</mi><mi>v</mi></msub><annotation encoding="application/x-tex">W_{v}</annotation></semantics></math> using the SVD to get <math alttext="U_{k}\Sigma_{k}B^{T}_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m3"><semantics><mrow><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_{k}\Sigma_{k}B^{T}_{k}</annotation></semantics></math> and <math alttext="U_{v}\Sigma_{v}B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m4"><semantics><mrow><msub><mi>U</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_{v}\Sigma_{v}B^{T}_{v}</annotation></semantics></math> respectively. We down-project <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m5"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> by <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m6"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m7"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> and quantize and cache these latent distributions, while also fusing <math alttext="\Sigma_{k}B^{T}_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m8"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{k}B^{T}_{k}</annotation></semantics></math> into one weight matrix and <math alttext="\Sigma_{v}B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m9"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_{v}B^{T}_{v}</annotation></semantics></math> into another weight matrix, both of which respectively multiply <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m10"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.m11"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> to rematerialize the Keys and Values.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Observed Outlier Property</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p">When we project <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m1"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> by <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m2"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math>, we observe an interesting property of these distributions across all layers for all models, where <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m3"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> transforms <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m4"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> such that all outliers are grouped on the first channel of the resulting matrix. Although we observe this phenomenon for both MHA and GQA models, we discuss it here in the context of GQA models. This is because down-projecting <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m5"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> by <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m6"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> gives us memory savings and is relevant to the <span class="ltx_text ltx_markedasmath ltx_font_smallcaps">XQuant</span> algorithm for GQA models, whereas for MHA models, we simply cache <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m8"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> itself. We visualize the <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m9"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m10"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math>, and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m11"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> distributions for the Llama-3.1-8B and Mistral-7B models in Figures <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F2" title="Figure B.2 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F3" title="Figure B.3 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.3</span></a> respectively. For each model, we visualize the distributions for 3 different layers on the WikiText-2 and C4 datasets to ensure that this observed property cannot simply be attributed to a specific dataset, model, or layer. Note that these distributions <em class="ltx_emph ltx_font_italic">do not include</em> the scaling term <math alttext="\Sigma" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m12"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> from the SVD – they only visualize each of the transformations that the left singular vectors matrices <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m13"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math> and <math alttext="U_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m14"><semantics><msub><mi>U</mi><mi>v</mi></msub><annotation encoding="application/x-tex">U_{v}</annotation></semantics></math> with orthonormal columns apply on the input <math alttext="X" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.m15"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p2">
<p class="ltx_p">As mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.SS3.SSS1" title="3.3.1 Extending XQuant to support GQA ‣ 3.3 Support for Grouped-Query Attention Models ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>, we notice a
pattern in the <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p2.m1"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> distributions across all layers of both models, where the first channel has massive outliers across all tokens, whereas all other channels are much smaller in magnitude. We do not, however, observe any similar interesting quality for the <math alttext="XU_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p2.m2"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> distributions. This observation implies that the activation embeddings for all tokens have high cosine similarity with the first column vector of <math alttext="U_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p2.m3"><semantics><msub><mi>U</mi><mi>k</mi></msub><annotation encoding="application/x-tex">U_{k}</annotation></semantics></math>, which is also the left singular vector associated with the largest singular value of <math alttext="W_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p2.m4"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math>. In other words, the activation embeddings for all tokens roughly lie close to the direction where <math alttext="W_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p2.m5"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> applies maximum scaling.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p3">
<p class="ltx_p">We attempt to exploit this finding by keeping the first outlier channel of <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p3.m1"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> in FP16, while quantizing the remaining channels using per-channel quantization with group-size 128 as before. We use the same setup as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4" title="4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">4</span></a>. These results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.T1" title="Table B.1 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.1</span></a>. The data for KIVI* and <span class="ltx_text ltx_font_smallcaps">XQuant</span> have been duplicated from Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T1" title="Table 1 ‣ 4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">1</span></a> to serve as a reference point. We find that keeping the outlier channel in FP16 gives non-trivial benefits for 2-bit quantization. For instance, Llama-3.1-8B sees an improvement of 0.2 perplexity on C4 in 2-bit precision.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Connection to KV cache quantization methods</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p">Previous KV cache quantization methods such as KVQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> preserve outlier values in the Keys in higher precision to reduce accuracy degradation from quantization. Since the Keys have been observed to have distinct outlier channels (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S3.F3" title="Figure 3 ‣ 3.2 XQuant-CL: Leveraging Cross-Layer Similarity in X ‣ 3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">3</span></a>), determining which outliers to preserve in the entire Keys matrix requires finding these outlier channels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite>. To identify these outlier channels, methods like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#bib.bib15" title="">15</a>]</cite> run models on calibration datasets, which allows them to note which channels tend to be outliers across diverse data samples.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p2">
<p class="ltx_p">Having observed the outlier behavior of the first channel in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m1"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math>, we explore if outlier channels in the Keys can be identified by inspecting the SVD decomposition of the <math alttext="W_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m2"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math> matrix offline without the need to run calibration. We follow this line of reasoning: <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m3"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> is an ordered distribution where the first channel (i.e., first column) is associated with the largest singular value of <math alttext="W_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m4"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_{k}</annotation></semantics></math>, the second channel is associated with the second largest singular value, and so on. If we multiply <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m5"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> by <math alttext="\Sigma_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m6"><semantics><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\Sigma_{k}</annotation></semantics></math>, the first channel in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m7"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> gets scaled by the largest singular value <math alttext="\sigma_{0}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m8"><semantics><msub><mi>σ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\sigma_{0}</annotation></semantics></math>, the second channel in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m9"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> gets scaled by the second largest singular value <math alttext="\sigma_{1}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m10"><semantics><msub><mi>σ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\sigma_{1}</annotation></semantics></math>, and so on. This matrix multiplication is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F1" title="Figure B.1 ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.1</span></a>. Thus, <math alttext="XU_{k}\cdot\Sigma_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m11"><semantics><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}\cdot\Sigma_{k}</annotation></semantics></math> preserves the ordering of the outlier channels (i.e., the first channel in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m12"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> is still the outlier channel). Finally to complete the rematerialization of Keys, <math alttext="(XU_{k}\Sigma_{k})" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m13"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(XU_{k}\Sigma_{k})</annotation></semantics></math> is multiplied by <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p2.m14"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>, which necessarily distributes the outliers in the first channel to other channels. These are the channels that KV cache quantization methods try to identify in the Keys distribution via running calibration.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p3">
<p class="ltx_p">However, we hypothesize that the column in <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m1"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> whose first element has the largest magnitude compared to the first element of all other columns will also tend to be the column of the Keys containing the outlier channel. For example, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F1" title="Figure B.1 ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.1</span></a>, the second column of <math alttext="B_{v}^{T}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m2"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B_{v}^{T}</annotation></semantics></math> is shown to contain <math alttext="b_{max}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m3"><semantics><msub><mi>b</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">b_{max}</annotation></semantics></math>, which is the largest magnitude value in the first row of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m4"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>. This means that the second column of the Keys will tend to be the outlier channel. This is because the values in the first row vector of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m5"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> are the scalars which interact with the first outlier channel in <math alttext="(XU_{k}\Sigma_{k})" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m6"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(XU_{k}\Sigma_{k})</annotation></semantics></math> when performing matrix multiplication. So, the column in the first row vector of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m7"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> with the largest scalar that "hits" the first column in <math alttext="(XU_{k}\Sigma_{k})" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m8"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(XU_{k}\Sigma_{k})</annotation></semantics></math> will result in the largest values in the Keys. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.F1" title="Figure B.1 ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.1</span></a>, the largest outlier channel <math alttext="\sigma_{0}X\vec{u}_{0}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m9"><semantics><mrow><msub><mi>σ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>u</mi><mo stretchy="false">→</mo></mover><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\sigma_{0}X\vec{u}_{0}</annotation></semantics></math> is highlighted in red, and the first row vector in <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p3.m10"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> containing the scalars which multiply this outlier channel are also highlighted in red.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p4">
<p class="ltx_p">However, there are two cases in which the above hypothesis does not hold: 1) other row vectors in <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p4.m1"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> contain scalars which cause another channel that is not the first channel in <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p4.m2"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> to blow up in magnitude and 2) the largest scalar of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p4.m3"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math>’s first row vector results in most values of the outlier channel having an opposite sign to the next largest outlier channel of <math alttext="(XU_{k}\Sigma_{k})" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p4.m4"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathvariant="normal">Σ</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(XU_{k}\Sigma_{k})</annotation></semantics></math>, causing their sum to have smaller magnitude. Nevertheless, these scenarios are less likely given that the other channels are themselves much smaller in magnitude than the first channel, so it is unlikely that 1) any other channel could blow up to surpass the first outlier channel in magnitude and 2) adding the first outlier channel to another channel whose elements are the opposite sign will impact the first outlier channel much.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p5">
<p class="ltx_p">We test this hypothesis by using the above method to determine the outlier channels in the Keys, and then check how many of these predictions are correct by comparing with the ground truth. For the ground truth, we pick the channel in the Keys matrix which has the largest average magnitude. As there can be multiple outlier channels in the Keys, we choose the column indices for the top-k largest magnitude values in the first row of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p5.m1"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> as our predictions. We then simply check if the ground truth index for the outlier channel in the Keys appears in any of the predicted indices by the above method. We mark the predictions as correct if any one of them contain the ground truth index, and we report the final accuracy as the percentage of correct predictions across the Keys for all layers of the model. We test this method on two different datasets, WikiText-2 and C4, to ascertain whether this weights-only based analysis is robust to different data. We also test on the Llama-3.1-8B and Mistral-7B models to check the method’s efficacy across different models. Results are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#A2.T2" title="Table B.2 ‣ Connection to KV cache quantization methods ‣ Appendix B Observed Outlier Property When Applying XQuant for GQA Models ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p6">
<p class="ltx_p">We find that for Llama-3.1-8B, only 8 of the largest magnitude values of the first of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p6.m1"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> are needed to determine the outlier channel for the Keys. For Mistral-7B, using the top-8 largest magnitude values attains 96.88% accuracy. Moreover, for both models, the accuracy is consistent across datasets, demonstrating the robustness of this method to determine the outlier indices for the Keys.</p>
</div>
<figure class="ltx_table" id="A2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table B.1</span>: </span><span class="ltx_text" style="font-size:90%;">We attempt to exploit the structured distribution of <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.T1.m2"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> by keeping the first outlier channel in FP16. The table shows <span class="ltx_text ltx_font_smallcaps">XQuant</span> evaluation using perplexity on WikiText-2 and C4 on Llama-3.1-8B and Mistral-7B. We duplicate the KIVI* and <span class="ltx_text ltx_font_smallcaps">XQuant</span> results from Table <a class="ltx_ref" href="https://arxiv.org/html/2508.10395v1#S4.T1" title="Table 1 ‣ 4.1 Main Results ‣ 4 Empirical Results ‣ XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization"><span class="ltx_text ltx_ref_tag">1</span></a> to serve as a reference point. For 2-bit precision, keeping the first channel in FP16 results in some perplexity improvement.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">KV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Llama-3.1-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">Mistral-7B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;">(GQA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Wiki2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">C4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">1.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-4bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.51</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:80%;">XQuant</span><span class="ltx_text" style="font-size:80%;">-4bit</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.60</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.33</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.49</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-4bit-FP16-outlier-channel</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">6.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">9.60</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">5.33</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">8.49</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-3bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">10.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.62</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:80%;">XQuant</span><span class="ltx_text" style="font-size:80%;">-3bit</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.89</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.39</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">8.57</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-3bit-FP16-outlier-channel</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.20</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">6.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">9.87</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">5.38</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">8.56</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">KIVI*-2bit</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">15.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">6.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.88</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:80%;">XQuant</span><span class="ltx_text" style="font-size:80%;">-2bit</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">0.14</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">7.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">12.27</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">5.79</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;">9.13</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_smallcaps" style="font-size:80%;background-color:#E6F2F2;">XQuant<span class="ltx_text ltx_font_upright">-2bit-FP16-outlier-channel</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text" style="font-size:80%;background-color:#E6F2F2;">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">7.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">12.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">5.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.85pt;padding-bottom:0.85pt;"><span class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#E6F2F2;">9.08</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table B.2</span>: </span><span class="ltx_text" style="font-size:90%;">
Percentage of outlier channels predicted correctly by only analyzing the top-k values of the weight matrix <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.T2.m3"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> offline, without any calibration data. For Llama-3.1-8B, only 8 of the largest magnitude values of the first row of <math alttext="B^{T}_{v}" class="ltx_Math" display="inline" id="A2.T2.m4"><semantics><msubsup><mi>B</mi><mi>v</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">B^{T}_{v}</annotation></semantics></math> are needed to determine the outlier channel for the Keys.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">top-k</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Llama-3.1-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Mistral-7B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">WikiText2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">C4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">WikiText2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">C4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">k=1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">66.14%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">71.15%</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">75.35%</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">71.91%</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">k=2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">72.08%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">75.08%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">87.55%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">83.48%</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">k=4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">87.71%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">90.62%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">93.75%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;">93.75%</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#E6F2F2;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" style="font-size:90%;background-color:#E6F2F2;">k=8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6F2F2;">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6F2F2;">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6F2F2;">96.88<span class="ltx_text ltx_font_medium">%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6F2F2;">96.88%</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="A2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="590" id="A2.F2.g1" src="x8.png" width="664"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="594" id="A2.F2.g2" src="x9.png" width="664"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.2</span>: </span><math alttext="X" class="ltx_Math" display="inline" id="A2.F2.m4"><semantics><mi mathsize="0.900em">X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (left), <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.F2.m5"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> (middle), and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="A2.F2.m6"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> (right) distributions for Llama-3.1-8B on samples from WikiText-2 (top) and C4 (bottom).</span></figcaption>
</figure>
<figure class="ltx_figure" id="A2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="593" id="A2.F3.g1" src="x10.png" width="664"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="594" id="A2.F3.g2" src="x11.png" width="664"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.3</span>: </span><math alttext="X" class="ltx_Math" display="inline" id="A2.F3.m4"><semantics><mi mathsize="0.900em">X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (left), <math alttext="XU_{k}" class="ltx_Math" display="inline" id="A2.F3.m5"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">XU_{k}</annotation></semantics></math> (middle), and <math alttext="XU_{v}" class="ltx_Math" display="inline" id="A2.F3.m6"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>U</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">XU_{v}</annotation></semantics></math> (right) distributions for Mistral-7B on samples from WikiText-2 (top) and C4 (bottom).</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 14 06:53:02 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
