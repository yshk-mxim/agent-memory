<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs</title>
<!--Generated on Wed Apr 16 03:48:30 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
LLM,  KV-Cache,  RAG,  Vector DB,  TTFT
" lang="en" name="keywords"/>
<base href="/html/2504.11765v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S1" title="In Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2" title="In Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background and Motivation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.SS1" title="In II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">KV Cache Utilization in LLM Inference</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.SS2" title="In II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">RAG Prompt Composition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.SS3" title="In II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Locality of Documents Retrieved by Queries in RAG</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.SS4" title="In II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Shared KV Cache for Multi-instance LLM Inference</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3" title="In Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Design and Implementation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.SS1" title="In III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Disk-based KV Cache Structure and Operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.SS2" title="In III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Multi-Instance Structure of RAG-DCache</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4" title="In Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.SS1" title="In IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">RAG-Dcache Results and Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.SS2" title="In IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Shared RAG-DCache Results and Analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S5" title="In Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:207%;">Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hyungwoo Lee<sup class="ltx_sup" id="id13.13.id1"><span class="ltx_text ltx_font_italic" id="id13.13.id1.1">1</span></sup>, Kihyun Kim<sup class="ltx_sup" id="id14.14.id2"><span class="ltx_text ltx_font_italic" id="id14.14.id2.1">1</span></sup>, Jinwoo Kim<sup class="ltx_sup" id="id15.15.id3"><span class="ltx_text ltx_font_italic" id="id15.15.id3.1">1</span></sup>, Jungmin So<sup class="ltx_sup" id="id16.16.id4">1</sup>, Myung-Hoon Cha<sup class="ltx_sup" id="id17.17.id5">2</sup>
<br class="ltx_break"/>Hong-Yeon Kim<sup class="ltx_sup" id="id18.18.id6">2</sup>, James J. Kim<sup class="ltx_sup" id="id19.19.id7">3</sup>, Youngjae Kim<sup class="ltx_sup" id="id20.20.id8"><span class="ltx_text ltx_font_italic" id="id20.20.id8.1">1,‚Ä†</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id21.21.id1"><span class="ltx_text ltx_font_italic" id="id21.21.id1.1">‚Ä†</span></sup>Y. Kim is the corresponding author.
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup" id="id22.22.id1">1</sup>Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea
<br class="ltx_break"/><sup class="ltx_sup" id="id23.23.id2">2</sup>ETRI, Daejeon, Republic of Korea,
<sup class="ltx_sup" id="id24.24.id3">3</sup>Soteria Inc.
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id25.id1">Recent large language models (LLMs) face increasing inference latency as input context length and model size continue to grow. In particular, the retrieval-augmented generation (RAG) technique, which enhances LLM responses by incorporating external knowledge, exacerbates this issue by significantly increasing the number of input tokens. This expansion in token length leads to a substantial rise in computational overhead, particularly during the prefill stage, resulting in prolonged time-to-first-token (TTFT). To address this issue, this paper proposes a method to reduce TTFT by leveraging a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage.
We also introduce a disk-based shared KV cache management system, called Shared RAG-DCache, for multi-instance LLM RAG service environments. This system, together with an optimal system configuration, improves both throughput and latency under given resource constraints.
Shared RAG-DCache exploits the locality of documents related to user queries in RAG, as well as the queueing delay in LLM inference services. It proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances to enhance inference performance.
In experiments on a single host equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15‚Äì71% increase in throughput and up to a 12‚Äì65% reduction in latency, depending on the resource configuration.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
LLM, KV-Cache, RAG, Vector DB, TTFT

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, and their increasing scale continues to deliver progressively more powerful capabilities. Models with billions or even trillions of parameters have significantly advanced the state-of-the-art in natural language processing, enabling remarkable abilities in understanding context, generating coherent text, and generalizing across diverse linguistic scenarios. Nevertheless, despite their extensive capacity, LLMs frequently encounter difficulties when tasked with providing accurate responses involving the most recent or specialized internal corporate data, as such information typically falls outside the scope of their static pre-training datasets. This limitation arises because these models do not inherently possess mechanisms to dynamically integrate or update knowledge post-training, which significantly restricts their applicability in scenarios requiring up-to-date or confidential domain-specific insights.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address this limitation, Retrieval-Augmented Generation (RAG)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib1" title="">1</a>]</cite> has gained attention. RAG improves LLM prompts by retrieving external documents related to the user query, thereby increasing the accuracy of responses regarding up-to-date information or domain-specific knowledge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib3" title="">3</a>]</cite>. However, incorporating external context documents into the prompt significantly increases its length, leading to longer Time-To-First-Token (TTFT)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib4" title="">4</a>]</cite>
and reduced throughput. This phenomenon arises primarily from the increased computational complexity during the prefill phase of LLM inference, where the model computes attention scores and generates the corresponding key-value (KV) matrices for all tokens in the expanded prompt. Specifically, the complexity of KV cache generation scales approximately as
(<math alttext="O(L\textperiodcentered N^{2}\textperiodcentered D)" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">O</mi><mo id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">‚Å¢</mo><mrow id="S1.p2.1.m1.1.1.1.1" xref="S1.p2.1.m1.1.1.1.1.1.cmml"><mo id="S1.p2.1.m1.1.1.1.1.2" stretchy="false" xref="S1.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S1.p2.1.m1.1.1.1.1.1" xref="S1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.1.1.1.2" xref="S1.p2.1.m1.1.1.1.1.1.2.cmml">L</mi><mo id="S1.p2.1.m1.1.1.1.1.1.1" xref="S1.p2.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p2.1.m1.1.1.1.1.1.3" mathvariant="normal" xref="S1.p2.1.m1.1.1.1.1.1.3.cmml">¬∑</mi><mo id="S1.p2.1.m1.1.1.1.1.1.1a" xref="S1.p2.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><msup id="S1.p2.1.m1.1.1.1.1.1.4" xref="S1.p2.1.m1.1.1.1.1.1.4.cmml"><mi id="S1.p2.1.m1.1.1.1.1.1.4.2" xref="S1.p2.1.m1.1.1.1.1.1.4.2.cmml">N</mi><mn id="S1.p2.1.m1.1.1.1.1.1.4.3" xref="S1.p2.1.m1.1.1.1.1.1.4.3.cmml">2</mn></msup><mo id="S1.p2.1.m1.1.1.1.1.1.1b" xref="S1.p2.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p2.1.m1.1.1.1.1.1.5" mathvariant="normal" xref="S1.p2.1.m1.1.1.1.1.1.5.cmml">¬∑</mi><mo id="S1.p2.1.m1.1.1.1.1.1.1c" xref="S1.p2.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p2.1.m1.1.1.1.1.1.6" xref="S1.p2.1.m1.1.1.1.1.1.6.cmml">D</mi></mrow><mo id="S1.p2.1.m1.1.1.1.1.3" stretchy="false" xref="S1.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><times id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2"></times><ci id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">ùëÇ</ci><apply id="S1.p2.1.m1.1.1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1.1"><times id="S1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1.1.1.1"></times><ci id="S1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.1.1.1.2">ùêø</ci><ci id="S1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.1.1.1.3">¬∑</ci><apply id="S1.p2.1.m1.1.1.1.1.1.4.cmml" xref="S1.p2.1.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.1.1.4.1.cmml" xref="S1.p2.1.m1.1.1.1.1.1.4">superscript</csymbol><ci id="S1.p2.1.m1.1.1.1.1.1.4.2.cmml" xref="S1.p2.1.m1.1.1.1.1.1.4.2">ùëÅ</ci><cn id="S1.p2.1.m1.1.1.1.1.1.4.3.cmml" type="integer" xref="S1.p2.1.m1.1.1.1.1.1.4.3">2</cn></apply><ci id="S1.p2.1.m1.1.1.1.1.1.5.cmml" xref="S1.p2.1.m1.1.1.1.1.1.5">¬∑</ci><ci id="S1.p2.1.m1.1.1.1.1.1.6.cmml" xref="S1.p2.1.m1.1.1.1.1.1.6">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">O(L\textperiodcentered N^{2}\textperiodcentered D)</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">italic_O ( italic_L ¬∑ italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ¬∑ italic_D )</annotation></semantics></math>)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib6" title="">6</a>]</cite>, where L represents the number of Transformer layers, N denotes the total token length of the input (including both original prompts and added contexts), and D corresponds to the dimensionality of the hidden representations.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">During this process, each token‚Äôs embedding is transformed into query, key, and value vectors, after which self-attention calculations occur between these vectors, producing the attention scores and resulting value vectors. These calculated key-value pairs, which constitute the KV cache, must be computed for every token within the input prompt during the prefill stage, imposing substantial computational overhead. Particularly, as the input length (L) grows due to appended retrieved documents, the self-attention operations require quadratic complexity (<math alttext="O(L\textperiodcentered N^{2}\textperiodcentered D)" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">O</mi><mo id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">‚Å¢</mo><mrow id="S1.p3.1.m1.1.1.1.1" xref="S1.p3.1.m1.1.1.1.1.1.cmml"><mo id="S1.p3.1.m1.1.1.1.1.2" stretchy="false" xref="S1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S1.p3.1.m1.1.1.1.1.1" xref="S1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.1.1.1.2" xref="S1.p3.1.m1.1.1.1.1.1.2.cmml">L</mi><mo id="S1.p3.1.m1.1.1.1.1.1.1" xref="S1.p3.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p3.1.m1.1.1.1.1.1.3" mathvariant="normal" xref="S1.p3.1.m1.1.1.1.1.1.3.cmml">¬∑</mi><mo id="S1.p3.1.m1.1.1.1.1.1.1a" xref="S1.p3.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><msup id="S1.p3.1.m1.1.1.1.1.1.4" xref="S1.p3.1.m1.1.1.1.1.1.4.cmml"><mi id="S1.p3.1.m1.1.1.1.1.1.4.2" xref="S1.p3.1.m1.1.1.1.1.1.4.2.cmml">N</mi><mn id="S1.p3.1.m1.1.1.1.1.1.4.3" xref="S1.p3.1.m1.1.1.1.1.1.4.3.cmml">2</mn></msup><mo id="S1.p3.1.m1.1.1.1.1.1.1b" xref="S1.p3.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p3.1.m1.1.1.1.1.1.5" mathvariant="normal" xref="S1.p3.1.m1.1.1.1.1.1.5.cmml">¬∑</mi><mo id="S1.p3.1.m1.1.1.1.1.1.1c" xref="S1.p3.1.m1.1.1.1.1.1.1.cmml">‚Å¢</mo><mi id="S1.p3.1.m1.1.1.1.1.1.6" xref="S1.p3.1.m1.1.1.1.1.1.6.cmml">D</mi></mrow><mo id="S1.p3.1.m1.1.1.1.1.3" stretchy="false" xref="S1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><times id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2"></times><ci id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">ùëÇ</ci><apply id="S1.p3.1.m1.1.1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1.1"><times id="S1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1.1.1.1"></times><ci id="S1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.1.1.1.2">ùêø</ci><ci id="S1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.1.1.1.3">¬∑</ci><apply id="S1.p3.1.m1.1.1.1.1.1.4.cmml" xref="S1.p3.1.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S1.p3.1.m1.1.1.1.1.1.4.1.cmml" xref="S1.p3.1.m1.1.1.1.1.1.4">superscript</csymbol><ci id="S1.p3.1.m1.1.1.1.1.1.4.2.cmml" xref="S1.p3.1.m1.1.1.1.1.1.4.2">ùëÅ</ci><cn id="S1.p3.1.m1.1.1.1.1.1.4.3.cmml" type="integer" xref="S1.p3.1.m1.1.1.1.1.1.4.3">2</cn></apply><ci id="S1.p3.1.m1.1.1.1.1.1.5.cmml" xref="S1.p3.1.m1.1.1.1.1.1.5">¬∑</ci><ci id="S1.p3.1.m1.1.1.1.1.1.6.cmml" xref="S1.p3.1.m1.1.1.1.1.1.6">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">O(L\textperiodcentered N^{2}\textperiodcentered D)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">italic_O ( italic_L ¬∑ italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ¬∑ italic_D )</annotation></semantics></math>), substantially escalating the computation demand and slowing inference speed. This increased complexity becomes especially pronounced with larger LLM models, whose parameter sizes further amplify the computational cost.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Nevertheless, we observe an opportunity arising from the existence of query locality, meaning that a subset of external documents tends to be frequently referenced across multiple user queries in RAG systems. We analyzed popular question and answer datasets and found that processing 50% of the queries requires only between 3.1¬†31.39% of the documents.
This result shown that locality exists in documents referenced by user queries across various workloads, indicating that precomputed KV caches for these commonly accessed documents could significantly reduce redundant computations during inference.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The existing studies, RAGCache<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib7" title="">7</a>]</cite> and TurboRAG<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib8" title="">8</a>]</cite>, have explored optimizing RAG inference by precomputing KV caches for frequently accessed external documents.
RAGCache utilizes a multi-level caching approach, leveraging GPU and CPU memories to dynamically cache intermediate KV states, which significantly reduces inference latency. However, it faces limitations due to the restricted capacities of GPU and CPU memory, making it challenging to store large numbers of frequently referenced documents. On the other hand, TurboRAG adopts a disk-based caching strategy, storing precomputed KV caches offline, and significantly decreasing the prefill computational overhead and inference latency. Yet, TurboRAG does not explicitly address the multi-instance or multi-host scenarios for KV cache sharing, which are essential for scaling real-world RAG services.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In contrast, our approach clearly differentiates itself by emphasizing disk-based KV cache storage to effectively manage the growing size of KV caches caused by larger LLM parameters and longer input prompts in RAG scenarios. By utilizing persistent disk storage, our method overcomes the capacity constraints associated with GPU and CPU memory, ensuring effective and semi-permanent storage. Furthermore, we explicitly support multi-instance and multi-host LLM service environments, facilitating efficient sharing and reuse of KV caches stored on disk. This design leverages the infrequent update nature of external documents, allowing persistent, shareable KV caching across multiple inference instances and hosts‚Äîcapabilities not fully addressed by existing methods.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To leverage this opportunity, we propose a disk-based KV cache management system composed of two solutions: <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">RAG-DCache</span> and <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">Shared RAG-DCache</span>. RAG-DCache precomputes and stores the KV cache for frequently retrieved document chunks within a disk-resident vector database. During inference, these precomputed KV caches are reused directly, eliminating the costly recomputation of the full document context. Shared RAG-DCache extends this concept to multi-instance inference environments, enabling multiple LLM instances to share a common KV cache stored on disk, thus further enhancing inference performance by proactively generating and distributing KV caches across instances during query waiting periods.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The proposed system is consist of three key components:</p>
<ul class="ltx_itemize" id="S1.p8.2">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i1.1.1.m1.1"><semantics id="S1.I1.i1.1.1.m1.1b"><mo id="S1.I1.i1.1.1.m1.1.1" xref="S1.I1.i1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.1.1.m1.1c"><ci id="S1.I1.i1.1.1.m1.1.1.cmml" xref="S1.I1.i1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">KV Cache Manager</span>: Responsible for the offline precomputation and management of KV caches for document chunks, maintaining them in disk-based storage integrated within the vector database.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i2.1.1.m1.1"><semantics id="S1.I1.i2.1.1.m1.1b"><mo id="S1.I1.i2.1.1.m1.1.1" xref="S1.I1.i2.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.1.1.m1.1c"><ci id="S1.I1.i2.1.1.m1.1.1.cmml" xref="S1.I1.i2.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">KV Cache Generator</span>: Operates proactively, especially in multi-instance settings, to prefetch and generate KV caches during query wait times, thus efficiently utilizing idle resources.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i3.1.1.m1.1"><semantics id="S1.I1.i3.1.1.m1.1b"><mo id="S1.I1.i3.1.1.m1.1.1" xref="S1.I1.i3.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.1.1.m1.1c"><ci id="S1.I1.i3.1.1.m1.1.1.cmml" xref="S1.I1.i3.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Prompt Generator (RAG Processor)</span>: Combines the retrieved KV caches with user queries to construct the final inference prompts, enabling the LLM to bypass redundant computations by directly leveraging pre-stored KV caches.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">To demonstrate the effectiveness of our proposed system, we conducted experiments on a server equipped with dual GPUs and a single CPU, using the SQuAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib9" title="">9</a>]</cite> dataset as representative workload.
Results showed that employing RAG-DCache reduced the TTFT by approximately 10%‚Äì20%, with throughput increasing significantly as the model size and batch size grew.
Furthermore, our multi-instance solution, Shared RAG-DCache, achieved even more substantial improvements, increasing throughput by up to 71% and reducing latency by up to 65%.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Motivation</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">KV Cache Utilization in LLM Inference</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Transformer-based LLMs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib5" title="">5</a>]</cite> generate text in an autoregressive manner by producing one token at a time. To generate each token, the model processes the entire prompt and then, during the decode phase, reuses the previously generated tokens as input to predict the next token<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib14" title="">14</a>]</cite>. Recomputing all tokens at every generation step would be inefficient, so the model stores the Key and Value matrices from previous steps in GPU memory as a cache, which is then reused for subsequent token predictions.
This KV cache is critical for avoiding redundant computations and helps reduce the overall complexity of the decode phase. For example, optimization libraries like DeepSpeed-Inference<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib15" title="">15</a>]</cite> incorporate KV caching to enhance inference efficiency in large Transformer models.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">RAG Prompt Composition</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In a RAG system, external documents are retrieved using a vector database. These documents are first divided into manageable chunks, then converted into vector embeddings using an embedding model. These embeddings, together with their document IDs and original texts, are stored in the vector database<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib17" title="">17</a>]</cite>. When a query is received, it is similarly embedded and matched against this vector store to retrieve the top-k most relevant document chunks. The final LLM prompt is then composed by concatenating the retrieved document texts with the user query, typically structured as: ‚ÄùDocument: (retrieved texts) + Query: (user question) + Answer:‚Äù. However, as the number of tokens in the prompt lengthens due to added context, the computational complexity during the prefill phase increases significantly, leading to higher TTFT and reduced throughput.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To address this issue, this paper proposes utilizing external knowledge documents not only as retrieval sources but also as precomputed Key-Value (KV) caches. Specifically, if the KV tensors of the external knowledge documents are precomputed and stored, they can be directly reused whenever the same document is included in future LLM prompts. By leveraging disk-based caching of these precomputed KV tensors, the costly prefill computations associated with document processing are significantly reduced, resulting in shorter TTFT and improved inference performance. However, the effectiveness of this caching mechanism fundamentally depends on query locality‚Äîthe frequency with which particular documents are repeatedly referenced across multiple queries. Hence, understanding and exploiting query locality becomes crucial for optimizing system performance, which we discuss further in the following section.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Locality of Documents Retrieved by Queries in RAG</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To verify the locality between queries and their retrieved documents when using RAG, we used popular question-answering datasets, HotpotQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib18" title="">18</a>]</cite>, SQuAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib9" title="">9</a>]</cite> and TriviaQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib19" title="">19</a>]</cite>. We first constructed a FAISS vector database by embedding documents(using all-MiniLM-L6-v2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib20" title="">20</a>]</cite> embedding model) from each dataset. Then, for each query, we measured the similarity between the query embeddings and the document embeddings using an inner-product-based similarity metric, retrieving the top-1 most similar document to query. And using the [query, top-1 document] data, we calculated the proportion of documents required to process the queries.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center" id="S2.F1.1"><span class="ltx_text" id="S2.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="235" id="S2.F1.1.1.g1" src="extracted/6365438/figs/fig1.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">CDF of query related documents.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The results(Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.F1" title="Figure 1 ‚Ä£ II-C Locality of Documents Retrieved by Queries in RAG ‚Ä£ II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>) show that only 22.9%, 3.1%, and 31.4% of the most frequently retrieved documents account for 50% of all queries in each dataset. This result suggests that caching a small subset of frequently accessed documents can effectively serve a large portion of queries, highlighting the efficiency of using document caching in RAG systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Shared KV Cache for Multi-instance LLM Inference</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">LLM-based inference services generally operate multiple model instances in parallel to handle numerous real-time user requests.
For example, on a server with two GPUs, two LLM instances can be run to process two queries simultaneously, or one GPU can be allocated to a different task. In such multi-instance environments, each instance performs inference independently, so an instance cannot inherently access the KV cache computed by another instance. Therefore, to maximize the benefits of caching, a structure is needed that allows instances to exchange cache data via shared memory or storage (e.g., disk). We focus on a disk-based KV cache sharing approach to cope with the increasing number of LLM parameters and the growing size of input tokens.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Moreover, as requests(queries) per second increase with rising service loads, requests that exceed the capacity of an individual instance incur queue wait time. In a single-instance LLM environment, only one request can be processed at a time, so subsequent requests must inevitably wait. However, in a multi-instance environment, there is a possibility of utilizing free resources or other devices to prepare tasks that are waiting in the queue. For instance, while one GPU is decoding a current query, it may be possible to leverage another idle GPU or a CPU to carry out preliminary work for the next query, thereby reducing response latency. The proposed Shared RAG-DCache implements this idea by prefetching necessary document KV caches for requests that wait in the inference service queue beyond a certain threshold. As a result, when the request eventually reaches an LLM instance, it can focus solely on the decoding step.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S2.F2" title="Figure 2 ‚Ä£ II-D Shared KV Cache for Multi-instance LLM Inference ‚Ä£ II Background and Motivation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> shows the average response time from when a client issues a query until the response is received‚Äîdivided into queue wait time, LLM processing time, and network time(communication time between client and LLM server)‚Äîunder varying query rates per second, using a single Llama-3.2-1B<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib21" title="">21</a>]</cite> model on one GPU. For detailed experimental settings, refer to Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T1" title="TABLE I ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">I</span></a> in Section 4.
At 1‚Äì2 queries per second, LLM processing time makes up over 70% of the total time. However, once the query arrival rate exceeds the LLM processing time, the queue wait time increases exponentially. This suggests that under heavy loads, it is beneficial to utilize the waiting period for prefetch operations. Therefore, in a multi-instance environment, it is desirable to share caches among multiple instances to avoid redundant computation, and prepare caches in advance during wait times, allowing the GPU to focus on pure inference tasks. Shared RAG-DCache is the system designed to meet these needs.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<p class="ltx_p ltx_align_center" id="S2.F2.1"><span class="ltx_text" id="S2.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="118" id="S2.F2.1.1.g1" src="extracted/6365438/figures/fig2.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.4.2" style="font-size:90%;">Inference Latency Based on Requests Per Second in a Multi-Instance LLM(with RAG) Service Environment.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Design and Implementation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we detail the architecture and operation of our proposed RAG-DCache system‚Äîa disk-based KV cache for single-instance LLM inference‚Äîas well as its extension to a multi-instance environment, called Shared RAG-DCache.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Disk-based KV Cache Structure and Operation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The RAG-DCache system extends the traditional Retrieval-Augmented Generation pipeline by adding a disk-resident Key‚ÄìValue cache storage and associated management modules. The basic idea is to precompute the KV cache for each document chunk in the vector database and store these caches on disk so that they can be reloaded during inference instead of recomputed from the original text.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:des_vdb</span> contrasts a standard vector database with our augmented version. In a conventional RAG setup, the vector database stores embeddings of document chunks for similarity search.
In our approach RAG-DCache, we augment each stored document chunk with its precomputed ‚Äúchunked-document KV cache,‚Äù pairing it with the document‚Äôs ID and embedding in the database.
Because document data changes rarely once the vector DB is built, we can leverage idle hardware to generate these KV caches offline and persist them to disk. (It is also possible to generate a KV cache on-the-fly during inference for a newly encountered document and then add it to the DB for future reuse.)</p>
</div>
<figure class="ltx_figure" id="S3.F4.sf1">
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">By caching each document‚Äôs transformer key-value pairs in advance, the LLM can skip directly to using this cached representation when that document is retrieved for a query, rather than processing the raw text each time.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.F4" title="Figure 4 ‚Ä£ III-A Disk-based KV Cache Structure and Operation ‚Ä£ III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the design and operation of RAG-DCache, which incorporates a KV Cache‚Äìlinked vector DB for LLM inference using RAG. The main Components of disk-based KV Cache are as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<ul class="ltx_itemize" id="S3.SS1.p5.1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i1.1.1.m1.1"><semantics id="S3.I1.i1.1.1.m1.1b"><mo id="S3.I1.i1.1.1.m1.1.1" xref="S3.I1.i1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.1.1.m1.1c"><ci id="S3.I1.i1.1.1.m1.1.1.cmml" xref="S3.I1.i1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">KV Cache Manager</span>: This module is responsible for creating and managing the stored caches. It generates the KV cache for document chunks using the LLM and stores the resulting key-value tensors in the vector database. It also handles retrieval of these caches from disk upon request.
To minimize disk I/O latency, the KV Cache Manager employs an in-memory(CPU RAM) cache to hold frequently or recently used KV entries, leveraging faster memory access and reducing repeated disk reads. In essence, it acts as the interface between slow persistent storage and the rest of the system, optimizing cache generation and lookup.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i2.1.1.m1.1"><semantics id="S3.I1.i2.1.1.m1.1b"><mo id="S3.I1.i2.1.1.m1.1.1" xref="S3.I1.i2.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.1.1.m1.1c"><ci id="S3.I1.i2.1.1.m1.1.1.cmml" xref="S3.I1.i2.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">RAG Processor</span>: The RAG Processor orchestrates the RAG inference workflow. Upon receiving a user query, it performs similarity search on the vector database to fetch the
relevant document ID. It then requests the corresponding KV caches for the document from the KV Cache Manager, and composes the final LLM prompt by combining the query with the retrieved KV caches.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i3.1.1.m1.1"><semantics id="S3.I1.i3.1.1.m1.1b"><mo id="S3.I1.i3.1.1.m1.1.1" xref="S3.I1.i3.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.1.1.m1.1c"><ci id="S3.I1.i3.1.1.m1.1.1.cmml" xref="S3.I1.i3.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Integrated Vector Database</span>: This is an extended vector store that holds not only each document chunk‚Äôs embedding and original text, but also the precomputed KV caches.
Each entry in the vector DB effectively becomes a tuple of the form (embedding, document ID, text, KV cache). The inclusion of the KV cache alongside the embedding means that after retrieval, the system immediately has access to the document‚Äôs encoded representation for the LLM. Since documents are largely static, these caches can be generated offline and remain valid unless the document content changes. For any new documents added to the corpus, on-demand cache generation can be performed and the caches appended to the database, keeping the cache store up-to-date. This integrated DB design ensures that the vector index serves a dual purpose: it provides nearest-neighbor search for relevant documents and acts as a lookup table for their cached LLM representations.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">The end-to-end operation of RAG-DCache proceeds as follows (refer to the numbered steps in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.F4" title="Figure 4 ‚Ä£ III-A Disk-based KV Cache Structure and Operation ‚Ä£ III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<svg class="ltx_picture" height="15.18" id="S3.SS1.p7.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p7.pic1.1.1.1.1.1" style="color:#FFFFFF;">1</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS1.p7.1">Offline Cache Preparation: Initially, use existing documents to pre-generate the KV caches and build a KV-augmented vector database. The KV Cache Manager takes each document (or document chunk) and computes its KV cache using the LLM model, then stores this cache in the vector DB alongside the document‚Äôs embedding and ID. This step can be done offline or in the background, populating the disk cache before queries arrive. By the end of this step, the system has a disk-based cache of key-value pairs ready for many documents in the corpus.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<svg class="ltx_picture" height="15.18" id="S3.SS1.p8.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p8.pic1.1.1.1.1.1" style="color:#FFFFFF;">2</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS1.p8.1">,  <svg class="ltx_picture" height="15.18" id="S3.SS1.p8.1.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p8.1.pic1.1.1.1.1.1" style="color:#FFFFFF;">3</span></foreignobject></g></g></svg> Query Retrieval: When a user query comes in, the RAG Processor embeds the query and searches the vector database for the most relevant document ID. This yields the ID of the document that will be used to augment the query.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<svg class="ltx_picture" height="15.18" id="S3.SS1.p9.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p9.pic1.1.1.1.1.1" style="color:#FFFFFF;">5</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS1.p9.3">,  <svg class="ltx_picture" height="15.18" id="S3.SS1.p9.1.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p9.1.pic1.1.1.1.1.1" style="color:#FFFFFF;">6</span></foreignobject></g></g></svg> KV Cache Retrieval: For each document ID obtained in step  <svg class="ltx_picture" height="15.18" id="S3.SS1.p9.2.pic2" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p9.2.pic2.1.1.1.1.1" style="color:#FFFFFF;">2</span></foreignobject></g></g></svg>,  <svg class="ltx_picture" height="15.18" id="S3.SS1.p9.3.pic3" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p9.3.pic3.1.1.1.1.1" style="color:#FFFFFF;">3</span></foreignobject></g></g></svg>, the RAG Processor requests the corresponding KV cache from the KV Cache Manager. The KV Cache Manager checks its memory cache for the entry; if present, it returns it immediately from RAM. If not, it loads the KV cache from disk storage into memory and returns it to the RAG Processor.</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<svg class="ltx_picture" height="15.18" id="S3.SS1.p10.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p10.pic1.1.1.1.1.1" style="color:#FFFFFF;">7</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS1.p10.1">,  <svg class="ltx_picture" height="15.18" id="S3.SS1.p10.1.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p10.1.pic1.1.1.1.1.1" style="color:#FFFFFF;">8</span></foreignobject></g></g></svg> Prompt Composition: Meanwhile, the user‚Äôs query text is converted into the appropriate embedding or token IDs for the LLM model if not already done during retrieval. Once the KV cache for the document is in hand, the RAG Processor constructs the final LLM input prompt. It does this by inserting the retrieved KV cache data into the model‚Äôs context as ‚Äúpast key-values‚Äù and appending the user query tokens as the current input.
Essentially, the LLM is tricked into believing it has already processed the retrieved documents, because their resulting key-value pairs are provided, and now it only needs to attend to the user‚Äôs query. In practice, this means setting the model‚Äôs internal key-value state to the cached values and providing the query tokens as the next sequence to process.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="298" id="S3.F4.g1" src="extracted/6365438/figs/fig4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Design and Operation of RAG-DCache.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p11">
<svg class="ltx_picture" height="15.18" id="S3.SS1.p11.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS1.p11.pic1.1.1.1.1.1" style="color:#FFFFFF;">9</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS1.p11.1">LLM Inference (Prefill + Decode): The LLM, now armed with the combined prompt (document KV Cache + query embeddings),
proceeds to generate an answer. It first goes through its prefill phase and then the decode phase to produce output tokens. Because of RAG-DCache, the prefill phase is dramatically accelerated: instead of recomputing key-value pairs for the document‚Äôs text on the GPU, the model directly uses the precomputed keys and values. It only needs to encode the user query itself and then can immediately attend to the cached document representations when predicting the answer.
After the prefill, the decode stage proceeds as usual to generate the response token by token.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p12">
<p class="ltx_p" id="S3.SS1.p12.1">By leveraging pre-stored caches in this manner, RAG-DCache reduces the TTFT and overall computational load on the GPUs. In a baseline RAG system without caching, the LLM must process the full text of the retrieved documents for every single query, leading to significant repeated work in the prefill stage.
Our approach avoids this repetition. There is an overhead for loading the KV cache from disk (when a cache is not already in memory), but as long as efficient storage (fast SSD) and caching strategies are used, the sum of ‚Äú(disk load time) + (cached prefill time)‚Äù is typically much less than the original prefill time required to encode the documents from scratch.
In other words, even accounting for disk I/O, the TTFT with RAG-DCache is lower than without it, provided the caches are effectively utilized. This will be quantitatively demonstrated in our evaluation. Note that when multiple documents are retrieved (k &gt;1), we do not calculate cross-attention between the documents we only calculate KV values between the user query and the retrieved documents. This may lead to accuracy degradation as shown in the evaluation(Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.F9" title="Figure 9 ‚Ä£ IV-A RAG-Dcache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">9</span></a>)</p>
</div>
<div class="ltx_para" id="S3.SS1.p13">
<p class="ltx_p" id="S3.SS1.p13.1">To address this issue, instead of precomputing each document‚Äôs KV Caches and storing them in the vector DB, we changed our approach so that during inference‚Äîwhen RAG retrieves documents‚Äîthe KV Caches for the top-k documents are generated and stored together with the vector DB. We also made sure that the generated KV Caches are stored and managed within the vector DB along with the documents‚Äô IDs, allowing us to handle any top-k scenario. For example, if top-k = 3 and the retrieved document IDs are 1, 2, and 3, we calculate the attention for those three documents together to generate their KV Caches, then add that combination of document IDs to the vector DB so the KV Cache Manager can easily locate them. This approach applied to Shared RAG-DCache, which will be described next, leverages idle time during the inference process to precompute KV caches, regardless of the number of retrieved documents. These precomputed KV caches are then stored on disk, shared across instances, and reused to improve efficiency and maintain accuracy.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Multi-Instance Structure of RAG-DCache</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While RAG-DCache improves single-instance performance, modern LLM services often deploy multiple LLM instances (across one or more GPUs or nodes) to handle high query throughput. In such environments, caches computed in one instance could be beneficial to others. We therefore introduce Shared RAG-DCache, an extension of RAG-DCache for multi-instance LLM service environments. Shared RAG-DCache enables cache sharing and cache prefetching across multiple parallel LLM inference processes.
The goal is to exploit both the locality of document usage across different queries and the idle time that queries spend waiting in a queue due to high load to proactively generate and distribute KV caches.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S3.F5.g1" src="extracted/6365438/figures/fig4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Shared RAG-DCache Architecture.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.F5" title="Figure 5 ‚Ä£ III-B Multi-Instance Structure of RAG-DCache ‚Ä£ III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">5</span></a> depicts the architecture of Shared RAG-DCache, which builds on the single-instance design with additional components for multi-instance coordination. The main components are as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<ul class="ltx_itemize" id="S3.SS2.p3.1">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I2.i1.1.1.m1.1"><semantics id="S3.I2.i1.1.1.m1.1b"><mo id="S3.I2.i1.1.1.m1.1.1" xref="S3.I2.i1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i1.1.1.m1.1c"><ci id="S3.I2.i1.1.1.m1.1.1.cmml" xref="S3.I2.i1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">KV Cache Generator</span>: This is a new background module that proactively creates KV caches for queued queries. It continuously monitors the central query request queue and identifies queries that have been waiting longer than a predetermined threshold. For a query that is stuck in the queue (indicating the system is busy and the query will not be served immediately), the KV Cache Generator takes action: it immediately computes an embedding for the query, uses it to search the vector database for top-k relevant documents, and then computes the KV caches for those documents on the fly. Essentially, it performs steps 2‚Äì5 of the RAG-DCache workflow ahead of time for queries that are still waiting. The KV generation uses the same LLM model that will ultimately answer the query, but it can be executed on any available device ‚Äì for example, on an idle GPU if one exists, or on the CPU if all GPUs are busy ‚Äì since this is done asynchronously. Once generated, the new KV cache is stored to disk via the KV Cache Manager and indexed by document ID so that any LLM instance can retrieve it later. If a KV cache for a particular document was already created previously, the generator will detect this and avoid redundant computation. In that case, the existing cache can be reused directly. This component effectively prefetches document caches during the query‚Äôs waiting time, leveraging otherwise idle compute resources to reduce future work.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I2.i2.1.1.m1.1"><semantics id="S3.I2.i2.1.1.m1.1b"><mo id="S3.I2.i2.1.1.m1.1.1" xref="S3.I2.i2.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i2.1.1.m1.1c"><ci id="S3.I2.i2.1.1.m1.1.1.cmml" xref="S3.I2.i2.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Shared KV Cache Manager</span>: In a multi-instance setup, instead of each LLM instance having its own independent KV Cache Manager, we deploy a centralized KV Cache Manager service. This service coordinates the storage and sharing of KV caches among all LLM instances. It receives newly generated caches from the KV Cache Generator and inserts them into the global disk-based cache. When an LLM instance needs a KV cache for a document, it queries this shared manager rather than a local disk, and the manager supplies the data to the instance over the network or inter-process channel. The Shared KV Cache Manager thus acts as a cache server, ensuring that all LLM instances have a consistent view of available KV entries and that once a document‚Äôs cache is generated by any one instance or the generator, it can be used by all. Like the single-instance manager, it also implements a memory caching layer(using CPU RAM) with an eviction policy (e.g., LRU) to keep frequently accessed caches readily available. This is especially important in multi-instance scenarios to avoid repeatedly hitting the disk if multiple instances request the same cache around the same time.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I2.i3.1.1.m1.1"><semantics id="S3.I2.i3.1.1.m1.1b"><mo id="S3.I2.i3.1.1.m1.1.1" xref="S3.I2.i3.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i3.1.1.m1.1c"><ci id="S3.I2.i3.1.1.m1.1.1.cmml" xref="S3.I2.i3.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Prompt Generator (per-instance)</span>: Each LLM instance is equipped with a Prompt Generator module. This component is conceptually similar to the RAG Processor‚Äôs prompt composition step in the single-instance case, but tailored for a multi-instance context. When a query is assigned to a specific LLM instance for processing, that instance‚Äôs Prompt Generator will request any needed KV caches from the Shared KV Cache Manager. It then combines the retrieved KV cache(s) with the query text to form the final input prompt for its local LLM, identical to how it was described in the single-instance workflow.
With the KV cache preloaded into the model‚Äôs context, the LLM instance can skip directly to decoding the answer, greatly reducing the latency for that query. Essentially, the Prompt Generator ensures each instance makes full use of the globally cached data: it injects the shared KV into the model and thereby avoids that instance doing any redundant prefill computation for the documents.

</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">With these components, Shared RAG-DCache transforms a multi-instance deployment into a cooperative caching system.
Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S3.F6" title="Figure 6 ‚Ä£ III-B Multi-Instance Structure of RAG-DCache ‚Ä£ III Design and Implementation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the Shared RAG-DCache operation sequence:</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<svg class="ltx_picture" height="15.18" id="S3.SS2.p5.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS2.p5.pic1.1.1.1.1.1" style="color:#FFFFFF;">1</span></foreignobject></g></g></svg><svg class="ltx_picture" height="15.18" id="S3.SS2.p5.pic2" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS2.p5.pic2.1.1.1.1.1" style="color:#FFFFFF;">2</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS2.p5.1">Queue Monitoring: The system monitors the central queue of incoming queries continuously. If a query‚Äôs wait time exceeds a configured threshold (meaning the query has been in queue for a while due to heavy load), that query is flagged for cache prefetching. The threshold can be tuned ‚Äì any query waiting longer is considered a good candidate to start processing early, since it likely will wait that long anyway.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<svg class="ltx_picture" height="15.18" id="S3.SS2.p6.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS2.p6.pic1.1.1.1.1.1" style="color:#FFFFFF;">3</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS2.p6.1">Document Pre-search: For each flagged query, the KV Cache Generator immediately kicks in. It takes the query, computes its embedding, and performs a vector DB lookup to fetch the top-k most similar documents. This step is analogous to the retrieval step normally done by an LLM instance, but here it happens in parallel, on an idle thread/CPU or a free GPU, while the query is still in queue. By doing this in advance, we obtain the set of documents we anticipate the query will need, without delaying the query‚Äôs actual service time.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<svg class="ltx_picture" height="15.18" id="S3.SS2.p7.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS2.p7.pic1.1.1.1.1.1" style="color:#FFFFFF;">4</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS2.p7.1">KV Cache Preparation: Next, for each of the k retrieved documents, the system prepares the KV cache. If the shared cache already contains a KV entry for a document, the KV Cache Generator will simply load that cache ‚Äì possibly from disk to memory ‚Äì immediately. If a cache is missing, the generator will perform a prefill computation for that document using the LLM model to create the KV cache. Once generated, the new KV cache is stored into the shared vector DB on disk via the Shared KV Cache Manager, making it available system-wide. This step effectively precomputes the heavy part of the LLM‚Äôs work for the document while the query is still waiting in line. It‚Äôs done for all top-k documents so that the query‚Äôs entire retrieved context is cached ahead of time.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S3.F6.g1" src="extracted/6365438/figs/fig6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Shared RAG-DCache operation sequence.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p8">
<svg class="ltx_picture" height="15.18" id="S3.SS2.p8.pic1" overflow="visible" version="1.1" width="15.18"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.18) matrix(1 0 0 -1 0 0) translate(7.59,0) translate(0,7.59)"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="stroke:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.SS2.p8.pic1.1.1.1.1.1" style="color:#FFFFFF;">5</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S3.SS2.p8.1">LLM Inference: Eventually, the queued query reaches the front of the queue and is assigned to an LLM instance for execution. At this point, thanks to the previous steps, the KV caches for its relevant documents have likely already been generated and stored. The assigned instance‚Äôs Prompt Generator fetches those caches from the Shared KV Cache Manager. The Prompt Generator then constructs the LLM prompt, combining the query text with the retrieved KV caches. Now the LLM can immediately begin decoding the answer, since the time-consuming document encoding work was done earlier. In effect, the query‚Äôs waiting time has been utilized to do useful computation, so when the query is actually processed, the response is much faster. A significant portion of the would-be latency has been shaved off, resulting in a substantially lower response time for the user.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">Through this mechanism, Shared RAG-DCache ensures that no two instances ever duplicate the same KV computation, and that the query wait times in a busy service are put to productive use. Since all KV caches reside in a shared disk-based store, any cache generated by one instance or by the prefetcher is immediately available to all other instances. This not only cuts down latency for the individual query that triggered the prefetch, but also improves throughput overall: multiple LLM instances can pull from the same cache repository, benefitting from each other‚Äôs work. The design takes advantage of the locality in user queries and the nature of queued request handling to significantly reduce redundant computation across the system.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation</span>
</h2>
<figure class="ltx_figure" id="S4.F9.sf1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="11" id="S4.F9.sf1.g1" src="extracted/6365438/figures/3_cat.jpg" width="479"/>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We implemented the above system and conducted experiments to evaluate the performance gains of RAG-DCache and Shared RAG-DCache. All experiments were performed on a single-host server with the following specifications(Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T1" title="TABLE I ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">I</span></a>):</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Experimental hardware specifications.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.4">
<tr class="ltx_tr" id="S4.T1.4.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1" style="font-size:90%;">Component</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.2.1">
<span class="ltx_p" id="S4.T1.4.1.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.2.1.1.1" style="font-size:90%;">Specification</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.2.1"><span class="ltx_text" id="S4.T1.4.2.1.1" style="font-size:90%;">CPU</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.2.1">
<span class="ltx_p" id="S4.T1.4.2.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T1.4.2.2.1.1.1" style="font-size:90%;">AMD Ryzen 9 3900XT (12 cores)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.3.1"><span class="ltx_text" id="S4.T1.4.3.1.1" style="font-size:90%;">GPU</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.2.1">
<span class="ltx_p" id="S4.T1.4.3.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T1.4.3.2.1.1.1" style="font-size:90%;">NVIDIA RTX 2080 SUPER(2 units, 8GB each)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.4.1"><span class="ltx_text" id="S4.T1.4.4.1.1" style="font-size:90%;">Memory</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.2.1">
<span class="ltx_p" id="S4.T1.4.4.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T1.4.4.2.1.1.1" style="font-size:90%;">64GB RAM</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.5.1"><span class="ltx_text" id="S4.T1.4.5.1.1" style="font-size:90%;">Storage</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.2.1">
<span class="ltx_p" id="S4.T1.4.5.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T1.4.5.2.1.1.1" style="font-size:90%;">SAMSUNG 970 EVO NVMe SSD 500GB (read 3.4GB/s Write:2,4GB/s)</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We used the SQuAD v1.1 dataset as a source of queries and documents for retrieval, and a FAISS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib22" title="">22</a>]</cite> vector database for similarity search. Different large language models were used for single-instance vs. multi-instance tests, and we measured key metrics including TTFT and throughput (queries processed per second), as well as breakdowns of latency where appropriate. TTFT captures the latency from when a query is submitted to when the LLM outputs the first token of the answer ‚Äì this primarily reflects the time spent in the prefill stage since decoding the very first token is usually quick once the model has the prompt. Throughput is measured as the number of queries that can be completed per second, reflecting the system‚Äôs capacity under load.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The increase in the vector database storage capacity due to disk-based KV cache usage is shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T2" title="TABLE II ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">II</span></a>. The FAISS Vector DB size is the sum of the index, the document embedding vectors, and the original text size of the documents. RAG-DCache and Shared RAG-DCache require additional disk space for storing KV caches in addition to the FAISS vector database storage. As expected, the KV cache size increases with the number of model parameters. Even with the same number of parameters, the KV Cache size can differ depending on the embedding method used by each LLM model.
Additionally, if the number of documents extracted in RAG (top-k) increases, the size of the KV Cache generated may also grow. However, by adjusting the disk-based KV Cache that leverages query-document locality, the required storage space can be reduced‚Äîthis may represent a tradeoff between query throughput and the necessary disk size.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">Size of Normal FAISS Vector DB and KV Cache when using SQuAD dataset and different LLMs.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.4">
<tr class="ltx_tr" id="S4.T2.4.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.4.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.4.1.1.1"><span class="ltx_text" id="S4.T2.4.1.1.1.1"></span> <span class="ltx_text" id="S4.T2.4.1.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.4.1.1.1.2.1">
<span class="ltx_tr" id="S4.T2.4.1.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.1.2.1.1.1.1">FAISS</span></span></span>
<span class="ltx_tr" id="S4.T2.4.1.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.1.2.1.2.1.1">Vector DB</span></span></span>
</span></span> <span class="ltx_text" id="S4.T2.4.1.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4" id="S4.T2.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.1">RAG-DCache and Shared RAG-DCache</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.2.1.1">opt-1.3b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.2.2.1">opt-2.7b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.2.3.1">opt-6.7b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.2.4.1">LLAMA-1B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.4.3.1">0.5MB</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.4.3.2">5.9GB</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.4.3.3">9.9GB</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.4.3.4">16GB</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.4.3.5">1GB</td>
</tr>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">RAG-Dcache Results and Analysis</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For the single-instance scenario, we evaluated RAG-DCache using the SQuAD dataset and Facebook‚Äôs OPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#bib.bib23" title="">23</a>]</cite> decoder-only LLM of varying sizes. We tested three model sizes to see how cache benefits scale with model complexity. The vector database was implemented with Faiss, and we chunked the SQuAD documents into passages for retrieval. Before inference, we pre-generated the KV caches for all document chunks that might be retrieved for the SQuAD queries, by running each chunk through the respective OPT model‚Äôs prefill stage and storing the resulting KV pairs on disk. The KV Cache Manager was given a 16GB memory cache to hold recently used caches, as described earlier. Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T3" title="TABLE III ‚Ä£ IV-A RAG-Dcache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes the experimental setup, including the LLM models, dataset size (2,000 queries from SQuAD v1.1 train), and other components. We compared two settings: a baseline RAG (meaning the LLM processes raw text of retrieved documents for every query) and RAG-DCache enabled, across different batch sizes. Here, ‚Äúbatch size‚Äù refers to the number of queries processed simultaneously by the model. For each combination of model size and batch size, we measured the average TTFT and the throughput in both the baseline and RAG-DCache configuration.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.2.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.3.2" style="font-size:90%;">Experimental Environment.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.4">
<tr class="ltx_tr" id="S4.T3.4.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.1" style="font-size:90%;">Component</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.2.1" style="font-size:90%;">Specification</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.2.1"><span class="ltx_text" id="S4.T3.4.2.1.1" style="font-size:90%;">LLM</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.2.2"><span class="ltx_text" id="S4.T3.4.2.2.1" style="font-size:90%;">facebook/opt-1.3b, 2.7b, 6.7b</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.3.1"><span class="ltx_text" id="S4.T3.4.3.1.1" style="font-size:90%;">Embedding Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.3.2"><span class="ltx_text" id="S4.T3.4.3.2.1" style="font-size:90%;">all-MiniLM-L6-v2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.4.1"><span class="ltx_text" id="S4.T3.4.4.1.1" style="font-size:90%;">Dataset</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.4.2"><span class="ltx_text" id="S4.T3.4.4.2.1" style="font-size:90%;">SQuAD v1.1 Train (2,000 samples)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.5.1"><span class="ltx_text" id="S4.T3.4.5.1.1" style="font-size:90%;">Vector DB</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.4.5.2"><span class="ltx_text" id="S4.T3.4.5.2.1" style="font-size:90%;">Faiss DB, IndexFlatIP</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Performance Breakdown</span>: Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_rag</span> presents the results for TTFT, Prefill time, and KV cache loading time under various conditions. Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_a</span> shows the overall TTFT for each model and batch size, comparing the baseline to RAG-DCache. Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_b</span> and Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_c</span> break this TTFT into two components for the RAG-DCache case: the time spent in the LLM‚Äôs prefill stage, and the time spent loading KV caches from disk. In the baseline, TTFT is essentially all prefill.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">As shown in Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_a</span>, RAG-DCache consistently reduces TTFT in almost all cases. This is because using the disk-based KV cache drastically reduces the prefill computation time on the GPU, and the memory cache further cuts down repeated disk reads. In Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_b</span>, we see that the prefill time with RAG-DCache is much lower than baseline since the LLM doesn‚Äôt need to encode the full documents from scratch. Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:expr_c</span> shows the KV cache loading time incurred for RAG-DCache ‚Äì this is an overhead not present in the baseline. However, because of our caching optimizations, this overhead is kept relatively small: many cache loads are served from the 16GB memory cache, and even disk loads are fast on NVMe SSD. The result is that Prefill time savings outweigh the KV load time, yielding a net gain. For example, with the OPT-6.7B model at batch size 4, RAG-DCache might add a few milliseconds to load caches but saves far more time in GPU computation, leading to a substantially lower TTFT overall.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">However, as shown in the Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.F9" title="Figure 9 ‚Ä£ IV-A RAG-Dcache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">9</span></a> RAG-DCache works well when only a single document(top-k=1) is retrieved. We measured the accuracy of the answer using the formula F1 √ó 0.5 + Exact Match √ó 0.5, and the results showed that when top-k=1, the accuracy was the same as when RAG-DCache was not used. However, when multiple documents (top-k<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mo id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><gt id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">&gt;</annotation></semantics></math>1) were retrieved, the accuracy was higher than when RAG was not used but lower than when RAG-DCache was not applied. This is because we do not calculate cross-attention between the documents.</p>
</div>
<figure class="ltx_figure" id="S4.F9">
<p class="ltx_p ltx_align_center" id="S4.F9.1"><span class="ltx_text" id="S4.F9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="241" id="S4.F9.1.1.g1" src="extracted/6365438/figs/fig-accu.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.4.2" style="font-size:90%;">Accuracy based on the number of retrieved documents (top-k) when using RAG-DCache.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Impact of Model Size</span>: The benefits of RAG-DCache become more pronounced for larger models and larger batch sizes. Larger models have heavier per-token computation, so removing a chunk of tokens from their workload yields a bigger absolute time savings. Similarly, at higher batch sizes, the GPU is encoding multiple queries‚Äô documents at once in the baseline, which is very compute-intensive; with caching, those computations are skipped, freeing the GPU to handle more queries.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">In our experiments, we observed TTFT reductions of roughly 10%‚Äì20% with RAG-DCache compared to the baseline, depending on the configuration. These percentages tended towards the higher end (closer to 20%) for larger OPT models and larger batches. Concretely, Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T4" title="TABLE IV ‚Ä£ IV-A RAG-Dcache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the throughput achieved in each scenario, averaged per model.
Without RAG-DCache, the throughput for OPT-1.3B was approximately 23.74 QPS, which increased to 26.63 QPS with RAG-DCache ‚Äì approximately a 12% improvement.
For the OPT-2.7B model, throughput went from 15.32 QPS to 18.01 QPS, approximately a 17.6% gain. The OPT-6.7B model was the slowest, but improved to 11.05 QPS with caching (15.7% increase). The average relative improvement in these models was around 14%‚Äì15%, aligning well with the TTFT savings noted above. These results validate that RAG-DCache not only lowers the latency per query but also increases the overall throughput of the system, as the GPUs spend less time on redundant tasks and can handle more queries.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.2.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S4.T4.3.2" style="font-size:90%;">Average Throughput of Baseline and RAG-DCache.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.4">
<tr class="ltx_tr" id="S4.T4.4.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.1">Component</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.2.1">opt-1.3b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.3.1">opt-2.7b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.4.1">opt-6.7b</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.5.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.4.2.1.1">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.2.2">23.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.2.3">15.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.2.4">9.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.2.5">17.53</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.4.3.1.1">RAG-DCache</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.4.3.2">26.63</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.4.3.3">18.01</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.4.3.4">11.05</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.4.3.5">20.07</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Shared RAG-DCache Results and Analysis</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We next evaluate Shared RAG-DCache in a multi-instance LLM service scenario. The test environment remained the same dual-GPU server described above. We switched the LLM to Meta‚Äôs LLaMA-3.2-1B model for these experiments.
We used 1,000 queries from the SQuAD v1.1 dataset and for each query.
We retrieved either k=1 or k=2 documents to examine the effect of different context sizes.
These queries were sent in a continuous stream at a rate of 40 requests per second to simulate a heavy multi-user load. This high query rate ensured that at any given moment there were multiple queries waiting in the queue, which is necessary to fully leverage the prefetching mechanism of Shared RAG-DCache. If the system is not under load, queries won‚Äôt wait in queue and the KV Cache Generator might not trigger.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">And whether Shared RAG-DCache was used or not, the KV cache values generated during the prefill stage remained the same regardless of changes in the top-k value.
Therefore, accuracy was not measured separately. Furthermore, to control for variability in the decode phase, this experiment focused solely on measuring the prefill stage. This is because response latency can vary depending on the length of the generated answer during the decode stage. Our goal was to reduce noise and highlight the optimization benefits of the prefill stage.
And to isolate the effect of shared disk caching, we disabled the memory caching in the KV Cache Manager for these experiments.
This means all cache fetches go to disk, ensuring that any performance improvements observed are due to the multi-instance sharing and prefetching, not just RAM hits. Finally, to measure the performance improvement effect as the Disk-based KV Cache exapnds,
we processed the entire dataset multiple times in a random order.
</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The test configuration for this experiment is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.F10" title="Figure 10 ‚Ä£ IV-B Shared RAG-DCache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">10</span></a>. To determine the optimal configuration in the given environment, we evaluated two resource allocation strategies for CPU and GPU.</p>
<ul class="ltx_itemize" id="S4.SS2.p3.2">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S4.I1.i1.1.1.m1.1"><semantics id="S4.I1.i1.1.1.m1.1b"><mo id="S4.I1.i1.1.1.m1.1.1" xref="S4.I1.i1.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i1.1.1.m1.1c"><ci id="S4.I1.i1.1.1.m1.1.1.cmml" xref="S4.I1.i1.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">In Configuration (A) ‚ÄúGPU-Only KV Cache Generation‚Äù, we dedicate one of the two GPUs entirely to KV cache generation tasks, and use only the other GPU to run the LLM inference for answering queries. In other words, GPU0 handles all LLM inference requests, and GPU1 is reserved for computing KV caches of retrieved documents in the background.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S4.I1.i2.1.1.m1.1"><semantics id="S4.I1.i2.1.1.m1.1b"><mo id="S4.I1.i2.1.1.m1.1.1" xref="S4.I1.i2.1.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.1.1.m1.1c"><ci id="S4.I1.i2.1.1.m1.1.1.cmml" xref="S4.I1.i2.1.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.1.1.m1.1e">‚àô</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">In Configuration (B) ‚ÄúCPU-Based KV Cache Generation‚Äù, we use both GPUs for LLM inference, and assign all KV cache generation to the CPU. In this setup, the KV Cache Generator runs on the CPU, while both GPU0 and GPU1 are busy serving LLM inference requests.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Configuration (A) tests the scenario where we sacrifice a GPU to speed up cache prep, whereas (B) tests using no GPU for cache prep at the expense of slower cache generation on CPU. In both cases, Shared RAG-DCache is active, which means that caches are shared across the two LLM instances, and prefetching is enabled. The baseline for comparison is a multi-instance system without Shared RAG-DCache(w/o KVGen). We measure the system‚Äôs throughput in queries/sec and the average end-to-end latency which in our prefill-only measurement corresponds to how long a query waits plus its prefill time for each configuration and each top-k documents.</p>
</div>
<figure class="ltx_figure" id="S4.F10">
<p class="ltx_p ltx_align_center" id="S4.F10.1"><span class="ltx_text" id="S4.F10.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="208" id="S4.F10.1.1.g1" src="extracted/6365438/figures/fig5.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.3.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.4.2" style="font-size:90%;">Experimental System Configuration.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Overall Improvements</span>: Shared RAG-DCache showed significant performance improvements over the baseline, though the magnitude depended on the configuration. In Configuration (A), as shown in Figure ¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:exp_fig9</span>, enabling the shared cache system to improve throughput by approximately 35%-71% and reduce average latency by 31%‚Äì65% compared to the baseline, according to our measurements.
As the number of times the dataset was processed(TRY Number in the figure) increased, the performance improved further.(For the baseline, since there is no disk-based KV Cache expansion according to the number of tries (as with Shared RAG-DCache), it was measured only once.)</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">For a example, in the k=1 scenario under Configuration (A), throughput increased from 23.96 QPS to 24.78 QPS with caching, and the average latency dropped from 75.75s to 73.25s. These particular numbers represent a modest 3.4% throughput gain and 3.3% latency reduction ‚Äì relatively small, because with only one document the baseline was already not very slow and the single inference GPU was not heavily bottlenecked.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">In the k=2 scenario with Configuration (A), we observed mixed results: the caching system sometimes incurred overhead that offset its benefit. Specifically, handling two documents per query on only one inference GPU proved challenging ‚Äì the throughput and latency with caching in some trials were on par with or slightly worse than the baseline. This is likely because in Configuration (A) the single GPU had to handle the combined work of two documents‚Äô KV insertion plus the query itself, sequentially, which increased contention. The KV generation GPU could produce caches quickly, but the inference GPU became a bottleneck when k was larger.
However, as the TRY Number, which represents the number of times the dataset was repeatedly processed, increased, throughput improved, and latency decreased progressively. This is attributed to the accumulation of the Disk KV Cache, which increased the likelihood that queries processed by the LLM would reference the same KV Cache, thereby improving the Disk KV Cache hit ratio.</p>
</div>
<figure class="ltx_figure" id="S4.F12.sf1">
</figure>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">In summary, Configuration (A) demonstrated the viability of shared caching, but it showed limited parallelism ‚Äì dedicating a GPU for caches helped less than expected when the remaining GPU was overloaded with inference work for larger contexts. However, as the Disk KV Cache accumulated, throughput improved, and latency progressively decreased.</p>
</div>
<div class="ltx_para" id="S4.SS2.p9">
<p class="ltx_p" id="S4.SS2.p9.1">In Configuration (B), as shown in Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:exp_fig10</span>, with caching enabled, overall throughput increased by approximately 15%-28% and latency decreased by 12%‚Äì29% compared to baseline in this configuration. For example, as shown in Fig.10, in the k=1 case, as the number of times the dataset was processed increased, the throughput improved from 23.96 to 27.98 QPS, and the average latency fell from 75.75s to 47.92s. This is a significant improvement: ¬†17% higher throughput and ¬†37% lower latency. For k=2, the system with caching went from 14.34 QPS to 17.96 QPS, and latency dropped from 208.22s to 146.17s. That‚Äôs roughly a 25% increase in throughput and a 30% reduction in latency, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.1">We note that k=2 queries are inherently slower ‚Äì even with caching, the latency was higher than any k=1 scenario simply because processing two documents‚Äô worth of context takes extra time and resources. However, the relative improvement with caching is still substantial for k=2. These results confirm that Shared RAG-DCache is effective even when additional context is included, although the best absolute performance naturally occurs with fewer documents. Indeed, comparing k=1 vs k=2 across the board, we see that k=1 had lower latency and higher throughput in all configurations (baseline and caching). This is expected because more documents means more work.</p>
</div>
<figure class="ltx_figure" id="S4.F12.sf2">
</figure>
<div class="ltx_para" id="S4.SS2.p11">
<p class="ltx_p" id="S4.SS2.p11.1">Importantly, however, Shared RAG-DCache mitigates the performance penalty of larger k: for instance, going from 1 to 2 documents in the baseline caused throughput to drop by ¬†40% and latency to almost triple, whereas with Shared RAG-DCache the drop in throughput was less severe and the latency increase was much smaller ‚Äì and some of that remaining latency was due to the heavier workload rather than idle waiting. Even with k=2, the caching system delivered significantly better performance than baseline.</p>
</div>
<div class="ltx_para" id="S4.SS2.p12">
<p class="ltx_p" id="S4.SS2.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p12.1.1">Comparison of Configurations (A) vs (B)</span>: The comparison result is shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T5" title="TABLE V ‚Ä£ IV-B Shared RAG-DCache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">V</span></a>. The CPU-based KV generation (B) clearly emerges as the preferable configuration in our experiments. It achieved the highest throughput in all cases and more consistent latency reductions. In Configuration (A), when one GPU was taken away from inference, the remaining single inference GPU became a choke point under heavier loads (like k=2). It could not parallelize the work enough, and the benefit of offloading some computation to the second GPU was negated by the loss of overall inference capacity. In configuration (B), both GPUs were fully utilized for inference tasks, doubling the inference parallelism, while the CPU handled cache generation without impeding the GPUs. The CPU was effectively leveraging otherwise idle time since GPUs were busy, CPU cycles could be used to precompute caches. This leads to better pipeline balance: GPU power focused on what GPUs do best running the model for answers, and CPU cycles used for background prep work.</p>
</div>
<div class="ltx_para" id="S4.SS2.p13">
<p class="ltx_p" id="S4.SS2.p13.1">As a result, configuration (B) achieved the highest observed throughput in our tests ‚Äì for example, ¬†27.98 QPS at k=1 with caching, which was higher than even the baseline with two GPUs. It even outperformed Configuration (A)‚Äôs throughput despite Configuration (A) having a whole GPU doing cache work, indicating that that GPU might have been under-utilized or its benefit was offset by the other GPU‚Äôs overload. Additionally, the latency in configuration (B) was dramatically better: in Table III, configuration (B) brought average latency down to ¬†47.9s for k=1 and 146.2s for k=2, whereas Configuration (A) had 73.3s (k=1) and a very high 295.4s .</p>
</div>
<div class="ltx_para" id="S4.SS2.p14">
<p class="ltx_p" id="S4.SS2.p14.1">The k=2 result for Configuration (A) (295s) suggests that the single GPU was so overwhelmed that queries ended up waiting a long time despite caching ‚Äì possibly because the KV generation GPU was producing caches faster than the inference GPU could use them, leading to a queue buildup. In contrast, configuration (B) kept latencies much lower by always utilizing both GPUs for serving queries. These findings underscore that offloading KV generation to a non-GPU resource yields better overall system performance, which aligns with our resource allocation optimization strategy.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.2.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S4.T5.3.2" style="font-size:90%;">Average Performance Results by Configuration</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.4" style="width:433.6pt;height:119.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(52.9pt,-14.5pt) scale(1.32285004608235,1.32285004608235) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.4.1">
<tr class="ltx_tr" id="S4.T5.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.4.1.1.1">Top-k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.1.2">Avg. Perf.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.1.3">Conf. (A)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.1.4">GPU: 2 LLM(Baseline)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.1.5">Conf. (B)</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.4.1.2.1" rowspan="2"><span class="ltx_text" id="S4.T5.4.1.2.1.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.2">Throughput</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.3">24.78</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.4">23.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.5.1">27.98</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.1">Latency</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.2">73.25</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.3">75.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.4"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.3.4.1">47.92</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.4.1.4.1" rowspan="2"><span class="ltx_text" id="S4.T5.4.1.4.1.1">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.4.2">Throughput</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.4.3">11.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.4.4">14.34</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.4.5.1">17.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.4.1.5.1">Latency</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.4.1.5.2">295.37</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.4.1.5.3">208.22</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.4.1.5.4"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.5.4.1">146.17</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.p15">
<p class="ltx_p" id="S4.SS2.p15.1">In summary, Shared RAG-DCache provided some performance improvements in both configurations compared to the baseline, Utilizing not only GPU but also CPU resources for cache generation can maximize effectiveness. Our optimal setup in this experiment was configuration (B), which improved throughput by ¬†16.8% and ¬†25.2% for k=1 and 2 respectively, and cut latencies by ¬†36.8% and ¬†29.8%, relative to the no-caching baseline as summarized in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.11765v1#S4.T5" title="TABLE V ‚Ä£ IV-B Shared RAG-DCache Results and Analysis ‚Ä£ IV Evaluation ‚Ä£ Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs"><span class="ltx_text ltx_ref_tag">V</span></a>. Configuration (A) was suboptimal, showing the importance of a balanced resource allocation when using Shared RAG-Dcache.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we proposed and implemented a disk-based shared KV cache management system‚Äî<span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Shared RAG-DCache</span>‚Äîto optimize LLM inference in multi-instance service environments.
By leveraging query locality and service queue waiting times, our approach prefetches and shares the KV caches of frequently accessed documents across multiple instances. This significantly reduces redundant prefill computations and increases overall throughput while reducing response latency.
Experiments on a dual-GPU/one-CPU server demonstrated throughput improvements of up to 70% and latency reductions of tens of percentage points.
In particular, an optimal configuration was achieved when all GPUs were dedicated to LLM inference and the CPU handled KV cache generation, ensuring maximal performance gains.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:80%;">
P.¬†Lewis, E.¬†Perez, A.¬†Piktus, F.¬†Petroni, V.¬†Karpukhin, N.¬†Goyal, H.¬†K√ºttler, M.¬†Lewis, W.-t. Yih, T.¬†Rockt√§schel, S.¬†Riedel, and D.¬†Kiela, ‚ÄúRetrieval-augmented generation for knowledge-intensive nlp tasks,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib1.2.2" style="font-size:80%;">Proceedings of the 34th International Conference on Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib1.3.3" style="font-size:80%;">, NIPS ‚Äô20, (Red Hook, NY, USA), Curran Associates Inc., 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:80%;">
S.¬†Siriwardhana, R.¬†Weerasekera, E.¬†Wen, T.¬†Kaluarachchi, R.¬†Rana, and S.¬†Nanayakkara, ‚ÄúImproving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib2.2.2" style="font-size:80%;">Transactions of the Association for Computational Linguistics</span><span class="ltx_text" id="bib.bib2.3.3" style="font-size:80%;">, vol.¬†11, pp.¬†1‚Äì17, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:80%;">
J.¬†Chen, H.¬†Lin, X.¬†Han, and L.¬†Sun, ‚ÄúBenchmarking large language models in retrieval-augmented generation,‚Äù 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:80%;">
Q.¬†Fu, M.¬†Cho, T.¬†Merth, S.¬†Mehta, M.¬†Rastegari, and M.¬†Najibi, ‚ÄúLazyLLM: Dynamic token pruning for efficient long context LLM inference,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib4.2.2" style="font-size:80%;">Workshop on Efficient Systems for Foundation Models II @ ICML2024</span><span class="ltx_text" id="bib.bib4.3.3" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:80%;">
A.¬†Vaswani, ‚ÄúAttention is all you need,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib5.2.2" style="font-size:80%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib5.3.3" style="font-size:80%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:80%;">
T.¬†Dao, D.¬†Fu, S.¬†Ermon, A.¬†Rudra, and C.¬†R√©, ‚ÄúFlashattention: Fast and memory-efficient exact attention with io-awareness,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib6.2.2" style="font-size:80%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib6.3.3" style="font-size:80%;"> (S.¬†Koyejo, S.¬†Mohamed, A.¬†Agarwal, D.¬†Belgrave, K.¬†Cho, and A.¬†Oh, eds.), vol.¬†35, pp.¬†16344‚Äì16359, Curran Associates, Inc., 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:80%;">
C.¬†Jin, Z.¬†Zhang, X.¬†Jiang, F.¬†Liu, X.¬†Liu, X.¬†Liu, and X.¬†Jin, ‚ÄúRagcache: Efficient knowledge caching for retrieval-augmented generation,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib7.2.2" style="font-size:80%;">ArXiv</span><span class="ltx_text" id="bib.bib7.3.3" style="font-size:80%;">, vol.¬†abs/2404.12457, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:80%;">
S.¬†Lu, H.¬†Wang, Y.¬†Rong, Z.¬†Chen, and Y.¬†Tang, ‚ÄúTurborag: Accelerating retrieval-augmented generation with precomputed kv caches for chunked text,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib8.2.2" style="font-size:80%;">ArXiv</span><span class="ltx_text" id="bib.bib8.3.3" style="font-size:80%;">, vol.¬†abs/2410.07590, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:80%;">
P.¬†Rajpurkar, J.¬†Zhang, K.¬†Lopyrev, and P.¬†Liang, ‚ÄúSquad: 100,000+ questions for machine comprehension of text,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib9.2.2" style="font-size:80%;">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</span><span class="ltx_text" id="bib.bib9.3.3" style="font-size:80%;">, pp.¬†2383‚Äì2392, Association for Computational Linguistics, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:80%;">
W.¬†Kwon, Z.¬†Li, S.¬†Zhuang, Y.¬†Sheng, L.¬†Zheng, C.¬†H. Yu, J.¬†Gonzalez, H.¬†Zhang, and I.¬†Stoica, ‚ÄúEfficient memory management for large language model serving with pagedattention,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib10.2.2" style="font-size:80%;">Proceedings of the 29th Symposium on Operating Systems Principles</span><span class="ltx_text" id="bib.bib10.3.3" style="font-size:80%;">, SOSP ‚Äô23, (New York, NY, USA), p.¬†611‚Äì626, Association for Computing Machinery, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:80%;">
B.¬†Wu, Y.¬†Zhong, Z.¬†Zhang, G.¬†Huang, X.¬†Liu, and X.¬†Jin, ‚ÄúFast distributed inference serving for large language models,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib11.2.2" style="font-size:80%;">ArXiv</span><span class="ltx_text" id="bib.bib11.3.3" style="font-size:80%;">, vol.¬†abs/2305.05920, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:80%;">
G.¬†Xiao, Y.¬†Tian, B.¬†Chen, S.¬†Han, and M.¬†Lewis, ‚ÄúEfficient streaming language models with attention sinks,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib12.2.2" style="font-size:80%;">The Twelfth International Conference on Learning Representations</span><span class="ltx_text" id="bib.bib12.3.3" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:80%;">
G.-I. Yu, J.¬†S. Jeong, G.-W. Kim, S.¬†Kim, and B.-G. Chun, ‚ÄúOrca: A distributed serving system for Transformer-Based generative models,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib13.2.2" style="font-size:80%;">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)</span><span class="ltx_text" id="bib.bib13.3.3" style="font-size:80%;">, (Carlsbad, CA), pp.¬†521‚Äì538, USENIX Association, July 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:80%;">
Y.¬†Zhong, S.¬†Liu, J.¬†Chen, J.¬†Hu, Y.¬†Zhu, X.¬†Liu, X.¬†Jin, and H.¬†Zhang, ‚ÄúDistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib14.2.2" style="font-size:80%;">18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)</span><span class="ltx_text" id="bib.bib14.3.3" style="font-size:80%;">, (Santa Clara, CA), pp.¬†193‚Äì210, USENIX Association, July 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:80%;">
J.¬†Rasley, O.¬†Ruwase, S.¬†He, S.¬†Ye, G.¬†Huang, J.¬†Liu, and Y.¬†Wang, ‚ÄúDeepspeed inference: Enabling efficient inference of transformer models at unprecedented scale,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib15.2.2" style="font-size:80%;">arXiv preprint arXiv:2207.00032</span><span class="ltx_text" id="bib.bib15.3.3" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:80%;">
V.¬†Karpukhin, B.¬†Oguz, S.¬†Min, P.¬†Lewis, L.¬†Wu, S.¬†Edunov, D.¬†Chen, and W.-t. Yih, ‚ÄúDense passage retrieval for open-domain question answering,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib16.2.2" style="font-size:80%;">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span><span class="ltx_text" id="bib.bib16.3.3" style="font-size:80%;"> (B.¬†Webber, T.¬†Cohn, Y.¬†He, and Y.¬†Liu, eds.), (Online), pp.¬†6769‚Äì6781, Association for Computational Linguistics, Nov. 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:80%;">
J.¬†Johnson, M.¬†Douze, and H.¬†J√©gou, ‚ÄúBillion-scale similarity search with gpus,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib17.2.2" style="font-size:80%;">IEEE Transactions on Big Data</span><span class="ltx_text" id="bib.bib17.3.3" style="font-size:80%;">, vol.¬†7, no.¬†3, pp.¬†535‚Äì547, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:80%;">
Z.¬†Yang, P.¬†Qi, S.¬†Zhang, Y.¬†Bengio, W.¬†W. Cohen, R.¬†Salakhutdinov, and C.¬†D. Manning, ‚ÄúHotpotqa: A dataset for diverse, explainable multi-hop question answering,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib18.2.2" style="font-size:80%;">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span><span class="ltx_text" id="bib.bib18.3.3" style="font-size:80%;">, pp.¬†2369‚Äì2380, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:80%;">
M.¬†Joshi, E.¬†Choi, D.¬†S. Weld, and L.¬†Zettlemoyer, ‚ÄúTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension,‚Äù in </span><span class="ltx_text ltx_font_italic" id="bib.bib19.2.2" style="font-size:80%;">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span><span class="ltx_text" id="bib.bib19.3.3" style="font-size:80%;">, pp.¬†1601‚Äì1611, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:80%;">
W.¬†Wang, H.¬†Bao, S.¬†Huang, L.¬†Dong, and F.¬†Wei, ‚ÄúMinilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib20.2.2" style="font-size:80%;">arXiv preprint arXiv:2012.15828</span><span class="ltx_text" id="bib.bib20.3.3" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:80%;">
H.¬†Touvron, T.¬†Lavril, G.¬†Izacard, X.¬†Martinet, M.-A. Lachaux, T.¬†Lacroix, B.¬†Rozi√®re, N.¬†Goyal, E.¬†Hambro, F.¬†Azhar, A.¬†Rodriguez, A.¬†Joulin, E.¬†Grave, and G.¬†Lample, ‚ÄúLlama: Open and efficient foundation language models,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib21.2.2" style="font-size:80%;">ArXiv</span><span class="ltx_text" id="bib.bib21.3.3" style="font-size:80%;">, vol.¬†abs/2302.13971, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:80%;">
J.¬†Johnson, M.¬†Douze, and H.¬†J√©gou, ‚ÄúBillion-scale similarity search with gpus,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib22.2.2" style="font-size:80%;">IEEE Transactions on Big Data</span><span class="ltx_text" id="bib.bib22.3.3" style="font-size:80%;">, vol.¬†7, no.¬†3, pp.¬†535‚Äì547, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:80%;">
S.¬†Zhang, S.¬†Roller, N.¬†Goyal, M.¬†Artetxe, M.¬†Chen, S.¬†Chen, C.¬†Dewan, </span><span class="ltx_text ltx_font_italic" id="bib.bib23.2.2" style="font-size:80%;">et¬†al.</span><span class="ltx_text" id="bib.bib23.3.3" style="font-size:80%;">, ‚ÄúOpt: Open pre-trained transformer language models,‚Äù </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.4" style="font-size:80%;">arXiv preprint arXiv:2205.01068</span><span class="ltx_text" id="bib.bib23.5.5" style="font-size:80%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Apr 16 03:48:30 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
