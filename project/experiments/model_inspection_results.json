{
  "Gemma 3 12B": {
    "model_name": "Gemma 3 12B",
    "model_id": "mlx-community/gemma-3-12b-it-4bit",
    "config_json": {
      "architectures": [
        "Gemma3ForConditionalGeneration"
      ],
      "boi_token_index": 255999,
      "eoi_token_index": 256000,
      "eos_token_id": [
        1,
        106
      ],
      "image_token_index": 262144,
      "initializer_range": 0.02,
      "mm_tokens_per_image": 256,
      "model_type": "gemma3",
      "quantization": {
        "group_size": 64,
        "bits": 4
      },
      "text_config": {
        "hidden_size": 3840,
        "intermediate_size": 15360,
        "model_type": "gemma3_text",
        "num_attention_heads": 16,
        "num_hidden_layers": 48,
        "num_key_value_heads": 8,
        "rope_scaling": {
          "factor": 8.0,
          "rope_type": "linear"
        },
        "sliding_window": 1024
      },
      "torch_dtype": "bfloat16",
      "transformers_version": "4.50.0.dev0",
      "vision_config": {
        "hidden_size": 1152,
        "image_size": 896,
        "intermediate_size": 4304,
        "model_type": "siglip_vision_model",
        "num_attention_heads": 16,
        "num_hidden_layers": 27,
        "patch_size": 14,
        "vision_use_head": false,
        "skip_vision": true
      }
    },
    "model_args": {
      "has_args": true,
      "vocab_size": 262208,
      "model_type": "gemma3",
      "other_text_config": "{'hidden_size': 3840, 'intermediate_size': 15360, 'model_type': 'gemma3_text', 'num_attention_heads': 16, 'num_hidden_layers': 48, 'num_key_value_heads': 8, 'rope_scaling': {'factor': 8.0, 'rope_type': 'linear'}, 'sliding_window': 1024, 'vocab_size': 262208}"
    },
    "model_type": "Model",
    "layer_info": {}
  },
  "GPT-OSS-20B": {
    "model_name": "GPT-OSS-20B",
    "model_id": "mlx-community/gpt-oss-20b-4bit",
    "config_json": {},
    "model_args": {},
    "model_type": null,
    "layer_info": {},
    "error": "404 Client Error. (Request ID: Root=1-69754dda-40de31255100e0ac05bad484;7f1e05ad-dca4-44d3-9775-6098d126657b)\n\nRepository Not Found for url: https://huggingface.co/api/models/mlx-community/gpt-oss-20b-4bit/revision/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication"
  },
  "Qwen 2.5-14B": {
    "model_name": "Qwen 2.5-14B",
    "model_id": "mlx-community/Qwen2.5-14B-Instruct-4bit",
    "config_json": {
      "architectures": [
        "Qwen2ForCausalLM"
      ],
      "attention_dropout": 0.0,
      "bos_token_id": 151643,
      "eos_token_id": 151645,
      "hidden_act": "silu",
      "hidden_size": 5120,
      "initializer_range": 0.02,
      "intermediate_size": 13824,
      "max_position_embeddings": 32768,
      "max_window_layers": 70,
      "model_type": "qwen2",
      "num_attention_heads": 40,
      "num_hidden_layers": 48,
      "num_key_value_heads": 8,
      "quantization": {
        "group_size": 64,
        "bits": 4
      },
      "rms_norm_eps": 1e-06,
      "rope_theta": 1000000.0,
      "sliding_window": 131072,
      "tie_word_embeddings": false,
      "torch_dtype": "bfloat16",
      "transformers_version": "4.43.1",
      "use_cache": true,
      "use_sliding_window": false,
      "vocab_size": 152064
    },
    "model_args": {
      "has_args": true,
      "num_hidden_layers": 48,
      "num_key_value_heads": 8,
      "num_attention_heads": 40,
      "hidden_size": 5120,
      "vocab_size": 152064,
      "model_type": "qwen2",
      "rope_theta": 1000000.0,
      "rope_traditional": false,
      "rope_scaling": null,
      "other_intermediate_size": "13824",
      "other_max_position_embeddings": "32768",
      "other_rms_norm_eps": "1e-06",
      "other_tie_word_embeddings": "False"
    },
    "model_type": "Model",
    "layer_info": {
      "num_layers": 48,
      "attention_type": "Attention"
    }
  },
  "Llama 3.1-8B": {
    "model_name": "Llama 3.1-8B",
    "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit",
    "config_json": {
      "architectures": [
        "LlamaForCausalLM"
      ],
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 128000,
      "eos_token_id": [
        128001,
        128008,
        128009
      ],
      "hidden_act": "silu",
      "hidden_size": 4096,
      "initializer_range": 0.02,
      "intermediate_size": 14336,
      "max_position_embeddings": 131072,
      "mlp_bias": false,
      "model_type": "llama",
      "num_attention_heads": 32,
      "num_hidden_layers": 32,
      "num_key_value_heads": 8,
      "pretraining_tp": 1,
      "quantization": {
        "group_size": 64,
        "bits": 4
      },
      "rms_norm_eps": 1e-05,
      "rope_scaling": {
        "factor": 8.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
      },
      "rope_theta": 500000.0,
      "tie_word_embeddings": false,
      "torch_dtype": "bfloat16",
      "transformers_version": "4.42.3",
      "use_cache": true,
      "vocab_size": 128256
    },
    "model_args": {
      "has_args": true,
      "num_hidden_layers": 32,
      "num_key_value_heads": 8,
      "num_attention_heads": 32,
      "hidden_size": 4096,
      "head_dim": null,
      "sliding_window": null,
      "vocab_size": 128256,
      "model_type": "llama",
      "attention_bias": false,
      "rope_theta": 500000.0,
      "rope_traditional": false,
      "rope_scaling": {
        "factor": 8.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
      },
      "other_intermediate_size": "14336",
      "other_layer_types": "['full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention']",
      "other_max_position_embeddings": "131072",
      "other_mlp_bias": "False",
      "other_rms_norm_eps": "1e-05",
      "other_tie_word_embeddings": "False"
    },
    "model_type": "Model",
    "layer_info": {
      "num_layers": 32,
      "attention_type": "Attention"
    }
  }
}